---
output:
  pdf_document: default
  html_document: default
---
# Text Mining Software Engineering Data 

In software engineering, there is a lot of information in plain text such as requirements, bug reports, mails, reviews from applicatons,  etc.  Typically that information can be extracted from Software Configuration Management Systems (SCM), Bug Tracking Systems (BTS) such as Bugzilla or application stores such as Google Play or Apple's AppStore, etc. can be mined to extract relevant information. Here we briefly explain the text mining process and how this can be done with R.

The main package for _text mining_ is `tm` [@FeinererH15][@FeinererHM08]. Another popular package is `wordcloud`. 

```{r}
# - Install packages

# pckgs_needed <- c("tm", "wordcloud")
# install.packages(pckgs_needed, dependencies = TRUE)
```

## Terminology 

We need to consider:
  * Importing data. A _Corpus_ is a collection of text documents, implemented as VCorpus (corpora are R object held in memory). The `tm` provides several corpus constructors: `DirSource`, `VectorSource`, or `DataframeSource` (`getSources()`). readerControl of the corpus constructor has to be a list with the named components reader and language
  * Preprocessing 
  * Inspecting and exploring data: Individual documents can be accessed via [[
  * Transformations: Transformations are done via the tm_map() function. 
    ++ tm_map(_____, stripWhitespace)  
    ++ tm_map(_____, content_transformer(tolower))  
    ++ tm_map(_____, removeWords, stopwords("english"))  
    ++ tm_map(reuters, stemDocument)
  * Creating `Term-Document` Matrices: TermDocumentMatrix and DocumentTermMatrix
    ++ 
  * Relationships between terms.
    ++ findFreqTerms(_____, anumber)
    ++ findAssocs(Mydtm, "aterm", anumbercorrelation)
    ++ A dictionary is a (multi-)set of strings. It is often used to denote relevant terms in text mining.  
    ++ inspect(DocumentTermMatrix( newsreuters, list(dictionary = c("term1", "term2", "term3")))).   It displays detailed information on a corpus or a term-document matrix.
  * Clustering
    

## Basic **tm** commands

For example, load a tm dataset  20 news articles with additional meta information from the Reuters-21578 dataset

```{r}
library('tm')
data("crude")
crude
crude[[2]]$content  # returns the text content of the second news article
```


Create a data frame source:

```{r}
# Create a vector source.
docs <- c("This is a text.", "This another one.")
(vs <- VectorSource(docs))
inspect(VCorpus(vs))

# create a data frame source
docs <- data.frame(c("This is a text.", "This another one."))
(ds <- DataframeSource(docs))
inspect(VCorpus(ds))

# accessing documents, terms
# For Docs and Terms, a character vector with document IDs and terms, respectively.
# For nDocs and nTerms, an integer with the number of document IDs and terms, respectively.

tdm <- TermDocumentMatrix(crude)[1:10,1:20] # 10 terms, 20 documents
tdm1 <- TermDocumentMatrix(crude)

Docs(tdm)
nDocs(tdm)
nTerms(tdm)
Terms(tdm)

# Inspect, i.e., display detailed information on a corpus or a term-document matrix.
inspect(crude[1:3])
tdm <- TermDocumentMatrix(crude)[1:10, 1:10]
inspect(tdm)


# TermDocumentMatrix  Constructs or coerces to a term-document matrix or a document-term matrix.

data("crude")
tdmRmPct <- TermDocumentMatrix(crude, control = list(removePunctuation = TRUE,
                                                stopwords = TRUE))
dtm <- DocumentTermMatrix(crude, control = list(weighting = function(x) weightTfIdf(x, normalize =
FALSE),
stopwords = TRUE))
inspect(tdmRmPct[202:205, 1:5])
inspect(tdmRmPct[c("price", "texas"), c("127", "144", "191", "194")])
inspect(dtm[1:5, 273:276])

#10 terms 
tdmRmPct10 <- TermDocumentMatrix(crude, control = list(removePunctuation = TRUE,
                                                stopwords = TRUE))[1:10, 1:10]
Docs(tdmRmPct10)
nDocs(tdmRmPct10)
nTerms(tdmRmPct10)
Terms(tdmRmPct10)

```




```{r}



# Visualize correlations between terms of a term-document matrix. Visualization requires that package Rgraphviz is available

# here we remove punctuation, numbers, stopwords
tdm <- TermDocumentMatrix(crude,
                          control = list(removePunctuation = TRUE,
                                         removeNumbers = TRUE,
                                         stopwords = TRUE))
plot(tdm, corThreshold = 0.6, weighting = TRUE)
```





```{r}
# Find associations in a document-term or term-document matrix.
tdm <- TermDocumentMatrix(crude)
findAssocs(tdm, c("oil", "opec", "xyz"), c(0.7, 0.75, 0.1))

# Find frequent terms in a document-term or term-document matrix
findFreqTerms(tdm, 6, 8)

# termFreq   Generate a term frequency vector from a text document
data("crude")
tmfq1 <- termFreq(crude[[14]])
tmfq1
str(tmfq1)
strsplit_space_tokenizer <- function(x) unlist(strsplit(as.character(x), "[[:space:]]+"))
ctrl <- list(tokenize = strsplit_space_tokenizer, removePunctuation = list(preserve_intra_word_dashes = TRUE),
             stopwords = c("reuter", "that"),
             stemming = TRUE,
             wordLengths = c(4, Inf))
tmfq2 <- termFreq(crude[[14]], control = ctrl)
str(tmfq2)


#weights
# weightBin(m) Weight Binary Binary weight a term-document matrix.
# WeightFunction Construct a weighting function for term-document matrices.
weightCutBin <- WeightFunction(function(m, cutoff) m > cutoff,
                               "binary with cutoff", "bincut")
# weightSMART Weight a term-document matrix according to a combination of weights specified in SMART notation.
TermDocumentMatrix(crude, 
                   control = list(removePunctuation = TRUE,
                                  stopwords = TRUE,
                                  weighting = function(x) weightSMART(x, spec = "ntc")))

# weightTf Weight by Term Frequency   weightTf(m)

# weightTfIdf Weight by Term Frequency - Inverse Document Frequency

```



```{r}

# Transformations
# Remove Numbers from a Text Document
crude[[1]]$content
crude1 <- removeNumbers(crude[[1]])
crude1
# Remove Punctuation Marks from a Text Document
crude[[14]]$content
cruderem <- removePunctuation(crude[[14]])
cruderem
cruderem1 <- removePunctuation(crude[[14]], preserve_intra_word_dashes = TRUE)
cruderem1$content

# Remove sparse terms from a document-term or term-document matrix. sparse: A numeric for the maximal allowed sparsity in the range from bigger zero to smaller one

tdm <- TermDocumentMatrix(crude)
tdm_lssparse <- removeSparseTerms(tdm, 0.2) 
tdm_lssparse
tdm_lssparse2 <- removeSparseTerms(tdm, 0.7) #allow more terms
tdm_lssparse2
Terms(tdm_lssparse2)
Terms(tdm_lssparse)
tdm_lssparse3 <- removeSparseTerms(tdm, 0.05) # remove almost all sparsity and empty terms
tdm_lssparse3
Terms(tdm_lssparse3)

# Remove words from a text document.

crude[[1]]$content
# remove common words in English
cruderemword <- removeWords(crude[[1]], stopwords("english"))
cruderemword$content

# Stem words in a text document using Porterâ€™s stemming algorithm.

crude[[1]]
crudestem <- stemDocument(crude[[1]])

# Return various kinds of stopwords with support for different languages.
stopwords("en")
stopwords("SMART")
stopwords("german")

# Strip Whitespace from a Text Document
crude[[1]]$content
stripWhitespace(crude[[1]]$content)

# getTransformations  Predefined transformations (mappings) which can be used with tm_map
## Document access triggers the stemming function
## (i.e., all other documents are not stemmed yet)
tm_map(crude, stemDocument, lazy = TRUE)[[1]]

## Use wrapper to apply character processing function
tm_map(crude, content_transformer(tolower))

## Generate a custom transformation function which takes the heading as new content
headings <- function(x)
PlainTextDocument(meta(x, "heading"), id = meta(x, "id"),
                  language = meta(x, "language"))
inspect(tm_map(crude, headings))


```



## Example of classifying bugs from Bugzilla 


Bugzilla is Issue Tracking System that allow us to maintain and track the evolution of a project.
The following example shows how to work with entries from Bugzilla. It is assumed that the data has been extracted and we have the records in a flat file (this can be done using Web crawlers or directly using the SQL database).


```{r message=FALSE, warning=FALSE}
library(foreign)
# path_name <- file.path("C:", "datasets", "textMining")
# path_name
# dir(path_name)

#d <- read.arff("./datasets/textMining/compendium.arff")
options(stringsAsFactors = FALSE)
d <- read.arff("./datasets/textMining/reviewsBugs.arff" )
str(d)
head(d,2)
# fifth entry
d$revContent[5]
d$revBug[5]
```

Creating a Document-Term Matrix (DTM)

```{r message=FALSE, warning=FALSE}
library(tm)

ds <- DataframeSource(as.data.frame(d$revContent))
dsc <- Corpus(ds)

  
# weighting=TfIdf weighting is Tf-Idf
# minWordLength=WL the minimum word length is WL
# minDocFreq=ND each word must appear at least in ND docs

# Other options of DTM
# These are not really needed, if preprocessing has been carried out:
# stemming=TRUE stemming is applied
# stopwords=TRUE stopwords are eliminated
# removeNumbers=TRUE numbers are eliminated


dtm <- DocumentTermMatrix(dsc, control = list(weighting = weightTfIdf, minDocFreq=3, stopwords = TRUE, removeNumbers = TRUE))

# dim(dtm)
# inspect(dtm) #[1:10,1:10])

# dtm.70=removeSparseTerms(dtm,sparse=0.7)
# dtm.70 # or dim(dtm.70)
# note that the term-document matrix needs to be transformed (casted)
# to a matrix form in the following barplot command

sparseparam <- 0.90
dtm_sprs <- removeSparseTerms(dtm,sparse=sparseparam)
maintitle <-paste0("Most frequent terms (sparseness=" ,sparseparam , "  )")
barplot(as.matrix(dtm_sprs),xlab="terms",ylab="number of occurrences", main=maintitle)
```

As data frame:

```{r results='hide'}
#dtmdf <- as.data.frame(dtm.90)
inspect(dtm_sprs)
dtmdf <- as.data.frame(as.matrix(dtm_sprs))
# rownames(dtm)<- 1:nrow(dtm)

class <- d$revBug
dtmdf <- cbind(dtmdf,class)
head(dtmdf, 3)
```

Now, we can explore things such as "which words are associated with "feature"?"
```{r}
# which words are associated with "bug"?
findAssocs(dtm, 'bug', .3)
```

And find frequent terms.

```{r}
findFreqTerms(dtm,15)
```

Use any classifier now:

```{r}
library(caret)
library(randomForest)

inTraining <- createDataPartition(dtmdf$class, p = .75, list = FALSE)
training <- dtmdf[ inTraining,]
testing  <- dtmdf[-inTraining,]

fitControl <- trainControl(## 5-fold CV
                           method = "repeatedcv",
                           number = 5,
                           ## repeated ten times
                           repeats = 5)


gbmFit1 <- train(class ~ ., data = training,
                 method = "gbm",
                 trControl = fitControl,
                 ## This last option is actually one
                 ## for gbm() that passes through
                 verbose = FALSE)

gbmFit1

# trellis.par.set(caretTheme())
# plot(gbmFit1)
# 
# trellis.par.set(caretTheme())
# plot(gbmFit1, metric = "Kappa")

head(predict(gbmFit1, testing, type = "prob"))

conf_mat <- confusionMatrix(testing$class, predict(gbmFit1, testing))
conf_mat
```


And finally, a word cloud as an example that appears everywhere these days.

```{r}
library(wordcloud)
# calculate the frequency of words and sort in descending order.
wordFreqs=sort(colSums(as.matrix(dtm_sprs)),decreasing=TRUE)
wordcloud(words=names(wordFreqs),freq=wordFreqs)
```


## Extracting data from Twitter

The hardest bit is to link with Twitter. Using the TwitteR package is explained following this [example](./twitter.Rmd).


