% This file was created with JabRef 2.10.
% Encoding: ISO8859_1


@InProceedings{Alves2010,
  Title                    = {Deriving metric thresholds from benchmark data},
  Author                   = {T.L. Alves and C.Ypma and J. Visser},
  Booktitle                = {IEEE International Conference on Software Maintenance (ICSM'2010)},
  Year                     = {2010},
  Month                    = {Sept},
  Pages                    = {1--10},

  Abstract                 = {A wide variety of software metrics have been proposed and a broad range of tools is available to measure them. However, the effective use of software metrics is hindered by the lack of meaningful thresholds. Thresholds have been proposed for a few metrics only, mostly based on expert opinion and a small number of observations. Previously proposed methodologies for systematically deriving metric thresholds have made unjustified assumptions about the statistical properties of source code metrics. As a result, the general applicability of the derived thresholds is jeopardized. We designed a method that determines metric thresholds empirically from measurement data. The measurement data for different software systems are pooled and aggregated after which thresholds are selected that (i) bring out the metric's variability between systems and (ii) help focus on a reasonable percentage of the source code volume. Our method respects the distributions and scales of source code metrics, and it is resilient against outliers in metric values or system size. We applied our method to a benchmark of 100 object-oriented software systems, both proprietary and open-source, to derive thresholds for metrics included in the SIG maintainability model.},
  Doi                      = {10.1109/ICSM.2010.5609747},
  ISSN                     = {1063-6773},
  Keywords                 = {object-oriented methods;public domain software;software metrics;benchmark data;metric threshold;object oriented software system;open source software;software metrics;source code metrics;Benchmark testing;Complexity theory;Histograms;Java;Measurement;Software systems}
}

@InProceedings{Benlarbi2000,
  Title                    = {Thresholds for object-oriented measures},
  Author                   = {S. Benlarbi and K. El Emam and N. Goel and S. Rai},
  Booktitle                = {Proceedings 11th International Symposium on Software Reliability Engineering (ISSRE 2000)},
  Year                     = {2000},
  Pages                    = {24--38},

  Abstract                 = {A practical application of object oriented measures is to predict which classes are likely to contain a fault. This is contended to be meaningful because object oriented measures are believed to be indicators of psychological complexity, and classes that are more complex are likely to be faulty. Recently, a cognitive theory was proposed suggesting that there are threshold effects for many object oriented measures. This means that object oriented classes are easy to understand as long as their complexity is below a threshold. Above that threshold their understandability decreases rapidly, leading to an increased probability of a fault. This occurs, according to the theory, due to an overflow of short-term human memory. If this theory is confirmed, then it would provide a mechanism that would explain the introduction of faults into object oriented systems, and would also provide some practical guidance on how to design object oriented programs. The authors empirically test this theory on two C++ telecommunications systems. They test for threshold effects in a subset of the Chidamber and Kemerer (CK) suite of measures (S. Chidamber and C. Kemerer, 1994). The dependent variable was the incidence of faults that lead to field failures. The results indicate that there are no threshold effects for any of the measures studied. This means that there is no value for the studied CK measures where the fault-proneness changes from being steady to rapidly increasing. The results are consistent across the two systems. Therefore, we can provide no support to the posited cognitive theory},
  Doi                      = {10.1109/ISSRE.2000.885858},
  ISSN                     = {1071-9458},
  Keywords                 = {C++ language;bibliographies;object-oriented programming;reverse engineering;software metrics;software performance evaluation;software quality;telecommunication computing;C++ telecommunications systems;CK measures;cognitive theory;dependent variable;fault introduction;fault-proneness;field failures;object oriented classes;object oriented measure thresholds;object oriented programs;object oriented systems;psychological complexity;short-term human memory overflow;threshold effects;understandability;Councils;Hospitals;Humans;Inspection;Object oriented modeling;Particle measurements;Psychology;Quality management;Software measurement;System testing}
}

@InProceedings{Bowes2015,
  Title                    = {Different Classifiers Find Different Defects Although With Different Level of Consistency},
  Author                   = {Bowes, David and Hall, Tracy and Petri\'{c}, Jean},
  Booktitle                = {Proceedings of the 11th International Conference on Predictive Models and Data Analytics in Software Engineering (PROMISE'15},
  Year                     = {2015},

  Address                  = {New York, NY, USA},
  Pages                    = {3:1--3:10},
  Publisher                = {ACM},
  Series                   = {PROMISE'15},

  Acmid                    = {2810149},
  Articleno                = {3},
  Doi                      = {10.1145/2810146.2810149},
  ISBN                     = {978-1-4503-3715-1},
  Location                 = {Beijing, China},
  Numpages                 = {10},
  Url                      = {http://doi.acm.org/10.1145/2810146.2810149}
}

@InProceedings{Canfora2013,
  Title                    = {Multi-objective Cross-Project Defect Prediction},
  Author                   = {G. Canfora and A. De Lucia and M. Di Penta and R. Oliveto and A. Panichella and S. Panichella},
  Booktitle                = {IEEE Sixth International Conference on Software Testing, Verification and Validation},
  Year                     = {2013},
  Month                    = {March},
  Pages                    = {252--261},

  Abstract                 = {Cross-project defect prediction is very appealing because (i) it allows predicting defects in projects for which the availability of data is limited, and (ii) it allows producing generalizable prediction models. However, existing research suggests that cross-project prediction is particularly challenging and, due to heterogeneity of projects, prediction accuracy is not always very good. This paper proposes a novel, multi-objective approach for cross-project defect prediction, based on a multi-objective logistic regression model built using a genetic algorithm. Instead of providing the software engineer with a single predictive model, the multi-objective approach allows software engineers to choose predictors achieving a compromise between number of likely defect-prone artifacts (effectiveness) and LOC to be analyzed/tested (which can be considered as a proxy of the cost of code inspection). Results of an empirical evaluation on 10 datasets from the Promise repository indicate the superiority and the usefulness of the multi-objective approach with respect to single-objective predictors. Also, the proposed approach outperforms an alternative approach for cross-project prediction, based on local prediction upon clusters of similar classes.},
  Doi                      = {10.1109/ICST.2013.38},
  ISSN                     = {2159-4848},
  Keywords                 = {genetic algorithms;pattern clustering;regression analysis;search problems;software engineering;software management;GA;LOC analysis;LOC testing;Promise repository;defect-prone artifacts;generalizable prediction models;genetic algorithm;multiobjective cross-project defect prediction accuracy;multiobjective logistic regression model;project heterogeneity;software engineers;Accuracy;Data models;Inspection;Logistics;Measurement;Predictive models;Software;Cross-project defect prediction;multi-objective optimization;search-based software engineering}
}

@Article{Catal2011,
  Title                    = {Software fault prediction: A literature review and current trends},
  Author                   = {Catal, Cagatay},
  Journal                  = {Expert Systems with Applications},
  Year                     = {2011},

  Month                    = {apr},
  Number                   = {4},
  Pages                    = {4626--4636},
  Volume                   = {38},

  Doi                      = {10.1016/j.eswa.2010.10.024},
  ISSN                     = {09574174},
  Url                      = {http://linkinghub.elsevier.com/retrieve/pii/S0957417410011681}
}

@Article{CC2009,
  Title                    = {A systematic review of software fault prediction studies},
  Author                   = {Cagatay Catal and Banu Diri},
  Journal                  = {Expert Systems with Applications},
  Year                     = {2009},
  Number                   = {4},
  Pages                    = {7346--7354},
  Volume                   = {36},

  Abstract                 = {This paper provides a systematic review of previous software fault prediction studies with a specific focus on metrics, methods, and datasets. The review uses 74 software fault prediction papers in 11 journals and several conference proceedings. According to the review results, the usage percentage of public datasets increased significantly and the usage percentage of machine learning algorithms increased slightly since 2005. In addition, method-level metrics are still the most dominant metrics in fault prediction research area and machine learning algorithms are still the most popular methods for fault prediction. Researchers working on software fault prediction area should continue to use public datasets and machine learning algorithms to build better fault predictors. The usage percentage of class-level is beyond acceptable levels and they should be used much more than they are now in order to predict the faults earlier in design phase of software life cycle.},
  Doi                      = {10.1016/j.eswa.2008.10.027},
  ISSN                     = {0957-4174},
  Keywords                 = {Machine learning},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0957417408007215}
}

@InProceedings{DHR2017,
  Title                    = {Preliminary Study on Applying Semi-Supervised Learning to App Store Analysis},
  Author                   = {Deocadez, Roger and Harrison, Rachel and Rodriguez, Daniel},
  Booktitle                = {Proceedings of the 21st International Conference on Evaluation and Assessment in Software Engineering (EASE'17)},
  Year                     = {2017},

  Address                  = {New York, NY, USA},
  Pages                    = {320--323},
  Publisher                = {ACM},
  Series                   = {EASE'17},

  Acmid                    = {3084285},
  Doi                      = {10.1145/3084226.3084285},
  ISBN                     = {978-1-4503-4804-1},
  Keywords                 = {Apps reviews, Mobile apps, Semi-supervised Learning},
  Location                 = {Karlskrona, Sweden},
  Numpages                 = {4},
  Url                      = {http://doi.acm.org/10.1145/3084226.3084285}
}

@Article{DOLADO2016,
  Title                    = {Evaluation of estimation models using the Minimum Interval of Equivalence},
  Author                   = {JosÃ© Javier Dolado and Daniel Rodriguez and Mark Harman and William B. Langdon and Federica Sarro},
  Journal                  = {Applied Soft Computing},
  Year                     = {2016},
  Pages                    = {956--967},
  Volume                   = {49},

  Abstract                 = {This article proposes a new measure to compare soft computing methods for software estimation. This new measure is based on the concepts of Equivalence Hypothesis Testing (EHT). Using the ideas of EHT, a dimensionless measure is defined using the Minimum Interval of Equivalence and a random estimation. The dimensionless nature of the metric allows us to compare methods independently of the data samples used. The motivation of the current proposal comes from the biases that other criteria show when applied to the comparison of software estimation methods. In this work, the level of error for comparing the equivalence of methods is set using EHT. Several soft computing methods are compared, including genetic programming, neural networks, regression and model trees, linear regression (ordinary and least mean squares) and instance-based methods. The experimental work has been performed on several publicly available datasets. Given a dataset and an estimation method we compute the upper point of Minimum Interval of Equivalence, MIEu, on the confidence intervals of the errors. Afterwards, the new measure, MIEratio, is calculated as the relative distance of the MIEu to the random estimation. Finally, the data distributions of the MIEratios are analysed by means of probability intervals, showing the viability of this approach. In this experimental work, it can be observed that there is an advantage for the genetic programming and linear regression methods by comparing the values of the intervals.},
  Doi                      = {http://dx.doi.org/10.1016/j.asoc.2016.03.026},
  ISSN                     = {1568-4946},
  Keywords                 = {Software estimations},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S1568494616301557}
}

@Article{Elish08,
  Title                    = {Predicting defect-prone software modules using support vector machines},
  Author                   = {Karim O. Elish and Mahmoud O. Elish},
  Journal                  = {Journal of Systems and Software},
  Year                     = {2008},
  Number                   = {5},
  Pages                    = {649--660},
  Volume                   = {81},

  Abstract                 = {Effective prediction of defect-prone software modules can enable software developers to focus quality assurance activities and allocate effort and resources more efficiently. Support vector machines (SVM) have been successfully applied for solving both classification and regression problems in many applications. This paper evaluates the capability of SVM in predicting defect-prone software modules and compares its prediction performance against eight statistical and machine learning models in the context of four NASA datasets. The results indicate that the prediction performance of SVM is generally better than, or at least, is competitive against the compared models.},
  Doi                      = {10.1016/j.jss.2007.07.040},
  ISSN                     = {0164-1212},
  Keywords                 = {Software metrics},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S016412120700235X}
}

@InProceedings{Gray2011,
  Title                    = {The misuse of the NASA metrics data program data sets for automated software defect prediction},
  Author                   = {D. Gray and D. Bowes and N. Davey and Y. Sun and B. Christianson},
  Booktitle                = {15th Annual Conference on Evaluation Assessment in Software Engineering (EASE 2011)},
  Year                     = {2011},
  Month                    = {April},
  Pages                    = {96--103},

  Abstract                 = {Background: The NASA Metrics Data Program data sets have been heavily used in software defect prediction experiments. Aim: To demonstrate and explain why these data sets require significant pre-processing in order to be suitable for defect prediction. Method: A meticulously documented data cleansing process involving all 13 of the original NASA data sets. Results: Post our novel data cleansing process; each of the data sets had between 6 to 90 percent less of their original number of recorded values. Conclusions: One: Researchers need to analyse the data that forms the basis of their findings in the context of how it will be used. Two: Defect prediction data sets could benefit from lower level code metrics in addition to those more commonly used, as these will help to distinguish modules, reducing the likelihood of repeated data points. Three: The bulk of defect prediction experiments based on the NASA Metrics Data Program data sets may have led to erroneous findings. This is mainly due to repeated data points potentially causing substantial amounts of training and testing data to be identical.},
  Doi                      = {10.1049/ic.2011.0012},
  Keywords                 = {data mining;fault tolerant computing;NASA metrics data program data set;automated software defect prediction;data cleansing process}
}

@Article{HallBBGC2012,
  Title                    = {A Systematic Literature Review on Fault Prediction Performance in Software Engineering},
  Author                   = {T. Hall and S. Beecham and D. Bowes and D. Gray and S. Counsell},
  Journal                  = {IEEE Transactions on Software Engineering},
  Year                     = {2012},

  Month                    = {Nov},
  Number                   = {6},
  Pages                    = {1276--1304},
  Volume                   = {38},

  Abstract                 = {Background: The accurate prediction of where faults are likely to occur in code can help direct test effort, reduce costs, and improve the quality of software. Objective: We investigate how the context of models, the independent variables used, and the modeling techniques applied influence the performance of fault prediction models. Method: We used a systematic literature review to identify 208 fault prediction studies published from January 2000 to December 2010. We synthesize the quantitative and qualitative results of 36 studies which report sufficient contextual and methodological information according to the criteria we develop and apply. Results: The models that perform well tend to be based on simple modeling techniques such as Naive Bayes or Logistic Regression. Combinations of independent variables have been used by models that perform well. Feature selection has been applied to these combinations when models are performing particularly well. Conclusion: The methodology used to build models seems to be influential to predictive performance. Although there are a set of fault prediction studies in which confidence is possible, more studies are needed that use a reliable methodology and which report their context, methodology, and performance comprehensively.},
  Doi                      = {10.1109/TSE.2011.103},
  ISSN                     = {0098-5589},
  Keywords                 = {Bayes methods;regression analysis;software fault tolerance;software quality;contextual information;cost reduction;fault prediction models;fault prediction performance;fault prediction study;feature selection;independent variables;logistic regression;methodological information;naive Bayes;predictive performance;reliable methodology;simple modeling techniques;software engineering;software quality;systematic literature review;Analytical models;Context modeling;Data models;Fault diagnosis;Predictive models;Software testing;Systematics;Systematic literature review;software fault prediction}
}

@Article{Hosseini2017,
  Title                    = {A benchmark study on the effectiveness of search-based data selection and feature selection for cross project defect prediction},
  Author                   = {Seyedrebvar Hosseini and Burak Turhan and Mika Mantyla},
  Journal                  = {Information and Software Technology},
  Year                     = {2017},
  Pages                    = {-},

  Doi                      = {https://doi.org/10.1016/j.infsof.2017.06.004},
  ISSN                     = {0950-5849},
  Keywords                 = {Cross project defect prediction},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0950584916303500}
}

@InProceedings{HuangNPGBT2011,
  Title                    = {{AutoODC}: Automated generation of Orthogonal Defect Classifications},
  Author                   = {LiGuo Huang and V. Ng and I. Persing and Ruili Geng and Xu Bai and Jeff Tian},
  Booktitle                = {26th IEEE/ACM International Conference on Automated Software Engineering (ASE'2011)},
  Year                     = {2011},
  Month                    = {Nov},
  Pages                    = {412--415},

  Abstract                 = {Orthogonal Defect Classification (ODC), the most influential framework for software defect classification and analysis, provides valuable in-process feedback to system development and maintenance. Conducting ODC classification on existing organizational defect reports is human intensive and requires experts' knowledge of both ODC and system domains. This paper presents AutoODC, an approach and tool for automating ODC classification by casting it as a supervised text classification problem. Rather than merely apply the standard machine learning framework to this task, we seek to acquire a better ODC classification system by integrating experts' ODC experience and domain knowledge into the learning process via proposing a novel Relevance Annotation Framework. We evaluated AutoODC on an industrial defect report from the social network domain. AutoODC is a promising approach: not only does it leverage minimal human effort beyond the human annotations typically required by standard machine learning approaches, but it achieves an overall accuracy of 80.2\% when using manual classifications as a basis of comparison.},
  Doi                      = {10.1109/ASE.2011.6100086},
  ISSN                     = {1938-4300},
  Keywords                 = {learning (artificial intelligence);program debugging;AutoODC;automated generation of orthogonal defect classifications;influential framework;machine learning framework;relevance annotation framework;social network;software bug;software defect analysis;software defect classification;text classification problem;Accuracy;Humans;Machine learning;Manuals;Support vector machines;Text categorization;Training;Orthogonal Defect Classification (ODC);natural language processing;text classification}
}

@InProceedings{Ibarguren2017,
  Title                    = {The Consolidated Tree Construction Algorithm in Imbalanced Defect Prediction Datasets},
  Author                   = {Igor Ibarguren and J.M. P\'erez and Javier Muguerza and Daniel Rodriguez and Rachel Harrison},
  Booktitle                = {Evolutionary Methods and Machine Learning in SE, Testing and SE Repositories. Proceedings of the 2017 IEEE Congress on Evolutionary Computation (CEC2017)},
  Year                     = {2017},
  Pages                    = {96--103}
}

@InProceedings{JureczkoS10,
  Title                    = {Using Object-Oriented Design Metrics to Predict Software Defects},
  Author                   = {Jureczko, Marian and Spinellis, Diomidis},
  Booktitle                = {Models and Methodology of System Dependability. Proceedings of {RELCOMEX} 2010 Fifth International Conference on Dependability of Computer Systems {DepCoS}},
  Year                     = {2010},

  Address                  = {Wroc{\l}aw, Poland},
  Pages                    = {69--81},
  Publisher                = {Oficyna Wydawnicza Politechniki Wroc{\l}awskiej},
  Series                   = {Monographs of System Dependability},

  ISBN                     = {978-83-7493-526-5},
  Url                      = {http://www.dmst.aueb.gr/dds/pubs/conf/2010-DepCoS-RELCOMEX-ckjm-defects/html/JS10.html}
}

@InProceedings{YasutakaEtAl:07,
  Title                    = {The effects of over and under sampling on fault--prone module detection},
  Author                   = {Y. Kamei and A. Monden and S. Matsumoto and T. Kakimoto and K. Matsumoto},
  Booktitle                = {Empirical Software Engineering and Measurement (ESEM)},
  Year                     = {2007},
  Pages                    = {196--204}
}

@Article{02Kho,
  Title                    = {Using regression trees to classify fault-prone software modules},
  Author                   = {T. Khoshgoftaar and E. Allen and J. Deng},
  Journal                  = {IEEE Transactions on Reliability},
  Year                     = {2002},

  Optnumber                = {4},
  Optvolume                = {51}
}

@Article{KhoshgoftaarEtAl:05,
  Title                    = {Assessment of a New Three-Group Software Quality Classification Technique: An Empirical Case Study},
  Author                   = {T.M. Khoshgoftaar and N. Seliya and K. Gao},
  Journal                  = {Empirical Software Engineering},
  Year                     = {2005},
  Number                   = {2},
  Pages                    = {183-218},
  Volume                   = {10}
}

@Article{Khoshgoftaar97,
  Title                    = {Application of Neural Networks to Software Quality Modeling of a Very Large Telecommunications System},
  Author                   = {T. M. Khoshgoftaar and E. Allen and J. Hudepohl and S. Aud},
  Journal                  = {IEEE Transactions on Neural Networks},
  Year                     = {1997},
  Number                   = {4},
  Pages                    = {902--909},
  Volume                   = {8}
}

@InProceedings{KhoshgoftaarGKN12,
  Title                    = {Exploring an iterative feature selection technique for highly imbalanced data sets},
  Author                   = {Khoshgoftaar, Taghi M. and Gao, Kehan and Napolitano, Amri},
  Booktitle                = {IEEE 13th International Conference on Information Reuse and Integration (IRI'2012)},
  Year                     = {2012},
  Month                    = {aug.},
  Pages                    = {101--108},

  Abstract                 = {The quality of a classification model is affected by two factors in a training data set: (1) the presence of excessive features and (2) the presence of imbalanced distributions between two classes in a binary classification problem. This paper presents an iterative feature selection method to deal with these two problems. The proposed method consists of an iterative process of data sampling followed by feature ranking and finally aggregating the results generated during the iterative process. In this study, we investigate a number of feature ranking techniques and a data sampling method with two different post-sampling proportions between the two classes. We compare the iterative feature selection technique to the one where a data sampling and a feature ranking technique are used together but only once (without iteration). The empirical study is carried out on two groups of highly imbalanced data sets from a real-world software system. The results demonstrate that our proposed iterative feature selection technique performs on average better than the method without iteration.},
  Doi                      = {10.1109/IRI.2012.6302997}
}

@Article{Khoshgoftaar2004,
  Title                    = {Comparative Assessment of Software Quality Classification Techniques: An Empirical Case Study},
  Author                   = {Taghi M. Khoshgoftaar and Naeem Seliya},
  Journal                  = {Empirical Software Engineering},
  Year                     = {2004},

  Month                    = sep,
  Number                   = {3},
  Pages                    = {229--257},
  Volume                   = {9},

  Acmid                    = {990393},
  Address                  = {Hingham, MA, USA},
  Doi                      = {10.1023/B:EMSE.0000027781.18360.9b},
  ISSN                     = {1382-3256},
  Issue_date               = {September 2004},
  Keywords                 = {Software quality classification, analysis of variance, case-based reasoning, decision trees, expected cost of misclassification, logistic regression},
  Numpages                 = {29},
  Publisher                = {Kluwer Academic Publishers},
  Url                      = {http://dx.doi.org/10.1023/B:EMSE.0000027781.18360.9b}
}

@Article{Khoshgoftaar03,
  Title                    = {Analogy-Based Practical Classification Rules for Software Quality Estimation},
  Author                   = {Taghi M. Khoshgoftaar and Naeem Seliya},
  Journal                  = {Empirical Software Engineering},
  Year                     = {2003},
  Number                   = {4},
  Pages                    = {325--350},
  Volume                   = {8},

  Address                  = {Hingham, MA, USA},
  Doi                      = {http://dx.doi.org/10.1023/A:1025316301168},
  ISSN                     = {1382-3256},
  Publisher                = {Kluwer Academic Publishers}
}

@Article{LBMP08,
  Title                    = {Benchmarking Classification Models for Software Defect Prediction: A Proposed Framework and Novel Findings},
  Author                   = {Lessmann, S. and Baesens, B. and Mues, C. and Pietsch, S.},
  Journal                  = {IEEE Transactions on Software Engineering},
  Year                     = {2008},

  Month                    = {July-Aug.},
  Number                   = {4},
  Pages                    = {485--496},
  Volume                   = {34},

  Abstract                 = {Software defect prediction strives to improve software quality and testing efficiency by constructing predictive classification models from code attributes to enable a timely identification of fault-prone modules. Several classification models have been evaluated for this task. However, due to inconsistent findings regarding the superiority of one classifier over another and the usefulness of metric-based classification in general, more research is needed to improve convergence across studies and further advance confidence in experimental results. We consider three potential sources for bias: comparing classifiers over one or a small number of proprietary data sets, relying on accuracy indicators that are conceptually inappropriate for software defect prediction and cross-study comparisons, and, finally, limited use of statistical testing procedures to secure empirical findings. To remedy these problems, a framework for comparative software defect prediction experiments is proposed and applied in a large-scale empirical comparison of 22 classifiers over 10 public domain data sets from the NASA Metrics Data repository. Overall, an appealing degree of predictive accuracy is observed, which supports the view that metric-based classification is useful. However, our results indicate that the importance of the particular classification algorithm may be less than previously assumed since no significant performance differences could be detected among the top 17 classifiers.},
  Doi                      = {10.1109/TSE.2008.35},
  ISSN                     = {0098-5589},
  Keywords                 = {benchmarking classification models;code attributes;fault-prone modules;metric-based classification;predictive classification models;proprietary data sets;software defect prediction;software quality;statistical testing procedures;testing efficiency;benchmark testing;software quality;statistical testing;}
}

@Article{Li2012,
  Title                    = {Sample-based software defect prediction with active and semi-supervised learning},
  Author                   = {Li, Ming
and Zhang, Hongyu
and Wu, Rongxin
and Zhou, Zhi-Hua},
  Journal                  = {Automated Software Engineering},
  Year                     = {2012},

  Month                    = {Jun},
  Number                   = {2},
  Pages                    = {201--230},
  Volume                   = {19},

  Abstract                 = {Software defect prediction can help us better understand and control software quality. Current defect prediction techniques are mainly based on a sufficient amount of historical project data. However, historical data is often not available for new projects and for many organizations. In this case, effective defect prediction is difficult to achieve. To address this problem, we propose sample-based methods for software defect prediction. For a large software system, we can select and test a small percentage of modules, and then build a defect prediction model to predict defect-proneness of the rest of the modules. In this paper, we describe three methods for selecting a sample: random sampling with conventional machine learners, random sampling with a semi-supervised learner and active sampling with active semi-supervised learner. To facilitate the active sampling, we propose a novel active semi-supervised learning method ACoForest which is able to sample the modules that are most helpful for learning a good prediction model. Our experiments on PROMISE datasets show that the proposed methods are effective and have potential to be applied to industrial practice.},
  Day                      = {01},
  Doi                      = {10.1007/s10515-011-0092-1},
  ISSN                     = {1573-7535},
  Url                      = {http://dx.doi.org/10.1007/s10515-011-0092-1}
}

@InProceedings{LuCC2012,
  Title                    = {Software defect prediction using semi-supervised learning with dimension reduction},
  Author                   = {H. Lu and B. Cukic and M. Culp},
  Booktitle                = {27th IEEE/ACM International Conference on Automated Software Engineering (ASE'12)},
  Year                     = {2012},
  Month                    = {Sept},
  Pages                    = {314--317},

  Abstract                 = {Accurate detection of fault prone modules offers the path to high quality software products while minimizing non essential assurance expenditures. This type of quality modeling requires the availability of software modules with known fault content developed in similar environment. Establishing whether a module contains a fault or not can be expensive. The basic idea behind semi-supervised learning is to learn from a small number of software modules with known fault content and supplement model training with modules for which the fault information is not available. In this study, we investigate the performance of semi-supervised learning for software fault prediction. A preprocessing strategy, multidimensional scaling, is embedded in the approach to reduce the dimensional complexity of software metrics. Our results show that the semi-supervised learning algorithm with dimension-reduction preforms significantly better than one of the best performing supervised learning algorithms, random forest, in situations when few modules with known fault content are available for training.},
  Doi                      = {10.1145/2351676.2351734},
  Keywords                 = {learning (artificial intelligence);software metrics;software quality;dimension reduction;fault content;fault prone module detection;high quality software products;model training;multidimensional scaling;preprocessing strategy;random forest;semisupervised learning;software defect prediction;software fault prediction;software metrics dimensional complexity;software modules availability;Software fault prediction;dimension reduction;semi-supervised learning;software metrics}
}

@Article{Madeyski2015,
  Title                    = {Which process metrics can significantly improve defect prediction models? An empirical study},
  Author                   = {Madeyski, Lech and Jureczko, Marian},
  Journal                  = {Software Quality Journal},
  Year                     = {2015},
  Number                   = {3},
  Pages                    = {393--422},
  Volume                   = {23},

  Abstract                 = {The knowledge about the software metrics which serve as defect indicators is vital for the efficient allocation of resources for quality assurance. It is the process metrics, although sometimes difficult to collect, which have recently become popular with regard to defect prediction. However, in order to identify rightly the process metrics which are actually worth collecting, we need the evidence validating their ability to improve the product metric-based defect prediction models. This paper presents an empirical evaluation in which several process metrics were investigated in order to identify the ones which significantly improve the defect prediction models based on product metrics. Data from a wide range of software projects (both, industrial and open source) were collected. The predictions of the models that use only product metrics (simple models) were compared with the predictions of the models which used product metrics, as well as one of the process metrics under scrutiny (advanced models). To decide whether the improvements were significant or not, statistical tests were performed and effect sizes were calculated. The advanced defect prediction models trained on a data set containing product metrics and additionally Number of Distinct Committers (NDC) were significantly better than the simple models without NDC, while the effect size was medium and the probability of superiority (PS) of the advanced models over simple ones was high p=.016, which is a substantial finding useful in defect prediction. A similar result with slightly smaller PS was achieved by the advanced models trained on a data set containing product metrics and additionally all of the investigated process metrics p=.038. The advanced models trained on a data set containing product metrics and additionally Number of Modified Lines (NML) were significantly better than the simple models without NML, but the effect size was small p=.038. Hence, it is reasonable to recommend the NDC process metric in building the defect prediction models.},
  Doi                      = {10.1007/s11219-014-9241-7},
  ISSN                     = {1573-1367},
  Url                      = {http://dx.doi.org/10.1007/s11219-014-9241-7}
}

@Article{Menzies07b,
  Title                    = {Problems with Precision: A Response to Comments on Data Mining Static Code Attributes to Learn Defect Predictors},
  Author                   = {Tim Menzies and Alex Dekhtyar and Justin Distefano and Jeremy Greenwald},
  Journal                  = {IEEE Transactions on Software Engineering},
  Year                     = {2007},
  Number                   = {9},
  Pages                    = {637--640},
  Volume                   = {33},

  Address                  = {Los Alamitos, CA, USA},
  Doi                      = {http://doi.ieeecomputersociety.org/10.1109/TSE.2007.70721},
  ISSN                     = {0098-5589},
  Publisher                = {IEEE Computer Society}
}

@Article{Menzies07,
  Title                    = {Data Mining Static Code Attributes to Learn Defect Predictors},
  Author                   = {T. Menzies and J. Greenwald and A. Frank},
  Journal                  = {IEEE Transactions on Software Engineering},
  Year                     = {2007},

  Optmonth                 = {January},
  Optnumber                = {1},
  Optpages                 = {2--13},
  Optvolume                = {33}
}

@InProceedings{Morasca2016,
  Title                    = {Slope-based Fault-proneness Thresholds for Software Engineering Measures},
  Author                   = {Morasca, Sandro and Lavazza, Luigi},
  Booktitle                = {Proceedings of the 20th International Conference on Evaluation and Assessment in Software Engineering},
  Year                     = {2016},

  Address                  = {New York, NY, USA},
  Pages                    = {12:1--12:10},
  Publisher                = {ACM},
  Series                   = {EASE '16},

  Acmid                    = {2915997},
  Articleno                = {12},
  Doi                      = {10.1145/2915970.2915997},
  ISBN                     = {978-1-4503-3691-8},
  Keywords                 = {fault-proneness, faultiness, logistic regression, probit regression, software measures, threshold},
  Location                 = {Limerick, Ireland},
  Numpages                 = {10},
  Url                      = {http://doi.acm.org/10.1145/2915970.2915997}
}

@InProceedings{MoserPS2008,
  Title                    = {A comparative analysis of the efficiency of change metrics and static code attributes for defect prediction},
  Author                   = {R. Moser and W. Pedrycz and G. Succi},
  Booktitle                = {2008 ACM/IEEE 30th International Conference on Software Engineering},
  Year                     = {2008},
  Month                    = {May},
  Pages                    = {181-190},

  Abstract                 = {In this paper we present a comparative analysis of the predictive power of two different sets of metrics for defect prediction. We choose one set of product related and one set of process related software metrics and use them for classifying Java files of the Eclipse project as defective respective defect-free. Classification models are built using three common machine learners: logistic regression, naive Bayes, and decision trees. To allow different costs for prediction errors we perform cost-sensitive classification, which proves to be very successful: >75\% percentage of correctly classified files, a recall of >80\%, and a false positive rate <30\%. Results indicate that for the Eclipse data, process metrics are more efficient defect predictors than code metrics.},
  Doi                      = {10.1145/1368088.1368114},
  ISSN                     = {0270-5257},
  Keywords                 = {Bayes methods;Java;decision trees;learning (artificial intelligence);pattern classification;program diagnostics;regression analysis;software metrics;software quality;Eclipse project;Java file classification;change metrics;comparative analysis;cost-sensitive classification;decision tree;defect prediction;logistic regression;machine learning;naive Bayes method;process related software metrics;product related software metrics;software quality;static code attribute;Classification tree analysis;Costs;Java;Logistics;Permission;Predictive models;Resource management;Software engineering;Software metrics;Testing;cost-sensitive classification;defect prediction;software metrics}
}

@InProceedings{NZZH12,
  Title                    = {Change Bursts as Defect Predictors},
  Author                   = {Nachiappan Nagappan and Andreas Zeller and Thomas Zimmermann and Kim Herzig and Brendan Murphy},
  Booktitle                = {21st IEEE International Symposium on Software Reliability Engineering (ISSRE 2012)},
  Year                     = {2012},
  Month                    = {November},

  Location                 = {San Jose, California, USA}
}

@InProceedings{NZZH10,
  Title                    = {Change Bursts as Defect Predictors},
  Author                   = {Nachiappan Nagappan and Andreas Zeller and Thomas Zimmermann and Kim Herzig and Brendan Murphy},
  Booktitle                = {Proceedings of the 21st IEEE International Symposium on Software Reliability Engineering (ISSRE 2012)},
  Year                     = {2010},
  Month                    = {November},

  Location                 = {San Jose, California, USA}
}

@InProceedings{Panichella2014,
  Title                    = {Cross-project defect prediction models: L'Union fait la force},
  Author                   = {A. Panichella and R. Oliveto and A. De Lucia},
  Booktitle                = {2014 Software Evolution Week - IEEE Conference on Software Maintenance, Reengineering, and Reverse Engineering (CSMR-WCRE)},
  Year                     = {2014},
  Month                    = {Feb},
  Pages                    = {164--173},

  Abstract                 = {Existing defect prediction models use product or process metrics and machine learning methods to identify defect-prone source code entities. Different classifiers (e.g., linear regression, logistic regression, or classification trees) have been investigated in the last decade. The results achieved so far are sometimes contrasting and do not show a clear winner. In this paper we present an empirical study aiming at statistically analyzing the equivalence of different defect predictors. We also propose a combined approach, coined as CODEP (COmbined DEfect Predictor), that employs the classification provided by different machine learning techniques to improve the detection of defect-prone entities. The study was conducted on 10 open source software systems and in the context of cross-project defect prediction, that represents one of the main challenges in the defect prediction field. The statistical analysis of the results indicates that the investigated classifiers are not equivalent and they can complement each other. This is also confirmed by the superior prediction accuracy achieved by CODEP when compared to stand-alone defect predictors.},
  Doi                      = {10.1109/CSMR-WCRE.2014.6747166},
  Keywords                 = {learning (artificial intelligence);pattern classification;program debugging;public domain software;statistical analysis;CODEP;L'Union fait la force;classification;combined defect predictor;cross-project defect prediction models;defect-prone entity detection;machine learning techniques;open source software systems;statistical analysis;Accuracy;Context;Logistics;Measurement;Predictive models;Regression tree analysis;Software}
}

@InProceedings{RRCA07,
  Title                    = {Detecting Fault Modules Applying Feature Selection to Classifiers},
  Author                   = {Rodriguez, D. and Ruiz, R. and Cuadrado, J. and Aguilar-Ruiz, J.},
  Booktitle                = {IEEE International Conference on Information Reuse and Integration (IRI 2007)},
  Year                     = {2007},
  Month                    = {aug.},
  Pages                    = {667--672},

  Doi                      = {10.1109/IRI.2007.4296696},
  Keywords                 = {PROMISE repository;attribute selection techniques;automated data collection tools;classifier learning;data mining algorithms;fault module detection;feature selection;project management;software engineering databases;data mining;feature extraction;learning (artificial intelligence);pattern classification;project management;software management;}
}

@Article{Rodriguez2012,
  Title                    = {Searching for rules to detect defective modules: A subgroup discovery approach},
  Author                   = {D. Rodriguez and R. Ruiz and J.C. Riquelme and J.S. Aguilar-Ruiz},
  Journal                  = {Information Sciences},
  Year                     = {2012},
  Pages                    = {14--30},
  Volume                   = {191},

  Abstract                 = {Data mining methods in software engineering are becoming increasingly important as they can support several aspects of the software development life-cycle such as quality. In this work, we present a data mining approach to induce rules extracted from static software metrics characterising fault-prone modules. Due to the special characteristics of the defect prediction data (imbalanced, inconsistency, redundancy) not all classification algorithms are capable of dealing with this task conveniently. To deal with these problems, Subgroup Discovery (SD) algorithms can be used to find groups of statistically different data given a property of interest. We propose EDER-SD (Evolutionary Decision Rules for Subgroup Discovery), a SD algorithm based on evolutionary computation that induces rules describing only fault-prone modules. The rules are a well-known model representation that can be easily understood and applied by project managers and quality engineers. Thus, rules can help them to develop software systems that can be justifiably trusted. Contrary to other approaches in SD, our algorithm has the advantage of working with continuous variables as the conditions of the rules are defined using intervals. We describe the rules obtained by applying our algorithm to seven publicly available datasets from the PROMISE repository showing that they are capable of characterising subgroups of fault-prone modules. We also compare our results with three other well known SD algorithms and the EDER-SD algorithm performs well in most cases.},
  Doi                      = {10.1016/j.ins.2011.01.039},
  ISSN                     = {0020-0255},
  Keywords                 = {Defect prediction},
  Url                      = {http://www.sciencedirect.com/science/article/pii/S0020025511000661}
}

@InProceedings{SeiffertEtAl07,
  Title                    = {An empirical study of the classification performance of learners on imbalanced and noisy software quality data},
  Author                   = {C. Seiffert and T.M. Khoshgoftaar and J. Van Hulse and A. Folleco},
  Booktitle                = {2007 IEEE International Conference on Information Reuse and Integration, IEEE IRI-2007},
  Year                     = {2007},
  Pages                    = {651--658}
}

@Article{Shatnawi2010,
  Title                    = {Finding software metrics threshold values using ROC curves},
  Author                   = {Shatnawi, Raed and Li, Wei and Swain, James and Newman, Tim},
  Journal                  = {Journal of Software Maintenance and Evolution: Research and Practice},
  Year                     = {2010},
  Number                   = {1},
  Pages                    = {1--16},
  Volume                   = {22},

  Abstract                 = {An empirical study of the relationship between object-oriented (OO) metrics and error-severity categories is presented. The focus of the study is to identify threshold values of software metrics using receiver operating characteristic curves. The study used the three releases of the Eclipse project and found threshold values for some OO metrics that separated no-error classes from classes that had high-impact errors. Although these thresholds cannot predict whether a class will definitely have errors in the future, they can provide a more scientific method to assess class error proneness and can be used by engineers easily.},
  Doi                      = {10.1002/smr.404},
  ISSN                     = {1532-0618},
  Keywords                 = {object-oriented design, object-oriented metrics, thresholds, ROC curve},
  Publisher                = {John Wiley \& Sons, Ltd.},
  Url                      = {http://dx.doi.org/10.1002/smr.404}
}

@Article{Shepperd2013,
  Title                    = {Data Quality: Some Comments on the NASA Software Defect Datasets},
  Author                   = {Shepperd, M. and Qinbao Song and Zhongbin Sun and Mair, C.},
  Journal                  = {IEEE Transactions on Software Engineering},
  Year                     = {2013},

  Month                    = {Sept},
  Number                   = {9},
  Pages                    = {1208--1215},
  Volume                   = {39},

  Doi                      = {10.1109/TSE.2013.11},
  ISSN                     = {0098-5589},
  Keywords                 = {data analysis;learning (artificial intelligence);pattern classification;software reliability;IEEE Transactions on Software Engineering;
 NASA software defect dataset;National Aeronautics and Space Administration;data preprocessing;data quality;data replication;
 dataset provenance;defect-prone classification;machine learning;not-defect-prone classification;
 software module classification;Abstracts;Communities;Educational institutions;NASA;PROM;Software;Sun;Empirical software engineering;
 data quality;defect prediction;machine learning}
}

@Article{SinghEDtAl2010,
  Title                    = {Empirical validation of object-oriented metrics for predicting fault proneness models},
  Author                   = {Singh, Yogesh and Kaur, Arvinder and Malhotra, Ruchika},
  Journal                  = {Software Quality Journal},
  Year                     = {2010},
  Note                     = {10.1007/s11219-009-9079-6},
  Pages                    = {3--35},
  Volume                   = {18},

  Abstract                 = {Empirical validation of software metrics used to predict software quality attributes is important to ensure their practical relevance in software organizations. The aim of this work is to find the relation of object-oriented (OO) metrics with fault proneness at different severity levels of faults. For this purpose, different prediction models have been developed using regression and machine learning methods. We evaluate and compare the performance of these methods to find which method performs better at different severity levels of faults and empirically validate OO metrics given by Chidamber and Kemerer. The results of the empirical study are based on public domain NASA data set. The performance of the predicted models was evaluated using Receiver Operating Characteristic (ROC) analysis. The results show that the area under the curve (measured from the ROC analysis) of models predicted using high severity faults is low as compared with the area under the curve of the model predicted with respect to medium and low severity faults. However, the number of faults in the classes correctly classified by predicted models with respect to high severity faults is not low. This study also shows that the performance of machine learning methods is better than logistic regression method with respect to all the severities of faults. Based on the results, it is reasonable to claim that models targeted at different severity levels of faults could help for planning and executing testing by focusing resources on fault-prone parts of the design and code that are likely to cause serious failures.},
  Affiliation              = {University School of Information Technology, GGS Indraprastha University Delhi 110403 India},
  ISSN                     = {0963-9314},
  Issue                    = {1},
  Keyword                  = {Computer Science},
  Publisher                = {Springer Netherlands},
  Url                      = {http://dx.doi.org/10.1007/s11219-009-9079-6}
}

@Article{SongTSE96,
  Title                    = {Software defect association mining and defect correction effort prediction},
  Author                   = {Q. Song and M. Shepperd and M. Cartwright and C. Mair},
  Journal                  = {IEEE Transactions on Software Engineering},
  Year                     = {2006},

  Month                    = {Feb.},
  Number                   = {2},
  Pages                    = {69--82},
  Volume                   = {32},

  Doi                      = {10.1109/TSE.2006.1599417},
  ISSN                     = {0098-5589},
  Keywords                 = {Accuracy;Association rules;Data mining;Inspection;Job shop scheduling;Project management;Resource management;Software development management;Software quality;Software systems; data mining; program testing; software process improvement; software quality; SEL defect data; association rule mining; defect correction effort prediction; software defect association prediction; software quality; Software defect prediction; defect association; defect correction effort.; defect isolation effort;}
}

@Article{TurhanESE12,
  Title                    = {On the dataset shift problem in software engineering prediction models},
  Author                   = {Turhan, Burak},
  Journal                  = {Empirical Software Engineering},
  Year                     = {2012},
  Note                     = {10.1007/s10664-011-9182-8},
  Pages                    = {62--74},
  Volume                   = {17},

  Affiliation              = {Department of Information Processing Science, University of Oulu, POB.3000, 90014 Oulu, Finland},
  ISSN                     = {1382-3256},
  Issue                    = {1},
  Keyword                  = {Computer Science},
  Publisher                = {Springer Netherlands}
}

@InProceedings{VanHulseEtAl:2007,
  Title                    = {Experimental perspectives on learning from imbalanced data},
  Author                   = {J. {Van Hulse} and T.~M. Khoshgoftaar and A. Napolitano},
  Booktitle                = {Proceedings of the 24th International Conference on Machine Learning (ICML07)},
  Year                     = {2007},

  Address                  = {New York, USA},
  Publisher                = {ACM}
}

@Article{Vandecruys2008,
  Title                    = {Mining software repositories for comprehensible software fault prediction models},
  Author                   = {Olivier Vandecruys and David Martens and Bart Baesens and Christophe Mues and Manu {De Backer} and Raf Haesen},
  Journal                  = {Journal of Systems and Software},
  Year                     = {2008},
  Number                   = {5},
  Pages                    = {823--839},
  Volume                   = {81},

  Abstract                 = {Software managers are routinely confronted with software projects that contain errors or inconsistencies and exceed budget and time limits. By mining software repositories with comprehensible data mining techniques, predictive models can be induced that offer software managers the insights they need to tackle these quality and budgeting problems in an efficient way. This paper deals with the role that the Ant Colony Optimization (ACO)-based classification technique AntMiner+ can play as a comprehensible data mining technique to predict erroneous software modules. In an empirical comparison on three real-world public datasets, the rule-based models produced by AntMiner+ are shown to achieve a predictive accuracy that is competitive to that of the models induced by several other included classification techniques, such as C4.5, logistic regression and support vector machines. In addition, we will argue that the intuitiveness and comprehensibility of the AntMiner+ models can be considered superior to the latter models.},
  Doi                      = {DOI: 10.1016/j.jss.2007.07.034},
  ISSN                     = {0164-1212},
  Keywords                 = {Classification},
  Url                      = {http://www.sciencedirect.com/science/article/B6V0N-4PHSC3R-2/2/129ce670cc51b090011f548066bb5711}
}

@InProceedings{WangKWN12,
  Title                    = {A Comparative Study on the Stability of Software Metric Selection Techniques},
  Author                   = {Huanjing Wang and Taghi M. Khoshgoftaar and Randall Wald and Amri Napolitano},
  Booktitle                = {11th International Conference on Machine Learning and Applications, ICMLA},
  Year                     = {2012},
  Pages                    = {301--307},

  Ee                       = {http://dx.doi.org/10.1109/ICMLA.2012.142}
}

@Book{WFH11,
  Title                    = {Data Mining: Practical machine learning tools and techniques},
  Author                   = {I.H. Witten and E. Frank and M.A. Hall},
  Publisher                = {Morgan Kaufmann},
  Year                     = {2011},

  Address                  = {San Francisco},
  Edition                  = {3rd Edition}
}

@Book{Wohlin2000ESE,
  Title                    = {Experimentation in software engineering: an introduction},
  Author                   = {Wohlin, Claes and Runeson, Per and H\"{o}st, Martin and Ohlsson, Magnus C. and Regnell, Bj\"{o}orn and Wessl{\'e}n, Anders},
  Publisher                = {Kluwer Academic Publishers},
  Year                     = {2000},

  Address                  = {Norwell, MA, USA},

  ISBN                     = {0-7923-8682-5}
}

@InBook{Wrobel01,
  Title                    = {Relational Data Mining},
  Author                   = {Stefan Wrobel},
  Chapter                  = {An algorithm for multi-relational discovery of subgroups},
  Editor                   = {Saso Dzeroski and Nada Lavra\u{c}},
  Pages                    = {74--101},
  Publisher                = {Springer},
  Year                     = {2001}
}

@InProceedings{Wrobel97,
  Title                    = {An algorithm for multi-relational discovery of subgroups},
  Author                   = {Stefan Wrobel},
  Booktitle                = {Proceedings of the 1st European Symposium on Principles of Data Mining},
  Year                     = {1997},
  Pages                    = {78--87}
}

@InProceedings{Zhang2009,
  Title                    = {An investigation of the relationships between lines of code and defects},
  Author                   = {H. Zhang},
  Booktitle                = {IEEE International Conference on Software Maintenance},
  Year                     = {2009},
  Month                    = {Sept},
  Pages                    = {274--283},

  Abstract                 = {It is always desirable to understand the quality of a software system based on static code metrics. In this paper, we analyze the relationships between lines of code (LOC) and defects (including both pre-release and post-release defects). We confirm the ranking ability of LOC discovered by Fenton and Ohlsson. Furthermore, we find that the ranking ability of LOC can be formally described using Weibull functions. We can use defect density values calculated from a small percentage of largest modules to predict the number of total defects accurately. We also find that, given LOC we can predict the number of defective components reasonably well using typical classification techniques. We perform an extensive experiment using the public Eclipse dataset, and replicate the study using the NASA dataset. Our results confirm that simple static code attributes such as LOC can be useful predictors of software quality.},
  Doi                      = {10.1109/ICSM.2009.5306304},
  ISSN                     = {1063-6773},
  Keywords                 = {program diagnostics;software metrics;software quality;Eclipse dataset;lines of code;post-release defects;pre-release defects;software system quality;static code metrics;Density measurement;Lab-on-a-chip;Laboratories;NASA;Packaging machines;Predictive models;Software metrics;Software quality;Software systems;System testing}
}

@Article{Zhang07,
  Title                    = {Comments on "Data Mining Static Code Attributes to Learn Defect Predictors"},
  Author                   = {Hongyu Zhang and Xiuzhen Zhang},
  Journal                  = {IEEE Transactions on Software Engineering},
  Year                     = {2007},
  Number                   = {9},
  Pages                    = {635--637},
  Volume                   = {33},

  Address                  = {Los Alamitos, CA, USA},
  Doi                      = {http://doi.ieeecomputersociety.org/10.1109/TSE.2007.70706},
  ISSN                     = {0098-5589},
  Publisher                = {IEEE Computer Society}
}

@InProceedings{Zimmermann2009,
  Title                    = {Cross-project Defect Prediction: A Large Scale Experiment on Data vs. Domain vs. Process},
  Author                   = {Zimmermann, Thomas and Nagappan, Nachiappan and Gall, Harald and Giger, Emanuel and Murphy, Brendan},
  Booktitle                = {Proceedings of the the 7th Joint Meeting of the European Software Engineering Conference and the ACM SIGSOFT Symposium on The Foundations of Software Engineering},
  Year                     = {2009},

  Address                  = {New York, NY, USA},
  Pages                    = {91--100},
  Publisher                = {ACM},
  Series                   = {ESEC/FSE'09},

  Acmid                    = {1595713},
  Doi                      = {10.1145/1595696.1595713},
  ISBN                     = {978-1-60558-001-2},
  Keywords                 = {churn, cross-project, decision trees, defect prediction, logistic regression, prediction quality},
  Location                 = {Amsterdam, The Netherlands},
  Numpages                 = {10},
  Url                      = {http://doi.acm.org/10.1145/1595696.1595713}
}

@InProceedings{Zimmermann07Eclipse,
  Title                    = {Predicting Defects for Eclipse},
  Author                   = {Zimmermann, T. and Premraj, R. and Zeller, A.},
  Booktitle                = {International Workshop on Predictor Models in Software Engineering (PROMISE'07)},
  Year                     = {2007},
  Month                    = {may},
  Pages                    = {9},

  Abstract                 = {We have mapped defects from the bug database of eclipse (one of the largest open-source projects) to source code locations. The resulting data set lists the number of pre- and post-release defects for every package and file in the eclipse releases 2.0, 2.1, and 3.0. We additionally annotated the data with common complexity metrics. All data is publicly available and can serve as a benchmark for defect prediction models.},
  Doi                      = {10.1109/PROMISE.2007.10},
  Keywords                 = {Eclipse;bug database;common complexity metrics;defect prediction models;open-source projects;source code locations;program debugging;public domain software;}
}

