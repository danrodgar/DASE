<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Chapter 10 Regression | Data Analysis in Software Engineering using R</title>
  <meta name="description" content="DASE Data Analysis in Software Engineering">
  <meta name="generator" content="bookdown  and GitBook 2.6.7">

  <meta property="og:title" content="Chapter 10 Regression | Data Analysis in Software Engineering using R" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="DASE Data Analysis in Software Engineering" />
  <meta name="github-repo" content="danrodgar/DASE" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 10 Regression | Data Analysis in Software Engineering using R" />
  
  <meta name="twitter:description" content="DASE Data Analysis in Software Engineering" />
  

<meta name="author" content="Daniel Rodriguez and Javier Dolado">


<meta name="date" content="2018-12-26">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="supervised-classification.html">
<link rel="next" href="unsupervised-or-descriptive-modeling.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />









<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Data Analysis in Software Engineering with R</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Welcome</a></li>
<li class="part"><span><b>I Introduction to the R Language</b></span></li>
<li class="chapter" data-level="1" data-path="r-intro.html"><a href="r-intro.html"><i class="fa fa-check"></i><b>1</b> Introduction to R</a><ul>
<li class="chapter" data-level="1.1" data-path="r-intro.html"><a href="r-intro.html#installation"><i class="fa fa-check"></i><b>1.1</b> Installation</a></li>
<li class="chapter" data-level="1.2" data-path="r-intro.html"><a href="r-intro.html#r-and-rstudio"><i class="fa fa-check"></i><b>1.2</b> R and RStudio</a></li>
<li class="chapter" data-level="1.3" data-path="r-intro.html"><a href="r-intro.html#basic-data-types"><i class="fa fa-check"></i><b>1.3</b> Basic Data Types</a><ul>
<li class="chapter" data-level="1.3.1" data-path="r-intro.html"><a href="r-intro.html#mising-values"><i class="fa fa-check"></i><b>1.3.1</b> Mising values</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="r-intro.html"><a href="r-intro.html#vectors"><i class="fa fa-check"></i><b>1.4</b> Vectors</a><ul>
<li class="chapter" data-level="1.4.1" data-path="r-intro.html"><a href="r-intro.html#coercion-for-vectors"><i class="fa fa-check"></i><b>1.4.1</b> Coercion for vectors</a></li>
<li class="chapter" data-level="1.4.2" data-path="r-intro.html"><a href="r-intro.html#vector-arithmetic"><i class="fa fa-check"></i><b>1.4.2</b> Vector arithmetic</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="r-intro.html"><a href="r-intro.html#arrays-and-matrices"><i class="fa fa-check"></i><b>1.5</b> Arrays and Matrices</a></li>
<li class="chapter" data-level="1.6" data-path="r-intro.html"><a href="r-intro.html#factors"><i class="fa fa-check"></i><b>1.6</b> Factors</a></li>
<li class="chapter" data-level="1.7" data-path="r-intro.html"><a href="r-intro.html#lists"><i class="fa fa-check"></i><b>1.7</b> Lists</a></li>
<li class="chapter" data-level="1.8" data-path="r-intro.html"><a href="r-intro.html#data-frames"><i class="fa fa-check"></i><b>1.8</b> Data frames</a></li>
<li class="chapter" data-level="1.9" data-path="r-intro.html"><a href="r-intro.html#reading-data"><i class="fa fa-check"></i><b>1.9</b> Reading Data</a></li>
<li class="chapter" data-level="1.10" data-path="r-intro.html"><a href="r-intro.html#plots"><i class="fa fa-check"></i><b>1.10</b> Plots</a></li>
<li class="chapter" data-level="1.11" data-path="r-intro.html"><a href="r-intro.html#flow-of-control"><i class="fa fa-check"></i><b>1.11</b> Flow of Control</a></li>
<li class="chapter" data-level="1.12" data-path="r-intro.html"><a href="r-intro.html#rattle"><i class="fa fa-check"></i><b>1.12</b> Rattle</a></li>
</ul></li>
<li class="part"><span><b>II Introduction to Data Mining</b></span></li>
<li class="chapter" data-level="2" data-path="what-is-data-mining-knowledge-discovery-in-databases-kdd.html"><a href="what-is-data-mining-knowledge-discovery-in-databases-kdd.html"><i class="fa fa-check"></i><b>2</b> What is Data Mining / Knowledge Discovery in Databases (KDD)</a><ul>
<li class="chapter" data-level="2.1" data-path="what-is-data-mining-knowledge-discovery-in-databases-kdd.html"><a href="what-is-data-mining-knowledge-discovery-in-databases-kdd.html#the-aim-of-data-analysis-and-statistical-learning"><i class="fa fa-check"></i><b>2.1</b> The Aim of Data Analysis and Statistical Learning</a></li>
<li class="chapter" data-level="2.2" data-path="what-is-data-mining-knowledge-discovery-in-databases-kdd.html"><a href="what-is-data-mining-knowledge-discovery-in-databases-kdd.html#basic-references"><i class="fa fa-check"></i><b>2.2</b> Basic References</a></li>
<li class="chapter" data-level="2.3" data-path="what-is-data-mining-knowledge-discovery-in-databases-kdd.html"><a href="what-is-data-mining-knowledge-discovery-in-databases-kdd.html#data-mining-with-r"><i class="fa fa-check"></i><b>2.3</b> Data Mining with R</a></li>
<li class="chapter" data-level="2.4" data-path="what-is-data-mining-knowledge-discovery-in-databases-kdd.html"><a href="what-is-data-mining-knowledge-discovery-in-databases-kdd.html#data-mining-with-weka"><i class="fa fa-check"></i><b>2.4</b> Data Mining with Weka</a></li>
</ul></li>
<li class="part"><span><b>III Data Sources and Metrics and Standards in Software Engineering Defect Prediction</b></span></li>
<li class="chapter" data-level="3" data-path="data-sources-in-software-engineering.html"><a href="data-sources-in-software-engineering.html"><i class="fa fa-check"></i><b>3</b> Data Sources in Software Engineering</a></li>
<li class="chapter" data-level="4" data-path="repositories.html"><a href="repositories.html"><i class="fa fa-check"></i><b>4</b> Repositories</a></li>
<li class="chapter" data-level="5" data-path="open-toolsdashboards-to-extract-data.html"><a href="open-toolsdashboards-to-extract-data.html"><i class="fa fa-check"></i><b>5</b> Open Tools/Dashboards to extract data</a><ul>
<li class="chapter" data-level="5.1" data-path="open-toolsdashboards-to-extract-data.html"><a href="open-toolsdashboards-to-extract-data.html#issues"><i class="fa fa-check"></i><b>5.1</b> Issues</a></li>
<li class="chapter" data-level="5.2" data-path="open-toolsdashboards-to-extract-data.html"><a href="open-toolsdashboards-to-extract-data.html#effort-estimation-data-in-software-engineering"><i class="fa fa-check"></i><b>5.2</b> Effort Estimation Data in Software Engineering</a></li>
</ul></li>
<li class="part"><span><b>IV Exploratory and Descriptive Data analysis</b></span></li>
<li class="chapter" data-level="6" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html"><i class="fa fa-check"></i><b>6</b> Exploratory Data Analysis</a><ul>
<li class="chapter" data-level="6.1" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#descriptive-statistics"><i class="fa fa-check"></i><b>6.1</b> Descriptive statistics</a></li>
<li class="chapter" data-level="6.2" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#basic-plots"><i class="fa fa-check"></i><b>6.2</b> Basic Plots</a></li>
<li class="chapter" data-level="6.3" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#normality"><i class="fa fa-check"></i><b>6.3</b> Normality</a></li>
<li class="chapter" data-level="6.4" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#using-a-running-example-to-visualise-the-different-plots"><i class="fa fa-check"></i><b>6.4</b> Using a running Example to visualise the different plots</a><ul>
<li class="chapter" data-level="6.4.1" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#example-with-the-china-dataset-from-the-tera-promise-repository"><i class="fa fa-check"></i><b>6.4.1</b> Example with the China dataset (from the tera-Promise Repository)</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#correlation"><i class="fa fa-check"></i><b>6.5</b> Correlation</a></li>
<li class="chapter" data-level="6.6" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#confidence-intervals.-bootstrap"><i class="fa fa-check"></i><b>6.6</b> Confidence Intervals. Bootstrap</a></li>
<li class="chapter" data-level="6.7" data-path="exploratory-data-analysis.html"><a href="exploratory-data-analysis.html#nonparametric-bootstrap"><i class="fa fa-check"></i><b>6.7</b> Nonparametric Bootstrap</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="classical-hypothesis-testing.html"><a href="classical-hypothesis-testing.html"><i class="fa fa-check"></i><b>7</b> Classical Hypothesis Testing</a><ul>
<li class="chapter" data-level="7.1" data-path="classical-hypothesis-testing.html"><a href="classical-hypothesis-testing.html#p-values"><i class="fa fa-check"></i><b>7.1</b> p-values</a></li>
</ul></li>
<li class="part"><span><b>V Preprocessing</b></span></li>
<li class="chapter" data-level="8" data-path="preprocessing.html"><a href="preprocessing.html"><i class="fa fa-check"></i><b>8</b> Preprocessing</a><ul>
<li class="chapter" data-level="8.1" data-path="preprocessing.html"><a href="preprocessing.html#data"><i class="fa fa-check"></i><b>8.1</b> Data</a></li>
<li class="chapter" data-level="8.2" data-path="preprocessing.html"><a href="preprocessing.html#missing-values"><i class="fa fa-check"></i><b>8.2</b> Missing values</a></li>
<li class="chapter" data-level="8.3" data-path="preprocessing.html"><a href="preprocessing.html#noise"><i class="fa fa-check"></i><b>8.3</b> Noise</a></li>
<li class="chapter" data-level="8.4" data-path="preprocessing.html"><a href="preprocessing.html#outliers"><i class="fa fa-check"></i><b>8.4</b> Outliers</a></li>
<li class="chapter" data-level="8.5" data-path="preprocessing.html"><a href="preprocessing.html#feature-selection"><i class="fa fa-check"></i><b>8.5</b> Feature selection</a><ul>
<li class="chapter" data-level="8.5.1" data-path="preprocessing.html"><a href="preprocessing.html#fselector-package-in-r"><i class="fa fa-check"></i><b>8.5.1</b> FSelector package in R</a></li>
</ul></li>
<li class="chapter" data-level="8.6" data-path="preprocessing.html"><a href="preprocessing.html#instance-selection"><i class="fa fa-check"></i><b>8.6</b> Instance selection</a></li>
<li class="chapter" data-level="8.7" data-path="preprocessing.html"><a href="preprocessing.html#discretization"><i class="fa fa-check"></i><b>8.7</b> Discretization</a></li>
<li class="chapter" data-level="8.8" data-path="preprocessing.html"><a href="preprocessing.html#correlation-coefficient-and-covariance-for-numeric-data"><i class="fa fa-check"></i><b>8.8</b> Correlation Coefficient and Covariance for Numeric Data</a></li>
<li class="chapter" data-level="8.9" data-path="preprocessing.html"><a href="preprocessing.html#normalization-1"><i class="fa fa-check"></i><b>8.9</b> Normalization</a><ul>
<li class="chapter" data-level="8.9.1" data-path="preprocessing.html"><a href="preprocessing.html#min-max-normalization"><i class="fa fa-check"></i><b>8.9.1</b> Min-Max Normalization</a></li>
<li class="chapter" data-level="8.9.2" data-path="preprocessing.html"><a href="preprocessing.html#z-score-normalization"><i class="fa fa-check"></i><b>8.9.2</b> Z-score normalization</a></li>
</ul></li>
<li class="chapter" data-level="8.10" data-path="preprocessing.html"><a href="preprocessing.html#transformations"><i class="fa fa-check"></i><b>8.10</b> Transformations</a><ul>
<li class="chapter" data-level="8.10.1" data-path="preprocessing.html"><a href="preprocessing.html#linear-transformations-and-quadratic-trans-formations"><i class="fa fa-check"></i><b>8.10.1</b> Linear Transformations and Quadratic Trans formations</a></li>
<li class="chapter" data-level="8.10.2" data-path="preprocessing.html"><a href="preprocessing.html#box-cox-transformation"><i class="fa fa-check"></i><b>8.10.2</b> Box-cox transformation</a></li>
<li class="chapter" data-level="8.10.3" data-path="preprocessing.html"><a href="preprocessing.html#nominal-to-binary-tranformations"><i class="fa fa-check"></i><b>8.10.3</b> Nominal to Binary tranformations</a></li>
</ul></li>
<li class="chapter" data-level="8.11" data-path="preprocessing.html"><a href="preprocessing.html#preprocessing-in-r"><i class="fa fa-check"></i><b>8.11</b> Preprocessing in R</a><ul>
<li class="chapter" data-level="8.11.1" data-path="preprocessing.html"><a href="preprocessing.html#the-dplyr-package"><i class="fa fa-check"></i><b>8.11.1</b> The <code>dplyr</code> package</a></li>
</ul></li>
<li class="chapter" data-level="8.12" data-path="preprocessing.html"><a href="preprocessing.html#other-libraries-and-tricks"><i class="fa fa-check"></i><b>8.12</b> Other libraries and tricks</a></li>
</ul></li>
<li class="part"><span><b>VI Supervised Models</b></span></li>
<li class="chapter" data-level="9" data-path="supervised-classification.html"><a href="supervised-classification.html"><i class="fa fa-check"></i><b>9</b> Supervised Classification</a><ul>
<li class="chapter" data-level="9.1" data-path="supervised-classification.html"><a href="supervised-classification.html#classification-trees"><i class="fa fa-check"></i><b>9.1</b> Classification Trees</a></li>
<li class="chapter" data-level="9.2" data-path="supervised-classification.html"><a href="supervised-classification.html#rules"><i class="fa fa-check"></i><b>9.2</b> Rules</a></li>
<li class="chapter" data-level="9.3" data-path="supervised-classification.html"><a href="supervised-classification.html#distanced-based-methods"><i class="fa fa-check"></i><b>9.3</b> Distanced-based Methods</a></li>
<li class="chapter" data-level="9.4" data-path="supervised-classification.html"><a href="supervised-classification.html#neural-networks"><i class="fa fa-check"></i><b>9.4</b> Neural Networks</a></li>
<li class="chapter" data-level="9.5" data-path="supervised-classification.html"><a href="supervised-classification.html#support-vector-machine"><i class="fa fa-check"></i><b>9.5</b> Support Vector Machine</a></li>
<li class="chapter" data-level="9.6" data-path="supervised-classification.html"><a href="supervised-classification.html#probabilistic-methods"><i class="fa fa-check"></i><b>9.6</b> Probabilistic Methods</a><ul>
<li class="chapter" data-level="9.6.1" data-path="supervised-classification.html"><a href="supervised-classification.html#naive-bayes"><i class="fa fa-check"></i><b>9.6.1</b> Naive Bayes</a></li>
</ul></li>
<li class="chapter" data-level="9.7" data-path="supervised-classification.html"><a href="supervised-classification.html#linear-discriminant-analysis-lda"><i class="fa fa-check"></i><b>9.7</b> Linear Discriminant Analysis (LDA)</a><ul>
<li class="chapter" data-level="9.7.1" data-path="supervised-classification.html"><a href="supervised-classification.html#predicting-the-number-of-defects-numerical-class"><i class="fa fa-check"></i><b>9.7.1</b> Predicting the number of defects (numerical class)</a></li>
</ul></li>
<li class="chapter" data-level="9.8" data-path="supervised-classification.html"><a href="supervised-classification.html#binary-logistic-regression-blr"><i class="fa fa-check"></i><b>9.8</b> Binary Logistic Regression (BLR)</a></li>
<li class="chapter" data-level="9.9" data-path="supervised-classification.html"><a href="supervised-classification.html#the-caret-package"><i class="fa fa-check"></i><b>9.9</b> The caret package</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="regression.html"><a href="regression.html"><i class="fa fa-check"></i><b>10</b> Regression</a><ul>
<li class="chapter" data-level="10.1" data-path="regression.html"><a href="regression.html#linear-regression-modeling"><i class="fa fa-check"></i><b>10.1</b> Linear Regression modeling</a><ul>
<li class="chapter" data-level="10.1.1" data-path="regression.html"><a href="regression.html#regression-galton-data"><i class="fa fa-check"></i><b>10.1.1</b> Regression: Galton Data</a></li>
<li class="chapter" data-level="10.1.2" data-path="regression.html"><a href="regression.html#simple-linear-regression"><i class="fa fa-check"></i><b>10.1.2</b> Simple Linear Regression</a></li>
<li class="chapter" data-level="10.1.3" data-path="regression.html"><a href="regression.html#least-squares"><i class="fa fa-check"></i><b>10.1.3</b> Least Squares</a></li>
<li class="chapter" data-level="10.1.4" data-path="regression.html"><a href="regression.html#linear-regression-in-r"><i class="fa fa-check"></i><b>10.1.4</b> Linear regression in R</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="regression.html"><a href="regression.html#linear-regression-diagnostics"><i class="fa fa-check"></i><b>10.2</b> Linear Regression Diagnostics</a><ul>
<li class="chapter" data-level="10.2.1" data-path="regression.html"><a href="regression.html#simulation-example"><i class="fa fa-check"></i><b>10.2.1</b> Simulation example</a></li>
<li class="chapter" data-level="10.2.2" data-path="regression.html"><a href="regression.html#diagnostics-fro-assessing-the-regression-line"><i class="fa fa-check"></i><b>10.2.2</b> Diagnostics fro assessing the regression line</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="regression.html"><a href="regression.html#multiple-linear-regression"><i class="fa fa-check"></i><b>10.3</b> Multiple Linear Regression</a><ul>
<li class="chapter" data-level="10.3.1" data-path="regression.html"><a href="regression.html#partial-least-squares"><i class="fa fa-check"></i><b>10.3.1</b> Partial Least Squares</a></li>
</ul></li>
<li class="chapter" data-level="10.4" data-path="regression.html"><a href="regression.html#linear-regression-in-software-effort-estimation"><i class="fa fa-check"></i><b>10.4</b> Linear regression in Software Effort estimation</a></li>
<li class="chapter" data-level="10.5" data-path="regression.html"><a href="regression.html#references"><i class="fa fa-check"></i><b>10.5</b> References</a></li>
</ul></li>
<li class="part"><span><b>VII Unsupervised Models</b></span></li>
<li class="chapter" data-level="11" data-path="unsupervised-or-descriptive-modeling.html"><a href="unsupervised-or-descriptive-modeling.html"><i class="fa fa-check"></i><b>11</b> Unsupervised or Descriptive modeling</a><ul>
<li class="chapter" data-level="11.1" data-path="unsupervised-or-descriptive-modeling.html"><a href="unsupervised-or-descriptive-modeling.html#clustering"><i class="fa fa-check"></i><b>11.1</b> Clustering</a><ul>
<li class="chapter" data-level="11.1.1" data-path="unsupervised-or-descriptive-modeling.html"><a href="unsupervised-or-descriptive-modeling.html#k-means"><i class="fa fa-check"></i><b>11.1.1</b> k-Means</a></li>
</ul></li>
<li class="chapter" data-level="11.2" data-path="unsupervised-or-descriptive-modeling.html"><a href="unsupervised-or-descriptive-modeling.html#association-rules"><i class="fa fa-check"></i><b>11.2</b> Association rules</a></li>
</ul></li>
<li class="part"><span><b>VIII Evaluation</b></span></li>
<li class="chapter" data-level="12" data-path="evaluation-of-models.html"><a href="evaluation-of-models.html"><i class="fa fa-check"></i><b>12</b> Evaluation of Models</a><ul>
<li class="chapter" data-level="12.1" data-path="evaluation-of-models.html"><a href="evaluation-of-models.html#building-and-validating-a-model"><i class="fa fa-check"></i><b>12.1</b> Building and Validating a Model</a><ul>
<li class="chapter" data-level="12.1.1" data-path="evaluation-of-models.html"><a href="evaluation-of-models.html#holdout-approach"><i class="fa fa-check"></i><b>12.1.1</b> Holdout approach</a></li>
<li class="chapter" data-level="12.1.2" data-path="evaluation-of-models.html"><a href="evaluation-of-models.html#cross-validation-cv"><i class="fa fa-check"></i><b>12.1.2</b> Cross Validation (CV)</a></li>
</ul></li>
<li class="chapter" data-level="12.2" data-path="evaluation-of-models.html"><a href="evaluation-of-models.html#evaluation-of-classification-models"><i class="fa fa-check"></i><b>12.2</b> Evaluation of Classification Models</a><ul>
<li class="chapter" data-level="12.2.1" data-path="evaluation-of-models.html"><a href="evaluation-of-models.html#prediction-in-probabilistic-classifiers"><i class="fa fa-check"></i><b>12.2.1</b> Prediction in probabilistic classifiers</a></li>
</ul></li>
<li class="chapter" data-level="12.3" data-path="evaluation-of-models.html"><a href="evaluation-of-models.html#other-metrics-used-in-software-engineering-with-classification"><i class="fa fa-check"></i><b>12.3</b> Other Metrics used in Software Engineering with Classification</a></li>
<li class="chapter" data-level="12.4" data-path="evaluation-of-models.html"><a href="evaluation-of-models.html#graphical-evaluation"><i class="fa fa-check"></i><b>12.4</b> Graphical Evaluation</a><ul>
<li class="chapter" data-level="12.4.1" data-path="evaluation-of-models.html"><a href="evaluation-of-models.html#receiver-operating-characteristic-roc"><i class="fa fa-check"></i><b>12.4.1</b> Receiver Operating Characteristic (ROC)</a></li>
<li class="chapter" data-level="12.4.2" data-path="evaluation-of-models.html"><a href="evaluation-of-models.html#precision-recall-curve-prc"><i class="fa fa-check"></i><b>12.4.2</b> Precision-Recall Curve (PRC)</a></li>
</ul></li>
<li class="chapter" data-level="12.5" data-path="evaluation-of-models.html"><a href="evaluation-of-models.html#numeric-prediction-evaluation"><i class="fa fa-check"></i><b>12.5</b> Numeric Prediction Evaluation</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="evaluationSE.html"><a href="evaluationSE.html"><i class="fa fa-check"></i><b>13</b> Measures of Evaluation in Software Engineering</a><ul>
<li class="chapter" data-level="13.1" data-path="evaluationSE.html"><a href="evaluationSE.html#evaluation-of-the-model-in-the-testing-data"><i class="fa fa-check"></i><b>13.1</b> Evaluation of the model in the Testing data</a></li>
<li class="chapter" data-level="13.2" data-path="evaluationSE.html"><a href="evaluationSE.html#building-a-linear-model-on-the-telecom1-dataset"><i class="fa fa-check"></i><b>13.2</b> Building a Linear Model on the Telecom1 dataset</a></li>
<li class="chapter" data-level="13.3" data-path="evaluationSE.html"><a href="evaluationSE.html#building-a-linear-model-on-the-telecom1-dataset-with-all-observations"><i class="fa fa-check"></i><b>13.3</b> Building a Linear Model on the Telecom1 dataset with all observations</a></li>
<li class="chapter" data-level="13.4" data-path="evaluationSE.html"><a href="evaluationSE.html#standardised-accuracy.-marp0.-chinatest"><i class="fa fa-check"></i><b>13.4</b> Standardised Accuracy. MARP0. ChinaTest</a></li>
<li class="chapter" data-level="13.5" data-path="evaluationSE.html"><a href="evaluationSE.html#standardised-accuracy.-marp0.-telecom1"><i class="fa fa-check"></i><b>13.5</b> Standardised Accuracy. MARP0. Telecom1</a><ul>
<li class="chapter" data-level="13.5.1" data-path="evaluationSE.html"><a href="evaluationSE.html#marp0-in-the-atkinson-dataset"><i class="fa fa-check"></i><b>13.5.1</b> MARP0 in the Atkinson dataset</a></li>
</ul></li>
<li class="chapter" data-level="13.6" data-path="evaluationSE.html"><a href="evaluationSE.html#exact-marp0"><i class="fa fa-check"></i><b>13.6</b> Exact MARP0</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="wbl-simple-r-code-to-calculate-shepperd-and-macdonells-marp0-exactly.html"><a href="wbl-simple-r-code-to-calculate-shepperd-and-macdonells-marp0-exactly.html"><i class="fa fa-check"></i><b>14</b> WBL simple R code to calculate Shepperd and MacDonell’s marp0 exactly</a><ul>
<li class="chapter" data-level="14.1" data-path="wbl-simple-r-code-to-calculate-shepperd-and-macdonells-marp0-exactly.html"><a href="wbl-simple-r-code-to-calculate-shepperd-and-macdonells-marp0-exactly.html#computing-the-bootstraped-confidence-interval-of-the-mean-for-the-test-observations-of-the-china-dataset"><i class="fa fa-check"></i><b>14.1</b> Computing the bootstraped confidence interval of the mean for the Test observations of the China dataset:</a></li>
</ul></li>
<li class="part"><span><b>IX Advanced Topics</b></span></li>
<li class="chapter" data-level="15" data-path="feature-selection-1.html"><a href="feature-selection-1.html"><i class="fa fa-check"></i><b>15</b> Feature Selection</a><ul>
<li class="chapter" data-level="15.1" data-path="feature-selection-1.html"><a href="feature-selection-1.html#instance-selection-1"><i class="fa fa-check"></i><b>15.1</b> Instance Selection</a></li>
<li class="chapter" data-level="15.2" data-path="feature-selection-1.html"><a href="feature-selection-1.html#missing-data-imputation"><i class="fa fa-check"></i><b>15.2</b> Missing Data Imputation</a></li>
</ul></li>
<li class="chapter" data-level="16" data-path="feature-selection-example.html"><a href="feature-selection-example.html"><i class="fa fa-check"></i><b>16</b> Feature Selection Example</a></li>
<li class="chapter" data-level="17" data-path="advanced-models.html"><a href="advanced-models.html"><i class="fa fa-check"></i><b>17</b> Advanced Models</a><ul>
<li class="chapter" data-level="17.1" data-path="advanced-models.html"><a href="advanced-models.html#genetic-programming-for-symbolic-regression"><i class="fa fa-check"></i><b>17.1</b> Genetic Programming for Symbolic Regression</a></li>
<li class="chapter" data-level="17.2" data-path="advanced-models.html"><a href="advanced-models.html#genetic-programming-example"><i class="fa fa-check"></i><b>17.2</b> Genetic Programming Example</a><ul>
<li class="chapter" data-level="17.2.1" data-path="advanced-models.html"><a href="advanced-models.html#load-data"><i class="fa fa-check"></i><b>17.2.1</b> Load Data</a></li>
<li class="chapter" data-level="17.2.2" data-path="advanced-models.html"><a href="advanced-models.html#genetic-programming-for-symbolic-regression-china-dataset."><i class="fa fa-check"></i><b>17.2.2</b> Genetic Programming for Symbolic Regression: China dataset.</a></li>
<li class="chapter" data-level="17.2.3" data-path="advanced-models.html"><a href="advanced-models.html#genetic-programming-for-symbolic-regression.-telecom1-dataset."><i class="fa fa-check"></i><b>17.2.3</b> Genetic Programming for Symbolic Regression. Telecom1 dataset.</a></li>
</ul></li>
<li class="chapter" data-level="17.3" data-path="advanced-models.html"><a href="advanced-models.html#neural-networks-1"><i class="fa fa-check"></i><b>17.3</b> Neural Networks</a></li>
<li class="chapter" data-level="17.4" data-path="advanced-models.html"><a href="advanced-models.html#support-vector-machines"><i class="fa fa-check"></i><b>17.4</b> Support Vector Machines</a></li>
<li class="chapter" data-level="17.5" data-path="advanced-models.html"><a href="advanced-models.html#ensembles"><i class="fa fa-check"></i><b>17.5</b> Ensembles</a><ul>
<li class="chapter" data-level="17.5.1" data-path="advanced-models.html"><a href="advanced-models.html#bagging"><i class="fa fa-check"></i><b>17.5.1</b> Bagging</a></li>
<li class="chapter" data-level="17.5.2" data-path="advanced-models.html"><a href="advanced-models.html#boosting"><i class="fa fa-check"></i><b>17.5.2</b> Boosting</a></li>
<li class="chapter" data-level="17.5.3" data-path="advanced-models.html"><a href="advanced-models.html#rotation-forests"><i class="fa fa-check"></i><b>17.5.3</b> Rotation Forests</a></li>
<li class="chapter" data-level="17.5.4" data-path="advanced-models.html"><a href="advanced-models.html#boosting-in-r"><i class="fa fa-check"></i><b>17.5.4</b> Boosting in R</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="18" data-path="further-classification-models.html"><a href="further-classification-models.html"><i class="fa fa-check"></i><b>18</b> Further Classification Models</a><ul>
<li class="chapter" data-level="18.1" data-path="further-classification-models.html"><a href="further-classification-models.html#multilabel-classification"><i class="fa fa-check"></i><b>18.1</b> Multilabel classification</a></li>
<li class="chapter" data-level="18.2" data-path="further-classification-models.html"><a href="further-classification-models.html#semi-supervised-learning"><i class="fa fa-check"></i><b>18.2</b> Semi-supervised Learning</a></li>
</ul></li>
<li class="chapter" data-level="19" data-path="social-network-analysis-in-se.html"><a href="social-network-analysis-in-se.html"><i class="fa fa-check"></i><b>19</b> Social Network Analysis in SE</a></li>
<li class="chapter" data-level="20" data-path="text-mining-software-engineering-data.html"><a href="text-mining-software-engineering-data.html"><i class="fa fa-check"></i><b>20</b> Text Mining Software Engineering Data</a><ul>
<li class="chapter" data-level="20.1" data-path="text-mining-software-engineering-data.html"><a href="text-mining-software-engineering-data.html#terminology"><i class="fa fa-check"></i><b>20.1</b> Terminology</a></li>
<li class="chapter" data-level="20.2" data-path="text-mining-software-engineering-data.html"><a href="text-mining-software-engineering-data.html#example-of-classifying-bugs-from-bugzilla"><i class="fa fa-check"></i><b>20.2</b> Example of classifying bugs from Bugzilla</a></li>
<li class="chapter" data-level="20.3" data-path="text-mining-software-engineering-data.html"><a href="text-mining-software-engineering-data.html#extracting-data-from-twitter"><i class="fa fa-check"></i><b>20.3</b> Extracting data from Twitter</a></li>
</ul></li>
<li class="chapter" data-level="21" data-path="time-series.html"><a href="time-series.html"><i class="fa fa-check"></i><b>21</b> Time Series</a><ul>
<li class="chapter" data-level="21.1" data-path="time-series.html"><a href="time-series.html#web-tutorials-about-time-series"><i class="fa fa-check"></i><b>21.1</b> Web tutorials about Time Series:</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references-1.html"><a href="references-1.html"><i class="fa fa-check"></i>References</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Data Analysis in Software Engineering using R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="regression" class="section level1">
<h1><span class="header-section-number">Chapter 10</span> Regression</h1>
<div id="linear-regression-modeling" class="section level2">
<h2><span class="header-section-number">10.1</span> Linear Regression modeling</h2>
<ul>
<li><p><em>Linear Regression</em> is one of the oldest and most known predictive methods. As its name says, the idea is to try to fit a linear equation between a dependent variable and an independent, or explanatory, variable. The idea is that the independent variable <span class="math inline">\(x\)</span> is something the experimenter controls and the dependent variable <span class="math inline">\(y\)</span> is something that the experimenter measures. The line is used to predict the value of <span class="math inline">\(y\)</span> for a known value of <span class="math inline">\(x\)</span>. The variable <span class="math inline">\(x\)</span> is the predictor variable and <span class="math inline">\(y\)</span> the response variable.</p></li>
<li><p><em>Multiple linear regression</em> uses 2 or more independent variables for building a model. See <a href="https://www.wikipedia.org/wiki/Linear_regression" class="uri">https://www.wikipedia.org/wiki/Linear_regression</a>.</p></li>
<li><p>First proposed many years ago but still very useful…</p></li>
</ul>
<div class="figure">
<img src="figures/galton.png" alt="Galton Data" />
<p class="caption">Galton Data</p>
</div>
<ul>
<li>The equation takes the form <span class="math inline">\(\hat{y}=b_0+b_1 * x\)</span></li>
<li>The method used to choose the values <span class="math inline">\(b_0\)</span> and <span class="math inline">\(b_1\)</span> is to minimize the sum of the squares of the residual errors.</li>
</ul>
<div id="regression-galton-data" class="section level3">
<h3><span class="header-section-number">10.1.1</span> Regression: Galton Data</h3>
<p>Not related to Software Engineering but …</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(UsingR)
<span class="kw">data</span>(galton)
<span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>))
<span class="kw">hist</span>(galton<span class="op">$</span>child,<span class="dt">col=</span><span class="st">&quot;blue&quot;</span>,<span class="dt">breaks=</span><span class="dv">100</span>)
<span class="kw">hist</span>(galton<span class="op">$</span>parent,<span class="dt">col=</span><span class="st">&quot;blue&quot;</span>,<span class="dt">breaks=</span><span class="dv">100</span>)</code></pre>
<p><img src="DASE_files/figure-html/unnamed-chunk-69-1.png" width="672" /></p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">plot</span>(galton<span class="op">$</span>parent,galton<span class="op">$</span>child,<span class="dt">pch=</span><span class="dv">1</span>,<span class="dt">col=</span><span class="st">&quot;blue&quot;</span>, <span class="dt">cex=</span><span class="fl">0.4</span>)
lm1 &lt;-<span class="st"> </span><span class="kw">lm</span>(galton<span class="op">$</span>child <span class="op">~</span><span class="st"> </span>galton<span class="op">$</span>parent)
<span class="kw">lines</span>(galton<span class="op">$</span>parent,lm1<span class="op">$</span>fitted,<span class="dt">col=</span><span class="st">&quot;red&quot;</span>,<span class="dt">lwd=</span><span class="dv">3</span>)
<span class="kw">plot</span>(galton<span class="op">$</span>parent,lm1<span class="op">$</span>residuals,<span class="dt">col=</span><span class="st">&quot;blue&quot;</span>,<span class="dt">pch=</span><span class="dv">1</span>, <span class="dt">cex=</span><span class="fl">0.4</span>)
<span class="kw">abline</span>(<span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">0</span>),<span class="dt">col=</span><span class="st">&quot;red&quot;</span>,<span class="dt">lwd=</span><span class="dv">3</span>)</code></pre>
<p><img src="DASE_files/figure-html/unnamed-chunk-69-2.png" width="672" /></p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">qqnorm</span>(galton<span class="op">$</span>child)</code></pre>
<p><img src="DASE_files/figure-html/unnamed-chunk-69-3.png" width="672" /></p>
</div>
<div id="simple-linear-regression" class="section level3">
<h3><span class="header-section-number">10.1.2</span> Simple Linear Regression</h3>
<ul>
<li>Given two variables <span class="math inline">\(Y\)</span> (response) and <span class="math inline">\(X\)</span> (predictor), the assumption is that there is an approximate (<span class="math inline">\(\approx\)</span>) <em>linear</em> relation between those variables.</li>
<li><p>The mathematical model of the observed data is described as (for the case of simple linear regression):
<span class="math display">\[ Y \approx \beta_0 + \beta_1 X\]</span></p></li>
<li>the parameter <span class="math inline">\(\beta_0\)</span> is named the <em>intercept</em> and <span class="math inline">\(\beta_1\)</span> is the <em>slope</em></li>
<li><p>Each observation can be modeled as</p></li>
</ul>
<p><span class="math display">\[y_i = \beta_0 + \beta_1 x_i + \epsilon_i;
\epsilon_i \sim N(0,\sigma^2)\]</span>
- <span class="math inline">\(\epsilon_i\)</span> is the <em>error</em>
- This means that the variable <span class="math inline">\(y\)</span> is normally distributed:
<span class="math display">\[ y_i \sim N( \beta_0 + \beta_1 x_i, \sigma^2) \]</span></p>
<ul>
<li>The <em>predictions</em> or <em>estimations</em> of this model are obtained by a linear equation of the form <span class="math inline">\(\hat{Y}=\hat{\beta_0} + \hat{\beta}_1X\)</span>, that is, each new prediction is computed with
<span class="math display">\[\hat{y}_i = \hat{\beta}_0 + \hat{\beta}_1x_i \]</span>.</li>
<li>The actual parameters <span class="math inline">\(\beta_0\)</span> and <span class="math inline">\(\beta_1\)</span> are unknown</li>
<li>The parameters <span class="math inline">\(\hat{\beta}_0\)</span> and <span class="math inline">\(\hat{\beta}_1\)</span> of the linear equation can be estimated with different methods.</li>
</ul>
</div>
<div id="least-squares" class="section level3">
<h3><span class="header-section-number">10.1.3</span> Least Squares</h3>
<ul>
<li>One of the most used methods for computing <span class="math inline">\(\hat{\beta}_0\)</span> and <span class="math inline">\(\hat{\beta}_1\)</span> is the criterion of “least squares” minimization.</li>
<li>The data is composed of <span class="math inline">\(n\)</span> pairs of observations <span class="math inline">\((x_i, y_i)\)</span></li>
<li>Given an observation <span class="math inline">\(y_i\)</span> and its corresponding estimation <span class="math inline">\(\hat{y_i})\)</span> the <em>residual</em> <span class="math inline">\(e_i\)</span> is defined as <span class="math display">\[e_i= y_i - \hat{y_i}\]</span></li>
<li>the Residual Sum of Squares is defined as <span class="math display">\[RSS=e_1^2+\dots + e_i^2+\dots+e_n^2\]</span></li>
<li>the Least Squares Approach minimizes the RSS</li>
<li>as result of that minimizitation, it can be obtained, by means of calculus, the estimation of <span class="math inline">\(\hat{\beta}_0\)</span> and <span class="math inline">\(\hat{\beta}_1\)</span> as <span class="math display">\[\hat{\beta}_1=\frac{\sum_{i=1}^{n}{(x_i-\bar{x})(y_i-\bar{y})}}{\sum_{i=1}^{n}(x_i-\bar{x})^2}\]</span> and <span class="math display">\[\hat{\beta}_0=\bar{y}-\hat{\beta}_1\bar{x} \]</span> where <span class="math inline">\(\bar{y}\)</span> and <span class="math inline">\(\bar{x}\)</span> are the sample means.</li>
<li>the variance <span class="math inline">\(\sigma^2\)</span> is estimated by
<span class="math display">\[\hat\sigma^2 = {RSS}/{(n-2)}\]</span> where n is the number of observations</li>
<li>The <em>Residual Standard Error</em> is defined as <span class="math display">\[RSE = \sqrt{{RSS}/{(n-2)}}\]</span></li>
<li>The equation <span class="math display">\[ Y = \beta_0 + \beta_1 X + \epsilon\]</span> defines the linear model, i.e., the <em>population regression line</em></li>
<li>The <em>least squares line</em> is <span class="math inline">\(\hat{Y}=\hat{\beta_0} + \hat{\beta}_1X\)</span></li>
<li><em>Confidence intervals</em> are computed using the <em>standard errors</em> of the intercept and the slope.</li>
<li>The <span class="math inline">\(95\%\)</span> confidence interval for the slope is computed as <span class="math display">\[[\hat{\beta}_1 - 2 \cdot SE(\hat{\beta}_1), \hat{\beta}_1+SE(\hat{\beta}_1)]\]</span></li>
<li>where <span class="math display">\[ SE(\hat{\beta}_1) = \sqrt{\frac{\sigma^2}{\sum_{i=1}^{n}(x_i-\bar{x})^2}}\]</span></li>
</ul>
</div>
<div id="linear-regression-in-r" class="section level3">
<h3><span class="header-section-number">10.1.4</span> Linear regression in R</h3>
<p>The following are the basic commands in R:</p>
<ul>
<li>The basic function is <code>lm()</code>, that returns an object with the model.</li>
<li>Other commands: <code>summary</code> prints out information about the regression, <code>coef</code> gives the coefficients for the linear model, <code>fitted</code> gives the predictd value of <span class="math inline">\(y\)</span> for each value of <span class="math inline">\(x\)</span>, <code>residuals</code> contains the differences between observed and fitted values.</li>
<li><a href="https://stat.ethz.ch/R-manual/R-devel/library/stats/html/predict.lm.html"><code>predict</code></a> will generate predicted values of the response for the values of the explanatory variable.</li>
</ul>
</div>
</div>
<div id="linear-regression-diagnostics" class="section level2">
<h2><span class="header-section-number">10.2</span> Linear Regression Diagnostics</h2>
<ul>
<li>Several plots help to evaluate the suitability of the linear regression
<ul>
<li><em>Residuals vs fitted</em>: The residuals should be randomly distributed around the horizontal line representing a residual error of zero; that is, there should not be a distinct trend in the distribution of points.</li>
<li><em>Standard Q-Q plot</em>: residual errors are normally distributed</li>
<li><em>Square root of the standardized residuals vs the fitted values</em>: there should be no obvious trend. This plot is similar to the residuals versus fitted values plot, but it uses the square root of the standardized residuals.</li>
<li><em>Leverage</em>: measures the importance of each point in determining the regression result. Smaller values means that removing the observation has little effect on the regression result.</li>
</ul></li>
</ul>
<div id="simulation-example" class="section level3">
<h3><span class="header-section-number">10.2.1</span> Simulation example</h3>
<div id="simulate-a-dataset" class="section level4">
<h4><span class="header-section-number">10.2.1.1</span> Simulate a dataset</h4>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">set.seed</span>(<span class="dv">3456</span>)
<span class="co"># equation is  y = -6.6 + 0.13 x +e</span>
<span class="co"># range x 100,400</span>
a &lt;-<span class="st"> </span><span class="fl">-6.6</span>
b &lt;-<span class="st"> </span><span class="fl">0.13</span>
num_obs &lt;-<span class="st"> </span><span class="dv">60</span>
xmin &lt;-<span class="st"> </span><span class="dv">100</span>
xmax &lt;-<span class="st"> </span><span class="dv">400</span>
x &lt;-<span class="st"> </span><span class="kw">sample</span>(<span class="kw">seq</span>(<span class="dt">from=</span>xmin, <span class="dt">to =</span> xmax, <span class="dt">by =</span><span class="dv">1</span>), <span class="dt">size=</span> num_obs, <span class="dt">replace=</span><span class="ot">FALSE</span>)

sderror &lt;-<span class="st"> </span><span class="dv">9</span> <span class="co"># sigma for the error term in the model</span>
e &lt;-<span class="st"> </span><span class="kw">rnorm</span>(num_obs, <span class="dv">0</span>, sderror) 

y &lt;-<span class="st"> </span>a <span class="op">+</span><span class="st"> </span>b <span class="op">*</span><span class="st"> </span>x <span class="op">+</span><span class="st"> </span>e


newlm &lt;-<span class="st"> </span><span class="kw">lm</span>(y<span class="op">~</span>x)
<span class="kw">summary</span>(newlm)</code></pre>
<pre><code>## 
## Call:
## lm(formula = y ~ x)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -15.937  -4.617  -0.923   3.797  21.442 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) -13.4765     3.0320   -4.44    4e-05 ***
## x             0.1550     0.0113   13.75   &lt;2e-16 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 7.99 on 58 degrees of freedom
## Multiple R-squared:  0.765,  Adjusted R-squared:  0.761 
## F-statistic:  189 on 1 and 58 DF,  p-value: &lt;2e-16</code></pre>
<pre class="sourceCode r"><code class="sourceCode r">cfa1 &lt;-<span class="st"> </span><span class="kw">coef</span>(newlm)[<span class="dv">1</span>]
cfb2 &lt;-<span class="st"> </span><span class="kw">coef</span>(newlm)[<span class="dv">2</span>]
<span class="kw">plot</span>(x,y, <span class="dt">xlab=</span><span class="st">&quot;x axis&quot;</span>, <span class="dt">ylab=</span> <span class="st">&quot;y axis&quot;</span>, <span class="dt">xlim =</span> <span class="kw">c</span>(xmin, xmax), <span class="dt">ylim =</span> <span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">60</span>), <span class="dt">sub =</span> <span class="st">&quot;Line in black is the actual model&quot;</span>)
<span class="kw">title</span>(<span class="dt">main =</span> <span class="kw">paste</span>(<span class="st">&quot;Line in blue is the Regression Line for &quot;</span>, num_obs, <span class="st">&quot; points.&quot;</span>))

<span class="kw">abline</span>(<span class="dt">a =</span> cfa1, <span class="dt">b =</span> cfb2, <span class="dt">col=</span> <span class="st">&quot;blue&quot;</span>, <span class="dt">lwd=</span><span class="dv">3</span>)
<span class="kw">abline</span>(<span class="dt">a =</span> a, <span class="dt">b =</span> b, <span class="dt">col=</span> <span class="st">&quot;black&quot;</span>, <span class="dt">lwd=</span><span class="dv">1</span>) <span class="co">#original line</span></code></pre>
<p><img src="DASE_files/figure-html/unnamed-chunk-70-1.png" width="672" /></p>
<div id="subset-a-set-of-points-from-the-same-sample" class="section level5">
<h5><span class="header-section-number">10.2.1.1.1</span> Subset a set of points from the same sample</h5>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># sample from  the same  x     to compare least squares lines </span>
<span class="co"># change the denominator in newsample to see how the least square lines changes accordingly. </span>
newsample &lt;-<span class="st"> </span><span class="kw">as.integer</span>(num_obs<span class="op">/</span><span class="dv">8</span>) <span class="co"># number of pairs x,y</span>

idxs_x1 &lt;-<span class="st"> </span><span class="kw">sample</span>(<span class="dv">1</span><span class="op">:</span>num_obs, <span class="dt">size =</span> newsample, <span class="dt">replace =</span> <span class="ot">FALSE</span>) <span class="co">#sample indexes</span>
x1 &lt;-<span class="st"> </span>x[idxs_x1]
e1 &lt;-<span class="st"> </span>e[idxs_x1]
y1 &lt;-<span class="st"> </span>a <span class="op">+</span><span class="st"> </span>b <span class="op">*</span><span class="st"> </span>x1 <span class="op">+</span><span class="st"> </span>e1
xy_obs &lt;-<span class="st"> </span><span class="kw">data.frame</span>(x1, y1)
<span class="kw">names</span>(xy_obs) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;x_obs&quot;</span>, <span class="st">&quot;y_obs&quot;</span>)

newlm1 &lt;-<span class="st"> </span><span class="kw">lm</span>(y1<span class="op">~</span>x1)
<span class="kw">summary</span>(newlm1)</code></pre>
<pre><code>## 
## Call:
## lm(formula = y1 ~ x1)
## 
## Residuals:
##      1      2      3      4      5      6      7 
##  3.722 -5.067  4.683 -4.716  3.095 -0.813 -0.904 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)   
## (Intercept) -14.5356     7.0962   -2.05   0.0958 . 
## x1            0.1494     0.0272    5.48   0.0027 **
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 4.35 on 5 degrees of freedom
## Multiple R-squared:  0.857,  Adjusted R-squared:  0.829 
## F-statistic: 30.1 on 1 and 5 DF,  p-value: 0.00275</code></pre>
<pre class="sourceCode r"><code class="sourceCode r">cfa21 &lt;-<span class="st"> </span><span class="kw">coef</span>(newlm1)[<span class="dv">1</span>]
cfb22 &lt;-<span class="st"> </span><span class="kw">coef</span>(newlm1)[<span class="dv">2</span>]

<span class="kw">plot</span>(x1,y1, <span class="dt">xlab=</span><span class="st">&quot;x axis&quot;</span>, <span class="dt">ylab=</span> <span class="st">&quot;y axis&quot;</span>, <span class="dt">xlim =</span> <span class="kw">c</span>(xmin, xmax), <span class="dt">ylim =</span> <span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">60</span>))
<span class="kw">title</span>(<span class="dt">main =</span> <span class="kw">paste</span>(<span class="st">&quot;New line in red with &quot;</span>, newsample, <span class="st">&quot; points in sample&quot;</span>))

<span class="kw">abline</span>(<span class="dt">a =</span> a, <span class="dt">b =</span> b, <span class="dt">col=</span> <span class="st">&quot;black&quot;</span>, <span class="dt">lwd=</span><span class="dv">1</span>)  <span class="co"># True line</span>
<span class="kw">abline</span>(<span class="dt">a =</span> cfa1, <span class="dt">b =</span> cfb2, <span class="dt">col=</span> <span class="st">&quot;blue&quot;</span>, <span class="dt">lwd=</span><span class="dv">1</span>)  <span class="co">#sample</span>
<span class="kw">abline</span>(<span class="dt">a =</span> cfa21, <span class="dt">b =</span> cfb22, <span class="dt">col=</span> <span class="st">&quot;red&quot;</span>, <span class="dt">lwd=</span><span class="dv">2</span>) <span class="co">#new line</span></code></pre>
<p><img src="DASE_files/figure-html/unnamed-chunk-71-1.png" width="672" /></p>
</div>
<div id="compute-a-confidence-interval-on-the-original-sample-regression-line" class="section level5">
<h5><span class="header-section-number">10.2.1.1.2</span> Compute a confidence interval on the original sample regression line</h5>
<pre class="sourceCode r"><code class="sourceCode r">newx &lt;-<span class="st"> </span><span class="kw">seq</span>(xmin, xmax)
ypredicted &lt;-<span class="st"> </span><span class="kw">predict</span>(newlm, <span class="dt">newdata=</span><span class="kw">data.frame</span>(<span class="dt">x=</span>newx), <span class="dt">interval=</span> <span class="st">&quot;confidence&quot;</span>, <span class="dt">level=</span> <span class="fl">0.90</span>, <span class="dt">se =</span> <span class="ot">TRUE</span>)

<span class="kw">plot</span>(x,y, <span class="dt">xlab=</span><span class="st">&quot;x axis&quot;</span>, <span class="dt">ylab=</span> <span class="st">&quot;y axis&quot;</span>, <span class="dt">xlim =</span> <span class="kw">c</span>(xmin, xmax), <span class="dt">ylim =</span> <span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">60</span>))
<span class="co"># points(x1, fitted(newlm1))</span>
<span class="kw">abline</span>(newlm)

<span class="kw">lines</span>(newx,ypredicted<span class="op">$</span>fit[,<span class="dv">2</span>],<span class="dt">col=</span><span class="st">&quot;red&quot;</span>,<span class="dt">lty=</span><span class="dv">2</span>)
<span class="kw">lines</span>(newx,ypredicted<span class="op">$</span>fit[,<span class="dv">3</span>],<span class="dt">col=</span><span class="st">&quot;red&quot;</span>,<span class="dt">lty=</span><span class="dv">2</span>)</code></pre>
<p><img src="DASE_files/figure-html/unnamed-chunk-72-1.png" width="672" /></p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Plot the residuals or errors</span>
ypredicted_x &lt;-<span class="st"> </span><span class="kw">predict</span>(newlm, <span class="dt">newdata=</span><span class="kw">data.frame</span>(<span class="dt">x=</span>x))
<span class="kw">plot</span>(x,y, <span class="dt">xlab=</span><span class="st">&quot;x axis&quot;</span>, <span class="dt">ylab=</span> <span class="st">&quot;y axis&quot;</span>, <span class="dt">xlim =</span> <span class="kw">c</span>(xmin, xmax), <span class="dt">ylim =</span> <span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">60</span>), <span class="dt">sub =</span> <span class="st">&quot;&quot;</span>, <span class="dt">pch=</span><span class="dv">19</span>, <span class="dt">cex=</span><span class="fl">0.75</span>)
<span class="kw">title</span>(<span class="dt">main =</span> <span class="kw">paste</span>(<span class="st">&quot;Residuals or errors&quot;</span>, num_obs, <span class="st">&quot; points.&quot;</span>))
<span class="kw">abline</span>(newlm)
<span class="kw">segments</span>(x, y, x, ypredicted_x)</code></pre>
<p><img src="DASE_files/figure-html/unnamed-chunk-72-2.png" width="672" /></p>
</div>
<div id="take-another-sample-from-the-model-and-explore" class="section level5">
<h5><span class="header-section-number">10.2.1.1.3</span> Take another sample from the model and explore</h5>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># equation is  y = -6.6 + 0.13 x +e</span>
<span class="co"># range x 100,400</span>
num_obs &lt;-<span class="st"> </span><span class="dv">35</span>
xmin &lt;-<span class="st"> </span><span class="dv">100</span>
xmax &lt;-<span class="st"> </span><span class="dv">400</span>
x3 &lt;-<span class="st"> </span><span class="kw">sample</span>(<span class="kw">seq</span>(<span class="dt">from=</span>xmin, <span class="dt">to =</span> xmax, <span class="dt">by =</span><span class="dv">1</span>), <span class="dt">size=</span> num_obs, <span class="dt">replace=</span><span class="ot">FALSE</span>)
sderror &lt;-<span class="st"> </span><span class="dv">14</span> <span class="co"># sigma for the error term in the model</span>
e3 &lt;-<span class="st"> </span><span class="kw">rnorm</span>(num_obs, <span class="dv">0</span>, sderror) 

y3 &lt;-<span class="st"> </span>a <span class="op">+</span><span class="st"> </span>b <span class="op">*</span><span class="st"> </span>x3 <span class="op">+</span><span class="st"> </span>e3

newlm3 &lt;-<span class="st"> </span><span class="kw">lm</span>(y3<span class="op">~</span>x3)
<span class="kw">summary</span>(newlm3)</code></pre>
<pre><code>## 
## Call:
## lm(formula = y3 ~ x3)
## 
## Residuals:
##    Min     1Q Median     3Q    Max 
## -25.59 -11.19   2.92   8.65  39.16 
## 
## Coefficients:
##             Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) -17.5335     7.6813   -2.28    0.029 *  
## x3            0.1657     0.0285    5.80  1.7e-06 ***
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## Residual standard error: 15 on 33 degrees of freedom
## Multiple R-squared:  0.505,  Adjusted R-squared:  0.49 
## F-statistic: 33.7 on 1 and 33 DF,  p-value: 1.72e-06</code></pre>
<pre class="sourceCode r"><code class="sourceCode r">cfa31 &lt;-<span class="st"> </span><span class="kw">coef</span>(newlm3)[<span class="dv">1</span>]
cfb32 &lt;-<span class="st"> </span><span class="kw">coef</span>(newlm3)[<span class="dv">2</span>]
<span class="kw">plot</span>(x3,y3, <span class="dt">xlab=</span><span class="st">&quot;x axis&quot;</span>, <span class="dt">ylab=</span> <span class="st">&quot;y axis&quot;</span>, <span class="dt">xlim =</span> <span class="kw">c</span>(xmin, xmax), <span class="dt">ylim =</span> <span class="kw">c</span>(<span class="dv">0</span>,<span class="dv">60</span>))
<span class="kw">title</span>(<span class="dt">main =</span> <span class="kw">paste</span>(<span class="st">&quot;Line in red is the Regression Line for &quot;</span>, num_obs, <span class="st">&quot; points.&quot;</span>))
<span class="kw">abline</span>(<span class="dt">a =</span> cfa31, <span class="dt">b =</span> cfb32, <span class="dt">col=</span> <span class="st">&quot;red&quot;</span>, <span class="dt">lwd=</span><span class="dv">3</span>)
<span class="kw">abline</span>(<span class="dt">a =</span> a, <span class="dt">b =</span> b, <span class="dt">col=</span> <span class="st">&quot;black&quot;</span>, <span class="dt">lwd=</span><span class="dv">2</span>) <span class="co">#original line</span>
<span class="kw">abline</span>(<span class="dt">a =</span> cfa1, <span class="dt">b =</span> cfb2, <span class="dt">col=</span> <span class="st">&quot;blue&quot;</span>, <span class="dt">lwd=</span><span class="dv">1</span>) <span class="co">#first sample</span>

<span class="co"># confidence intervals for the new sample</span>

newx &lt;-<span class="st"> </span><span class="kw">seq</span>(xmin, xmax)
ypredicted &lt;-<span class="st"> </span><span class="kw">predict</span>(newlm3, <span class="dt">newdata=</span><span class="kw">data.frame</span>(<span class="dt">x3=</span>newx), <span class="dt">interval=</span> <span class="st">&quot;confidence&quot;</span>, <span class="dt">level=</span> <span class="fl">0.90</span>, <span class="dt">se =</span> <span class="ot">TRUE</span>)

<span class="kw">lines</span>(newx,ypredicted<span class="op">$</span>fit[,<span class="dv">2</span>],<span class="dt">col=</span><span class="st">&quot;red&quot;</span>,<span class="dt">lty=</span><span class="dv">2</span>, <span class="dt">lwd=</span><span class="dv">2</span>)
<span class="kw">lines</span>(newx,ypredicted<span class="op">$</span>fit[,<span class="dv">3</span>],<span class="dt">col=</span><span class="st">&quot;red&quot;</span>,<span class="dt">lty=</span><span class="dv">2</span>, <span class="dt">lwd=</span><span class="dv">2</span>)</code></pre>
<p><img src="DASE_files/figure-html/unnamed-chunk-73-1.png" width="672" /></p>
</div>
</div>
</div>
<div id="diagnostics-fro-assessing-the-regression-line" class="section level3">
<h3><span class="header-section-number">10.2.2</span> Diagnostics fro assessing the regression line</h3>
<div id="residual-standard-error" class="section level4">
<h4><span class="header-section-number">10.2.2.1</span> Residual Standard Error</h4>
<ul>
<li>It gives us an idea of the typical or average error of the model. It is the estimated standard deviation of the residuals.</li>
</ul>
</div>
<div id="r2-statistic" class="section level4">
<h4><span class="header-section-number">10.2.2.2</span> <span class="math inline">\(R^2\)</span> statistic</h4>
<ul>
<li>This is the proportion of variability in the data that is explained by the model. Best values are those close to 1.</li>
</ul>
</div>
</div>
</div>
<div id="multiple-linear-regression" class="section level2">
<h2><span class="header-section-number">10.3</span> Multiple Linear Regression</h2>
<div id="partial-least-squares" class="section level3">
<h3><span class="header-section-number">10.3.1</span> Partial Least Squares</h3>
<ul>
<li>If several predictors are highly correlated, the least squares approach has high variability.</li>
<li>PLS finds linear combinations of the predictors, that are called <em>components</em> or <em>latent</em> variables.</li>
</ul>
</div>
</div>
<div id="linear-regression-in-software-effort-estimation" class="section level2">
<h2><span class="header-section-number">10.4</span> Linear regression in Software Effort estimation</h2>
<p>Fitting a linear model to log-log
- the predictive power equation is <span class="math inline">\(y= e^{b_0 + b_1 log(x)}\)</span>, ignoring the bias corrections
- First, we are fitting the model to the whole dataset. But it is not the right way to do it, because of overfitting.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(foreign)
china &lt;-<span class="st"> </span><span class="kw">read.arff</span>(<span class="st">&quot;./datasets/effortEstimation/china.arff&quot;</span>)
china_size &lt;-<span class="st"> </span>china<span class="op">$</span>AFP
<span class="kw">summary</span>(china_size)</code></pre>
<pre><code>##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##       9     100     215     487     438   17518</code></pre>
<pre class="sourceCode r"><code class="sourceCode r">china_effort &lt;-<span class="st"> </span>china<span class="op">$</span>Effort
<span class="kw">summary</span>(china_effort)</code></pre>
<pre><code>##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##      26     704    1829    3921    3826   54620</code></pre>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>))
<span class="kw">hist</span>(china_size, <span class="dt">col=</span><span class="st">&quot;blue&quot;</span>, <span class="dt">xlab=</span><span class="st">&quot;Adjusted Function Points&quot;</span>, <span class="dt">main=</span><span class="st">&quot;Distribution of AFP&quot;</span>)
<span class="kw">hist</span>(china_effort, <span class="dt">col=</span><span class="st">&quot;blue&quot;</span>,<span class="dt">xlab=</span><span class="st">&quot;Effort&quot;</span>, <span class="dt">main=</span><span class="st">&quot;Distribution of Effort&quot;</span>)</code></pre>
<p><img src="DASE_files/figure-html/unnamed-chunk-74-1.png" width="672" /></p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">boxplot</span>(china_size)
<span class="kw">boxplot</span>(china_effort)</code></pre>
<p><img src="DASE_files/figure-html/unnamed-chunk-74-2.png" width="672" /></p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">qqnorm</span>(china_size)
<span class="kw">qqline</span>(china_size)
<span class="kw">qqnorm</span>(china_effort)
<span class="kw">qqline</span>(china_effort)</code></pre>
<p><img src="DASE_files/figure-html/unnamed-chunk-74-3.png" width="672" /></p>
<p>Applying the <code>log</code> function</p>
<p><img src="DASE_files/figure-html/unnamed-chunk-75-1.png" width="672" /><img src="DASE_files/figure-html/unnamed-chunk-75-2.png" width="672" /></p>
<pre class="sourceCode r"><code class="sourceCode r">linmodel_logchina &lt;-<span class="st"> </span><span class="kw">lm</span>(logchina_effort <span class="op">~</span><span class="st"> </span>logchina_size)
<span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">1</span>))
<span class="kw">plot</span>(logchina_size, logchina_effort)
<span class="kw">abline</span>(linmodel_logchina, <span class="dt">lwd=</span><span class="dv">3</span>, <span class="dt">col=</span><span class="dv">3</span>)</code></pre>
<p><img src="DASE_files/figure-html/unnamed-chunk-76-1.png" width="672" /></p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">par</span>(<span class="dt">mfrow=</span><span class="kw">c</span>(<span class="dv">1</span>,<span class="dv">2</span>))
<span class="kw">plot</span>(linmodel_logchina, <span class="dt">ask =</span> <span class="ot">FALSE</span>)</code></pre>
<p><img src="DASE_files/figure-html/unnamed-chunk-76-2.png" width="672" /><img src="DASE_files/figure-html/unnamed-chunk-76-3.png" width="672" /></p>
<pre class="sourceCode r"><code class="sourceCode r">linmodel_logchina</code></pre>
<pre><code>## 
## Call:
## lm(formula = logchina_effort ~ logchina_size)
## 
## Coefficients:
##   (Intercept)  logchina_size  
##         3.301          0.768</code></pre>
</div>
<div id="references" class="section level2">
<h2><span class="header-section-number">10.5</span> References</h2>
<ul>
<li>The New Statistics with R, Andy Hector, 2015</li>
<li>An Introduction to R, W.N. Venables and D.M. Smith and the R Development Core Team</li>
<li>Practical Data Science with R, Nina Zumel and John Mount</li>
<li>G. James et al, An Introduction to Statistical Learning with Applications in R, Springer, 2013</li>
</ul>

</div>
</div>



            </section>

          </div>
        </div>
      </div>
<a href="supervised-classification.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="unsupervised-or-descriptive-modeling.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/danrodgar/dasedown/edit/master/417_regression.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"download": ["DASE.pdf", "DASE.epub"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
