# Unsupervised Classification

From the predictive (unsupervised) point of view, patterns are found to predict future behaviour or estimate. This include association rules, clustering, or tree clustering which purpose is to join together objects (e.g., animals) into successively larger clusters, using some measure of similarity or distance. The dataset will be as the previous table without the $C$ class attribute

| $Att_1$  | ... | Attn |
|----------|-----| -----|
| $a_{11}$ | ... | a1n  |
| $a_{21}$ | ... | a2n  |
| ...      | ... | ...  |
| $a_{m1}$ | ... | amn  |




## Clustering

```{r}
library(foreign)
library(fpc)

kc1 <- read.arff("./datasets/defectPred/D1/KC1.arff")

# Split into training and test datasets
set.seed(1)
ind <- sample(2, nrow(kc1), replace = TRUE, prob = c(0.7, 0.3))
kc1.train <- kc1[ind==1, ]
kc1.test <- kc1[ind==2, ]

# No class
kc1.train$Defective <- NULL

ds <- dbscan(kc1.train, eps = 0.42, MinPts = 5)

kc1.kmeans <- kmeans(kc1.train, 2)

```


kMeans

```{r}
#library(reshape, quietly=TRUE)
#kc1.kmeans <- kmeans(sapply(na.omit(kc1.train), rescaler, "range"), 10)

```


## Association rules

```{r}
library(arules)

x <- as.numeric(kc1$LOC_TOTAL)
str(x)
summary(x)

hist(x, breaks=30, main="LoC Total")

 xDisc <- discretize(x, categories=5)
# table(xDisc)

 for(i in 1:21) kc1[,i] <- discretize(kc1[,i],  "frequency", categories=5)

str(kc1)
rules <- apriori(kc1, parameter = list(support=0.60, confidence=0.800, minlen=3))

rules

rules <- apriori(kc1,
   parameter = list(minlen=3, supp=0.6, conf=0.8),
   appearance = list(rhs=c("Defective=Y", "Defective=N"),
   default="lhs"),
   control = list(verbose=F))
 
 #rules <- apriori(kc1,
 #   parameter = list(minlen=2, supp=0.05, conf=0.3),
 #   appearance = list(rhs=c("Defective=Y", "Defective=N"),
 #   default="lhs"))
  
 inspect(rules)
 
 rules
 
 library(arulesViz)
 plot(rules)
 
```

