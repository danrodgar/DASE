[
["index.html", "Data Analysis in Software Engineering using R Welcome", " Data Analysis in Software Engineering using R Daniel Rodriguez and Javier Dolado 2017-04-03 Welcome This Data Analysis in Software Engineering (DASE) book/notes will try teach you how to do data science with R in Software Engineering. This work is licensed under the Creative Commons Attribution-NonCommercial-NoDerivs 3.0 United States License. "],
["r-intro.html", "Chapter 1 Introduction to R 1.1 Installation 1.2 R and RStudio 1.3 Basic Data Types 1.4 Vectors 1.5 Arrays and Matrices 1.6 Factors 1.7 Lists 1.8 Data frames 1.9 Reading Data 1.10 Plots 1.11 Flow of Control 1.12 Rattle", " Chapter 1 Introduction to R The goal of the first part of this book is to get you up to speed with the basics of R as quickly as possible. 1.1 Installation Follow the procedures according to your operating system. Linux: You need to have blas and gfortran installed on your Linux, for installing the ‘coin’ package. Rgraphviz requires installation from source(“http://bioconductor.org/biocLite.R”) biocLite(“Rgraphviz”) 1.2 R and RStudio R is a programming language for statistical computing and data analysis that supports a variety of programming styles. See R in Wikipedia R has multiple online resources and books. R coding style Getting help in R RStudio cheat sheet Base R cheat sheet Advanced R cheat sheet Data Visualization cheat sheet R Markdown cheat sheet [R Markdown Basics] (http://rmarkdown.rstudio.com/authoring_basics.html) help(&quot; “) command R as a calculator. Console: It uses the command-line interface. Examples: x &lt;- c(1,2,3,4,5,6) # Create ordered collection (vector) y &lt;- x^2 # Square the elements of x print(y) # print (vector) y ## [1] 1 4 9 16 25 36 mean(y) # Calculate average (arithmetic mean) of (vector) y; result is scalar ## [1] 15.16667 var(y) # Calculate sample variance ## [1] 178.9667 lm_1 &lt;- lm(y ~ x) # Fit a linear regression model &quot;y = f(x)&quot; or &quot;y = B0 + (B1 * x)&quot; # store the results as lm_1 print(lm_1) # Print the model from the (linear model object) lm_1 ## ## Call: ## lm(formula = y ~ x) ## ## Coefficients: ## (Intercept) x ## -9.333 7.000 summary(lm_1) # Compute and print statistics for the fit ## ## Call: ## lm(formula = y ~ x) ## ## Residuals: ## 1 2 3 4 5 6 ## 3.3333 -0.6667 -2.6667 -2.6667 -0.6667 3.3333 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -9.3333 2.8441 -3.282 0.030453 * ## x 7.0000 0.7303 9.585 0.000662 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.055 on 4 degrees of freedom ## Multiple R-squared: 0.9583, Adjusted R-squared: 0.9478 ## F-statistic: 91.88 on 1 and 4 DF, p-value: 0.000662 # of the (linear model object) lm_1 par(mfrow=c(2, 2)) # Request 2x2 plot layout plot(lm_1) # Diagnostic plot of regression model - R script. # A file with R commands # comments source(“filewithcommands.R”) sink(“recordmycommands.lis”) savehistory() - From command line: + Rscript Rscript file | -e (e.g. Rscript -e 2+2) quit() Variables Operators assign operator &lt;- sequence operator : Example: mynums &lt;- 0:20 arithmetic operators: + - = / ^ %/% (integer division) %% (modulus operator) The workspace. Objects. ls() objects() ls.str() lists and describes rm(x) delete a variable s.str() objects() rm(totalCost) str( ) #(Structure) provides information about the variable -RStudio, RCommander and RKWard are the best known IDEs for R. Four # (‘####’) create an environment in RStudio. An environment binds a set of names to a set of values. You can think of an environment as a bag of names. Environment basics Working directories: # set your working directory # setwd(&quot;~/workingDir/&quot;) getwd() ## [1] &quot;/home/drg/Projects/DASE&quot; # record R commands: # sink(&quot;recordmycommands.txt&quot;, append = TRUE) 1.3 Basic Data Types class( ) logical: TRUE FALSE numeric, integer: is.numeric( ) is.integer( ) character Examples: TRUE ## [1] TRUE class(TRUE) ## [1] &quot;logical&quot; FALSE ## [1] FALSE NA # missing ## [1] NA class(NA) ## [1] &quot;logical&quot; T ## [1] TRUE F ## [1] FALSE # numeric data type 2 ## [1] 2 class(2) ## [1] &quot;numeric&quot; 2.5 ## [1] 2.5 2L # integer ## [1] 2 class(2L) ## [1] &quot;integer&quot; is.numeric(2) ## [1] TRUE is.numeric(2L) ## [1] TRUE is.integer(2) ## [1] FALSE is.integer(2L) ## [1] TRUE data type coercion: as.numeric( ) as.character( ) as.integer( ) Examples: truenum &lt;- as.numeric(TRUE) truenum ## [1] 1 class(truenum) ## [1] &quot;numeric&quot; falsenum &lt;- as.numeric(FALSE) falsenum ## [1] 0 num2char &lt;- as.character(55) num2char ## [1] &quot;55&quot; char2num &lt;- as.numeric(&quot;55.3&quot;) char2int &lt;- as.integer(&quot;55.3&quot;) 1.3.1 Mising values NA Not Available, which is not a number as well. It applies to missing values. NaN means ‘Not a Number’ Examples: NA + 1 ## [1] NA mean(c(5,NA,7)) ## [1] NA mean(c(5,NA,7), na.rm=TRUE) # some functions allow to remove NAs ## [1] 6 1.4 Vectors Examples: phases &lt;- c(&quot;reqs&quot;, &quot;dev&quot;, &quot;test1&quot;, &quot;test2&quot;, &quot;maint&quot;) str(phases) ## chr [1:5] &quot;reqs&quot; &quot;dev&quot; &quot;test1&quot; &quot;test2&quot; &quot;maint&quot; is.vector(phases) ## [1] TRUE thevalues &lt;- c(15, 60, 30, 35, 22) names(thevalues) &lt;- phases str(thevalues) ## Named num [1:5] 15 60 30 35 22 ## - attr(*, &quot;names&quot;)= chr [1:5] &quot;reqs&quot; &quot;dev&quot; &quot;test1&quot; &quot;test2&quot; ... thevalues ## reqs dev test1 test2 maint ## 15 60 30 35 22 # a single value is a vector aphase &lt;- 44 is.vector(aphase) ## [1] TRUE A single value is a vector! Example: aphase &lt;- 44 is.vector(aphase) ## [1] TRUE length(aphase) ## [1] 1 length(thevalues) ## [1] 5 1.4.1 Coercion for vectors thevalues1 &lt;- c(15, 60, &quot;30&quot;, 35, 22) class(thevalues1) ## [1] &quot;character&quot; thevalues1 ## [1] &quot;15&quot; &quot;60&quot; &quot;30&quot; &quot;35&quot; &quot;22&quot; # &lt;- is equivalent to assign ( ) assign(&quot;costs&quot;, c(50, 100, 30)) 1.4.2 Vector arithmetic It is done in all elements. For example: assign(&quot;costs&quot;, c(50, 100, 30)) costs/3 ## [1] 16.66667 33.33333 10.00000 costs - 5 ## [1] 45 95 25 costs &lt;- costs - 5 incomes &lt;- c(200, 800, 10) earnings &lt;- incomes - costs sum(earnings) ## [1] 845 # R recycles values in vectors! Subsetting vectors ### Subsetting vectors [] phase1 &lt;- phases[1] phase1 ## [1] &quot;reqs&quot; phase3 &lt;- phases[3] phase3 ## [1] &quot;test1&quot; thevalues[phase1] ## reqs ## 15 thevalues[&quot;reqs&quot;] ## reqs ## 15 testphases &lt;- phases[c(3,4)] thevalues[testphases] ## test1 test2 ## 30 35 ### Negative indexes phases1 &lt;- phases[-5] phases ## [1] &quot;reqs&quot; &quot;dev&quot; &quot;test1&quot; &quot;test2&quot; &quot;maint&quot; phases1 ## [1] &quot;reqs&quot; &quot;dev&quot; &quot;test1&quot; &quot;test2&quot; #phases2 &lt;- phases[-testphases] ## error in argument phases2 &lt;- phases[-c(3,4)] phases2 ## [1] &quot;reqs&quot; &quot;dev&quot; &quot;maint&quot; ### subset using logical vector phases3 &lt;- phases[c(FALSE, TRUE, TRUE, FALSE)] #recicled first value phases3 ## [1] &quot;dev&quot; &quot;test1&quot; selectionv &lt;- c(FALSE, TRUE, TRUE, FALSE) phases3 &lt;- phases[selectionv] phases3 ## [1] &quot;dev&quot; &quot;test1&quot; selectionvec2 &lt;- c(TRUE, FALSE) thevalues2 &lt;- thevalues[selectionvec2] thevalues2 ## reqs test1 maint ## 15 30 22 ### Generating regular sequenceswith : and seq aseqofvalues &lt;- 1:20 aseqofvalues2 &lt;- seq(from=-3, to=3, by=0.5 ) aseqofvalues2 ## [1] -3.0 -2.5 -2.0 -1.5 -1.0 -0.5 0.0 0.5 1.0 1.5 2.0 2.5 3.0 aseqofvalues3 &lt;- seq(0, 100, by=10) aseqofvalues4 &lt;- aseqofvalues3[c(2, 4, 6, 8)] aseqofvalues4 ## [1] 10 30 50 70 aseqofvalues4 &lt;- aseqofvalues3[-c(2, 4, 6, 8)] aseqofvalues4 ## [1] 0 20 40 60 80 90 100 aseqofvalues3[c(1,2)] &lt;- c(666,888) aseqofvalues3 ## [1] 666 888 20 30 40 50 60 70 80 90 100 ### Logical values in vectors TRUE/FALSE aseqofvalues3 &gt; 50 ## [1] TRUE TRUE FALSE FALSE FALSE FALSE TRUE TRUE TRUE TRUE TRUE aseqofvalues5 &lt;- aseqofvalues3[aseqofvalues3 &gt; 50] aseqofvalues5 ## [1] 666 888 60 70 80 90 100 aseqofvalues6 &lt;- aseqofvalues3[!(aseqofvalues3 &gt; 50)] aseqofvalues6 ## [1] 20 30 40 50 ### Comparison functions aseqofvalues7 &lt;- aseqofvalues3[aseqofvalues3 == 50] aseqofvalues7 ## [1] 50 aseqofvalues8 &lt;- aseqofvalues3[aseqofvalues3 == 22] aseqofvalues8 ## numeric(0) aseqofvalues9 &lt;- aseqofvalues3[aseqofvalues3 != 50] aseqofvalues9 ## [1] 666 888 20 30 40 60 70 80 90 100 logicalcond &lt;- aseqofvalues3 &gt;= 50 aseqofvalues10 &lt;- aseqofvalues3[logicalcond] aseqofvalues10 ## [1] 666 888 50 60 70 80 90 100 ### Remove Missing Values (NAs) aseqofvalues3[c(1,2)] &lt;- c(NA,NA) aseqofvalues3 ## [1] NA NA 20 30 40 50 60 70 80 90 100 aseqofvalues3 &lt;- aseqofvalues3[!is.na(aseqofvalues3)] aseqofvalues3 ## [1] 20 30 40 50 60 70 80 90 100 1.5 Arrays and Matrices mymat &lt;- matrix(1:12, nrow =2) mymat ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] 1 3 5 7 9 11 ## [2,] 2 4 6 8 10 12 mymat &lt;- matrix(1:12, ncol =3) mymat ## [,1] [,2] [,3] ## [1,] 1 5 9 ## [2,] 2 6 10 ## [3,] 3 7 11 ## [4,] 4 8 12 mymat &lt;- matrix(1:12, nrow=2, byrow = TRUE) mymat ## [,1] [,2] [,3] [,4] [,5] [,6] ## [1,] 1 2 3 4 5 6 ## [2,] 7 8 9 10 11 12 mymat &lt;- matrix(1:12, nrow=3, ncol=4) mymat ## [,1] [,2] [,3] [,4] ## [1,] 1 4 7 10 ## [2,] 2 5 8 11 ## [3,] 3 6 9 12 mymat &lt;- matrix(1:12, nrow=3, ncol=4, byrow=TRUE) mymat ## [,1] [,2] [,3] [,4] ## [1,] 1 2 3 4 ## [2,] 5 6 7 8 ## [3,] 9 10 11 12 ### recycling mymat &lt;- matrix(1:5, nrow=3, ncol=4, byrow=TRUE) ## Warning in matrix(1:5, nrow = 3, ncol = 4, byrow = TRUE): data length [5] ## is not a sub-multiple or multiple of the number of rows [3] mymat ## [,1] [,2] [,3] [,4] ## [1,] 1 2 3 4 ## [2,] 5 1 2 3 ## [3,] 4 5 1 2 ### rbind cbind cbind(1:3, 1:3) ## [,1] [,2] ## [1,] 1 1 ## [2,] 2 2 ## [3,] 3 3 rbind(1:3, 1:3) ## [,1] [,2] [,3] ## [1,] 1 2 3 ## [2,] 1 2 3 mymat &lt;- matrix(1) mymat &lt;- matrix(1:8, nrow=2, ncol=4, byrow=TRUE) mymat ## [,1] [,2] [,3] [,4] ## [1,] 1 2 3 4 ## [2,] 5 6 7 8 rbind(mymat, 9:12) ## [,1] [,2] [,3] [,4] ## [1,] 1 2 3 4 ## [2,] 5 6 7 8 ## [3,] 9 10 11 12 mymat &lt;- cbind(mymat, c(5,9)) mymat ## [,1] [,2] [,3] [,4] [,5] ## [1,] 1 2 3 4 5 ## [2,] 5 6 7 8 9 mymat &lt;- matrix(1:8, byrow = TRUE, nrow=2) mymat ## [,1] [,2] [,3] [,4] ## [1,] 1 2 3 4 ## [2,] 5 6 7 8 rownames(mymat) &lt;- c(&quot;row1&quot;, &quot;row2&quot;) mymat ## [,1] [,2] [,3] [,4] ## row1 1 2 3 4 ## row2 5 6 7 8 colnames(mymat) &lt;- c(&quot;col1&quot;, &quot;col2&quot;, &quot;col3&quot;, &quot;col4&quot;) mymat ## col1 col2 col3 col4 ## row1 1 2 3 4 ## row2 5 6 7 8 mymat2 &lt;- matrix(1:12, byrow=TRUE, nrow=3, dimnames=list(c(&quot;row1&quot;, &quot;row2&quot;, &quot;row3&quot;), c(&quot;col1&quot;, &quot;col2&quot;, &quot;col3&quot;, &quot;col4&quot;))) mymat2 ## col1 col2 col3 col4 ## row1 1 2 3 4 ## row2 5 6 7 8 ## row3 9 10 11 12 ### Coercion in Arrays matnum &lt;- matrix(1:8, ncol = 2) matnum ## [,1] [,2] ## [1,] 1 5 ## [2,] 2 6 ## [3,] 3 7 ## [4,] 4 8 matchar &lt;- matrix(LETTERS[1:6], nrow = 4, ncol = 3) matchar ## [,1] [,2] [,3] ## [1,] &quot;A&quot; &quot;E&quot; &quot;C&quot; ## [2,] &quot;B&quot; &quot;F&quot; &quot;D&quot; ## [3,] &quot;C&quot; &quot;A&quot; &quot;E&quot; ## [4,] &quot;D&quot; &quot;B&quot; &quot;F&quot; matchars &lt;- cbind(matnum, matchar) matchars ## [,1] [,2] [,3] [,4] [,5] ## [1,] &quot;1&quot; &quot;5&quot; &quot;A&quot; &quot;E&quot; &quot;C&quot; ## [2,] &quot;2&quot; &quot;6&quot; &quot;B&quot; &quot;F&quot; &quot;D&quot; ## [3,] &quot;3&quot; &quot;7&quot; &quot;C&quot; &quot;A&quot; &quot;E&quot; ## [4,] &quot;4&quot; &quot;8&quot; &quot;D&quot; &quot;B&quot; &quot;F&quot; ### Subsetting mymat3 &lt;- matrix(sample(-8:15, 12), nrow=3) mymat3 ## [,1] [,2] [,3] [,4] ## [1,] -7 -8 5 7 ## [2,] 4 -3 -1 -6 ## [3,] 10 11 12 9 mymat3[2,3] ## [1] -1 mymat3[1,4] ## [1] 7 mymat3[3,] ## [1] 10 11 12 9 mymat3[,4] ## [1] 7 -6 9 mymat3[5] # counts elements by column ## [1] -3 mymat3[9] ## [1] 12 ## Subsetting multiple elements mymat3[2, c(1,3)] ## [1] 4 -1 mymat3[c(2,3), c(1,3,4)] ## [,1] [,2] [,3] ## [1,] 4 -1 -6 ## [2,] 10 12 9 rownames(mymat3) &lt;- c(&quot;r1&quot;, &quot;r2&quot;, &quot;r3&quot;) colnames(mymat3) &lt;- c(&quot;c1&quot;, &quot;c2&quot;, &quot;c3&quot;, &quot;c4&quot;) mymat3[&quot;r2&quot;, c(&quot;c1&quot;, &quot;c3&quot;)] ## c1 c3 ## 4 -1 ### Subset by logical vector mymat3[c(FALSE, TRUE, FALSE), c(TRUE, FALSE, TRUE, FALSE)] ## c1 c3 ## 4 -1 mymat3[c(FALSE, TRUE, TRUE), c(TRUE, FALSE, TRUE, TRUE)] ## c1 c3 c4 ## r2 4 -1 -6 ## r3 10 12 9 ### matrix arithmetic row1 &lt;- c(220, 137) row2 &lt;- c(345, 987) row3 &lt;- c(111, 777) mymat4 &lt;- rbind(row1, row2, row3) rownames(mymat4) &lt;- c(&quot;row_1&quot;, &quot;row_2&quot;, &quot;row_3&quot;) colnames(mymat4) &lt;- c(&quot;col_1&quot;, &quot;col_2&quot;) mymat4 ## col_1 col_2 ## row_1 220 137 ## row_2 345 987 ## row_3 111 777 mymat4/10 ## col_1 col_2 ## row_1 22.0 13.7 ## row_2 34.5 98.7 ## row_3 11.1 77.7 mymat4 -100 ## col_1 col_2 ## row_1 120 37 ## row_2 245 887 ## row_3 11 677 mymat5 &lt;- rbind(c(50,50), c(10,10), c(100,100)) mymat5 ## [,1] [,2] ## [1,] 50 50 ## [2,] 10 10 ## [3,] 100 100 mymat4 - mymat5 ## col_1 col_2 ## row_1 170 87 ## row_2 335 977 ## row_3 11 677 mymat4 * (mymat5/100) ## col_1 col_2 ## row_1 110.0 68.5 ## row_2 34.5 98.7 ## row_3 111.0 777.0 ### index matrices m1 &lt;- array(1:20, dim=c(4,5)) m1 ## [,1] [,2] [,3] [,4] [,5] ## [1,] 1 5 9 13 17 ## [2,] 2 6 10 14 18 ## [3,] 3 7 11 15 19 ## [4,] 4 8 12 16 20 index &lt;- array(c(1:3, 3:1), dim=c(3,2)) index ## [,1] [,2] ## [1,] 1 3 ## [2,] 2 2 ## [3,] 3 1 m1[index] &lt;-0 m1 ## [,1] [,2] [,3] [,4] [,5] ## [1,] 1 5 0 13 17 ## [2,] 2 0 10 14 18 ## [3,] 0 7 11 15 19 ## [4,] 4 8 12 16 20 1.6 Factors Factors in R are stored as a vector of integer values with a corresponding set of character values to use when the factor is displayed. personnel &lt;- c(&quot;Analyst1&quot;, &quot;ManagerL2&quot;, &quot;Analyst1&quot;, &quot;Analyst2&quot;, &quot;Boss&quot;, &quot;ManagerL1&quot;, &quot;ManagerL2&quot;, &quot;Programmer1&quot;, &quot;Programmer2&quot;, &quot;Programmer3&quot;, &quot;Designer1&quot;,&quot;Designer2&quot;, &quot;OtherStaff&quot;) # staff in a company personnel_factors &lt;- factor(personnel) personnel_factors #sorted alphabetically ## [1] Analyst1 ManagerL2 Analyst1 Analyst2 Boss ## [6] ManagerL1 ManagerL2 Programmer1 Programmer2 Programmer3 ## [11] Designer1 Designer2 OtherStaff ## 11 Levels: Analyst1 Analyst2 Boss Designer1 Designer2 ... Programmer3 str(personnel_factors) ## Factor w/ 11 levels &quot;Analyst1&quot;,&quot;Analyst2&quot;,..: 1 7 1 2 3 6 7 9 10 11 ... personnel2 &lt;- factor(personnel, levels = c(&quot;Boss&quot;, &quot;ManagerL1&quot;, &quot;ManagerL2&quot;, &quot;Analyst1&quot;, &quot;Analyst2&quot;, &quot;Designer1&quot;,&quot;Designer2&quot;, &quot;Programmer1&quot;, &quot;Programmer2&quot;, &quot;Programmer3&quot;, &quot;OtherStaff&quot;)) #do not duplicate the same factors personnel2 ## [1] Analyst1 ManagerL2 Analyst1 Analyst2 Boss ## [6] ManagerL1 ManagerL2 Programmer1 Programmer2 Programmer3 ## [11] Designer1 Designer2 OtherStaff ## 11 Levels: Boss ManagerL1 ManagerL2 Analyst1 Analyst2 ... OtherStaff str(personnel2) ## Factor w/ 11 levels &quot;Boss&quot;,&quot;ManagerL1&quot;,..: 4 3 4 5 1 2 3 8 9 10 ... # a factor&#39;s levels will always be character values. levels(personnel2) &lt;- c(&quot;B&quot;, &quot;M1&quot;, &quot;M2&quot;, &quot;A1&quot;, &quot;A2&quot;, &quot;D1&quot;, &quot;D2&quot;, &quot;P1&quot;, &quot;P2&quot;, &quot;P3&quot;, &quot;OS&quot;) personnel2 ## [1] A1 M2 A1 A2 B M1 M2 P1 P2 P3 D1 D2 OS ## Levels: B M1 M2 A1 A2 D1 D2 P1 P2 P3 OS personnel3 &lt;- factor(personnel, levels = c(&quot;Boss&quot;, &quot;ManagerL1&quot;, &quot;ManagerL2&quot;, &quot;Analyst1&quot;, &quot;Analyst2&quot;, &quot;Designer1&quot;,&quot;Designer2&quot;, &quot;Programmer1&quot;, &quot;Programmer2&quot;, &quot;Programmer3&quot;, &quot;OtherStaff&quot;), c(&quot;B&quot;, &quot;M1&quot;, &quot;M2&quot;, &quot;A1&quot;, &quot;A2&quot;, &quot;D1&quot;, &quot;D2&quot;, &quot;P1&quot;, &quot;P2&quot;, &quot;P3&quot;, &quot;OS&quot;)) personnel3 ## [1] A1 M2 A1 A2 B M1 M2 P1 P2 P3 D1 D2 OS ## Levels: B M1 M2 A1 A2 D1 D2 P1 P2 P3 OS ### Nominal versus ordinal, ordered factors personnel3[1] &lt; personnel3[2] # error, factors not ordered ## Warning in Ops.factor(personnel3[1], personnel3[2]): &#39;&lt;&#39; not meaningful for ## factors ## [1] NA tshirts &lt;- c(&quot;M&quot;, &quot;L&quot;, &quot;S&quot;, &quot;S&quot;, &quot;L&quot;, &quot;M&quot;, &quot;L&quot;, &quot;M&quot;) tshirt_factor &lt;- factor(tshirts, ordered = TRUE, levels = c(&quot;S&quot;, &quot;M&quot;, &quot;L&quot;)) tshirt_factor ## [1] M L S S L M L M ## Levels: S &lt; M &lt; L tshirt_factor[1] &lt; tshirt_factor[2] ## [1] TRUE 1.7 Lists ‘[’ returns a list ‘[[’ returns the list element ‘$’ returns the content of that element in the list c(&quot;R good times&quot;, 190, 5) ## [1] &quot;R good times&quot; &quot;190&quot; &quot;5&quot; song &lt;- list(&quot;R good times&quot;, 190, 5) is.list(song) ## [1] TRUE str(song) ## List of 3 ## $ : chr &quot;R good times&quot; ## $ : num 190 ## $ : num 5 names(song) &lt;- c(&quot;title&quot;, &quot;duration&quot;, &quot;track&quot;) song ## $title ## [1] &quot;R good times&quot; ## ## $duration ## [1] 190 ## ## $track ## [1] 5 song$title ## [1] &quot;R good times&quot; song2 &lt;- list(title=&quot;Good Friends&quot;, duration = 125, track = 2, rank = 6) song3 &lt;- list(title=&quot;Many Friends&quot;, duration = 125, track= 2, rank = 1, similar2 = song2) song[1] ## $title ## [1] &quot;R good times&quot; song$title ## [1] &quot;R good times&quot; str(song[1]) ## List of 1 ## $ title: chr &quot;R good times&quot; song[[1]] ## [1] &quot;R good times&quot; str(song[[1]]) ## chr &quot;R good times&quot; song2[3] ## $track ## [1] 2 song3[5] # a list ## $similar2 ## $similar2$title ## [1] &quot;Good Friends&quot; ## ## $similar2$duration ## [1] 125 ## ## $similar2$track ## [1] 2 ## ## $similar2$rank ## [1] 6 str(song3[5]) ## List of 1 ## $ similar2:List of 4 ## ..$ title : chr &quot;Good Friends&quot; ## ..$ duration: num 125 ## ..$ track : num 2 ## ..$ rank : num 6 song3[[5]] ## $title ## [1] &quot;Good Friends&quot; ## ## $duration ## [1] 125 ## ## $track ## [1] 2 ## ## $rank ## [1] 6 song3$similar2 ## $title ## [1] &quot;Good Friends&quot; ## ## $duration ## [1] 125 ## ## $track ## [1] 2 ## ## $rank ## [1] 6 song[c(1,3)] ## $title ## [1] &quot;R good times&quot; ## ## $track ## [1] 5 str(song[c(1,3)]) ## List of 2 ## $ title: chr &quot;R good times&quot; ## $ track: num 5 result &lt;- song[c(1,3)] result[1] ## $title ## [1] &quot;R good times&quot; result[[1]] ## [1] &quot;R good times&quot; str(result) ## List of 2 ## $ title: chr &quot;R good times&quot; ## $ track: num 5 result$title ## [1] &quot;R good times&quot; result$track ## [1] 5 # access with [[ to content song3[[5]][[1]] ## [1] &quot;Good Friends&quot; song3$similar2[[1]] ## [1] &quot;Good Friends&quot; # Subsets ### subset by names song[c(&quot;title&quot;, &quot;track&quot;)] ## $title ## [1] &quot;R good times&quot; ## ## $track ## [1] 5 song3[&quot;similar2&quot;] ## $similar2 ## $similar2$title ## [1] &quot;Good Friends&quot; ## ## $similar2$duration ## [1] 125 ## ## $similar2$track ## [1] 2 ## ## $similar2$rank ## [1] 6 resultsimilar &lt;- song3[&quot;similar2&quot;] str(resultsimilar) ## List of 1 ## $ similar2:List of 4 ## ..$ title : chr &quot;Good Friends&quot; ## ..$ duration: num 125 ## ..$ track : num 2 ## ..$ rank : num 6 resultsimilar1 &lt;-song3[[&quot;similar2&quot;]] str(resultsimilar1) ## List of 4 ## $ title : chr &quot;Good Friends&quot; ## $ duration: num 125 ## $ track : num 2 ## $ rank : num 6 resultsimilar1$title ## [1] &quot;Good Friends&quot; # subset by logicals song[c(TRUE, FALSE, TRUE, FALSE)] ## $title ## [1] &quot;R good times&quot; ## ## $track ## [1] 5 result3 &lt;- song[c(TRUE, FALSE, TRUE, FALSE)] # is a list of two elements # extending the list shared &lt;- c(&quot;Hillary&quot;, &quot;Javi&quot;, &quot;Mikel&quot;, &quot;Patty&quot;) song3$shared &lt;- shared str(song3) ## List of 6 ## $ title : chr &quot;Many Friends&quot; ## $ duration: num 125 ## $ track : num 2 ## $ rank : num 1 ## $ similar2:List of 4 ## ..$ title : chr &quot;Good Friends&quot; ## ..$ duration: num 125 ## ..$ track : num 2 ## ..$ rank : num 6 ## $ shared : chr [1:4] &quot;Hillary&quot; &quot;Javi&quot; &quot;Mikel&quot; &quot;Patty&quot; cities &lt;- list(&quot;Bilbao&quot;, &quot;New York&quot;, &quot;Tartu&quot;) song3[[&quot;cities&quot;]] &lt;- cities str(song3) ## List of 7 ## $ title : chr &quot;Many Friends&quot; ## $ duration: num 125 ## $ track : num 2 ## $ rank : num 1 ## $ similar2:List of 4 ## ..$ title : chr &quot;Good Friends&quot; ## ..$ duration: num 125 ## ..$ track : num 2 ## ..$ rank : num 6 ## $ shared : chr [1:4] &quot;Hillary&quot; &quot;Javi&quot; &quot;Mikel&quot; &quot;Patty&quot; ## $ cities :List of 3 ## ..$ : chr &quot;Bilbao&quot; ## ..$ : chr &quot;New York&quot; ## ..$ : chr &quot;Tartu&quot; 1.8 Data frames thenames &lt;- c(&quot;Ane&quot;, &quot;Mike&quot;, &quot;Xabi&quot;, &quot;Viktoria&quot;, &quot;Edurne&quot;) ages &lt;- c(44, 20, 33, 15, 65) employee &lt;- c(FALSE, FALSE, TRUE, TRUE, FALSE) mydataframe &lt;- data.frame(thenames, ages, employee) mydataframe ## thenames ages employee ## 1 Ane 44 FALSE ## 2 Mike 20 FALSE ## 3 Xabi 33 TRUE ## 4 Viktoria 15 TRUE ## 5 Edurne 65 FALSE names(mydataframe) &lt;- c(&quot;FirstName&quot;, &quot;Age&quot;, &quot;Employee&quot;) str(mydataframe) ## &#39;data.frame&#39;: 5 obs. of 3 variables: ## $ FirstName: Factor w/ 5 levels &quot;Ane&quot;,&quot;Edurne&quot;,..: 1 3 5 4 2 ## $ Age : num 44 20 33 15 65 ## $ Employee : logi FALSE FALSE TRUE TRUE FALSE #strings are not factors! mydataframe &lt;- data.frame(thenames, ages, employee, stringsAsFactors=FALSE) names(mydataframe) &lt;- c(&quot;FirstName&quot;, &quot;Age&quot;, &quot;Employee&quot;) str(mydataframe) ## &#39;data.frame&#39;: 5 obs. of 3 variables: ## $ FirstName: chr &quot;Ane&quot; &quot;Mike&quot; &quot;Xabi&quot; &quot;Viktoria&quot; ... ## $ Age : num 44 20 33 15 65 ## $ Employee : logi FALSE FALSE TRUE TRUE FALSE # subset data frame mydataframe[4,2] ## [1] 15 mydataframe[4, &quot;Age&quot;] ## [1] 15 mydataframe[, &quot;FirstName&quot;] ## [1] &quot;Ane&quot; &quot;Mike&quot; &quot;Xabi&quot; &quot;Viktoria&quot; &quot;Edurne&quot; mydataframe[c(2,5), c(&quot;Age&quot;, &quot;Employee&quot;)] ## Age Employee ## 2 20 FALSE ## 5 65 FALSE matfromframe &lt;- as.matrix(mydataframe[c(2,5), c(&quot;Age&quot;, &quot;Employee&quot;)]) str(matfromframe) ## num [1:2, 1:2] 20 65 0 0 ## - attr(*, &quot;dimnames&quot;)=List of 2 ## ..$ : chr [1:2] &quot;2&quot; &quot;5&quot; ## ..$ : chr [1:2] &quot;Age&quot; &quot;Employee&quot; mydataframe[3] ## Employee ## 1 FALSE ## 2 FALSE ## 3 TRUE ## 4 TRUE ## 5 FALSE # convert to vector mydf0 &lt;- mydataframe[3] #data.frame str(mydf0) ## &#39;data.frame&#39;: 5 obs. of 1 variable: ## $ Employee: logi FALSE FALSE TRUE TRUE FALSE myvec &lt;- mydataframe[[3]] #vector str(myvec) ## logi [1:5] FALSE FALSE TRUE TRUE FALSE mydf0asvec &lt;- as.vector(mydataframe[3]) # but it doesn&#39;t work . Use [[]] str(mydf0asvec) ## &#39;data.frame&#39;: 5 obs. of 1 variable: ## $ Employee: logi FALSE FALSE TRUE TRUE FALSE mydf0asvec &lt;- as.vector(mydataframe[[3]]) str(mydf0asvec) ## logi [1:5] FALSE FALSE TRUE TRUE FALSE # add column height &lt;- c(166, 165, 158, 176, 199) weight &lt;- c(66, 77, 99, 88, 109) mydataframe$height &lt;- height mydataframe[[&quot;weight&quot;]] &lt;- weight mydataframe ## FirstName Age Employee height weight ## 1 Ane 44 FALSE 166 66 ## 2 Mike 20 FALSE 165 77 ## 3 Xabi 33 TRUE 158 99 ## 4 Viktoria 15 TRUE 176 88 ## 5 Edurne 65 FALSE 199 109 # add a column birthplace &lt;- c(&quot;Tallinn&quot;, &quot;London&quot;, &quot;Donostia&quot;, &quot;Paris&quot;, &quot;New York&quot;) mydataframe &lt;- cbind(mydataframe, birthplace) mydataframe ## FirstName Age Employee height weight birthplace ## 1 Ane 44 FALSE 166 66 Tallinn ## 2 Mike 20 FALSE 165 77 London ## 3 Xabi 33 TRUE 158 99 Donostia ## 4 Viktoria 15 TRUE 176 88 Paris ## 5 Edurne 65 FALSE 199 109 New York # add a row anton &lt;- data.frame(FirstName = &quot;Anton&quot;, Age = 77, Employee=TRUE, height= 170, weight = 65, birthplace =&quot;Amsterdam&quot;, stringsAsFactors=FALSE) mydataframe &lt;- rbind (mydataframe, anton) mydataframe ## FirstName Age Employee height weight birthplace ## 1 Ane 44 FALSE 166 66 Tallinn ## 2 Mike 20 FALSE 165 77 London ## 3 Xabi 33 TRUE 158 99 Donostia ## 4 Viktoria 15 TRUE 176 88 Paris ## 5 Edurne 65 FALSE 199 109 New York ## 6 Anton 77 TRUE 170 65 Amsterdam # sorting mydataframeSorted &lt;- mydataframe[order(mydataframe$Age, decreasing = TRUE), ] #all columns mydataframeSorted ## FirstName Age Employee height weight birthplace ## 6 Anton 77 TRUE 170 65 Amsterdam ## 5 Edurne 65 FALSE 199 109 New York ## 1 Ane 44 FALSE 166 66 Tallinn ## 3 Xabi 33 TRUE 158 99 Donostia ## 2 Mike 20 FALSE 165 77 London ## 4 Viktoria 15 TRUE 176 88 Paris mydataframeSorted2 &lt;- mydataframe[order(mydataframe$Age, decreasing = TRUE), c(1,2,6) ] mydataframeSorted2 ## FirstName Age birthplace ## 6 Anton 77 Amsterdam ## 5 Edurne 65 New York ## 1 Ane 44 Tallinn ## 3 Xabi 33 Donostia ## 2 Mike 20 London ## 4 Viktoria 15 Paris 1.9 Reading Data library(foreign) isbsg &lt;- read.arff(&quot;datasets/effortEstimation/isbsg10teaser.arff&quot;) mydataISBSG &lt;- isbsg[, c(&quot;FS&quot;, &quot;N_effort&quot;)] str(mydataISBSG) ## &#39;data.frame&#39;: 37 obs. of 2 variables: ## $ FS : num 225 599 333 748 158 427 461 257 115 116 ... ## $ N_effort: num 1856 10960 5661 1518 3670 ... 1.10 Plots There are several graphic packages that are recommended, in particular ggplot. However, there is some basic support in the R base for graphics. The following Figure 1.1 shows a simple plot. plot(mydataISBSG$FS, mydataISBSG$N_effort) Figure 1.1: Simple plot 1.11 Flow of Control Ifelse: library(foreign) kc1 &lt;- read.arff(&quot;datasets/defectPred/D1/KC1.arff&quot;) kc1$Defective &lt;- ifelse(kc1$Defective == &quot;Y&quot;, 1, 0) head(kc1, 1) ## LOC_BLANK BRANCH_COUNT LOC_CODE_AND_COMMENT LOC_COMMENTS ## 1 0 1 0 0 ## CYCLOMATIC_COMPLEXITY DESIGN_COMPLEXITY ESSENTIAL_COMPLEXITY ## 1 1 1 1 ## LOC_EXECUTABLE HALSTEAD_CONTENT HALSTEAD_DIFFICULTY HALSTEAD_EFFORT ## 1 3 11.58 2.67 82.35 ## HALSTEAD_ERROR_EST HALSTEAD_LENGTH HALSTEAD_LEVEL HALSTEAD_PROG_TIME ## 1 0.01 11 0.38 4.57 ## HALSTEAD_VOLUME NUM_OPERANDS NUM_OPERATORS NUM_UNIQUE_OPERANDS ## 1 30.88 4 7 3 ## NUM_UNIQUE_OPERATORS LOC_TOTAL Defective ## 1 4 5 0 1.12 Rattle There is graphical interface, Rattle, that allow us to perform some data mining tasks (Williams 2011). Rattle: GUI for Data mining with R We will provide an overview of data analsyis using different techniques. References "],
["what-is-data-mining-knowledge-discovery-in-databases-kdd.html", "Chapter 2 What is Data Mining / Knowledge Discovery in Databases (KDD) 2.1 The Aim of Data Analysis and Statistical Learning 2.2 Basic References 2.3 Data Mining with R 2.4 Data Mining with Weka", " Chapter 2 What is Data Mining / Knowledge Discovery in Databases (KDD) The non-trivial process of identifying valid, novel, potentially useful, and ultimately understandable patterns in data (Fayyad, Piatetsky-Shapiro, and Smyth 1996) KDD Process The Cross Industry Process for Data Mining (CRISP-DM, 1996) also provides a common and well-developed framework for delivering data mining projects identifying six steps: Problem Understanding Data Understanding Data Preparation Modeling Evaluation Deployment 2.1 The Aim of Data Analysis and Statistical Learning The aim of any data analysis is to understand the data and to build models for making predictions and estimating future events based on past data and to make statistical inferences from our data. We may want to test different hypothesis on the data We want to generate conclusions about the population where our sample data comes from Most probably we are interested in building a model for quality, time, defects or effort prediction We want to find a function \\(f()\\), that given \\(X1, X2, ...\\) computes \\(Y=f(X1, X2, ..., Xn)\\) 2.2 Basic References Generic books about statistics: John Verzani, simpleR - Using R for Introductory Statistics Peter Dalgaard, Introductory Statistics with R, 2nd Edt., Springer, 2008 Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani, An Introduction to Statistical Learning with Applications in R, Springer, 2013 Geoff Cumming, Understanding the New Statistics: Effect Sizes, Confidence Intervals, and Meta-Analysis, Routledge, New York, 2012 2.3 Data Mining with R Graham Williams, Data Mining with Rattle and R: The Art of Excavating Data for Knowledge Discovery, Springer 2011 Also the author maintains a Web site: http://rattle.togaware.com/ Luis Torgo, Data Mining with R: Learning with Case Studies, Chapman and Hall/CRC, 2010 http://www.rdatamining.com/ 2.4 Data Mining with Weka Weka is another popular framework in Java: Ian Witten, Eibe Frank, Mark Hall, Christopher J. Pal, Data Mining: Practical Machine Learning Tools and Techniques (4th Edt), Morgan Kaufmann, 2016, ISBN: 978-0128042915 References "],
["data-sources-in-software-engineering.html", "Chapter 3 Data Sources in Software Engineering 3.1 Types of information stored in the repositories 3.2 Repositories 3.3 Some Tools/Dashboards to extract data 3.4 Effort Estimation Data in Software Engineering", " Chapter 3 Data Sources in Software Engineering We classify this trail in the following categories: Source code can be studied to measure its properties, such as size or complexity. Source Code Management Systems (SCM) make it possible to store all the changes that the different source code files undergo during the project. Also, SCM systems allow for work to be done in parallel by different developers over the same source code tree. Every change recorded in the system is accompanied with meta-information (author, date, reason for the change, etc) that can be used for research purposes. Issue or Bug tracking systems (ITS). Bugs, defects and user requests are managed in ISTs, where users and developers can fill tickets with a description of a defect found, or a desired new functionality. All the changes to the ticket are recorded in the system, and most of the systems also record the comments and communications among all the users and developers implied in the task. Messages between developers and users. In the case of free/open source software, the projects are open to the world, and the messages are archived in the form of mailing lists and social networks which can also be mined for research purposes. There are also some other open message systems, such as IRC or forums. Meta-data about the projects. As well as the low level information of the software processes, we can also find meta-data about the software projects which can be useful for research. This meta-data may include intended-audience, programming language, domain of application, license (in the case of open source), etc. Usage data. There are statistics about software downloads, logs from servers, software reviews, etc. 3.1 Types of information stored in the repositories Meta-information about the project itself and the people that participated. Low-level information Mailing Lists (ML) Bugs Tracking Systems (BTS) or Project Tracker System (PTS) Software Configuration Management Systems (SCM) Processed information. For example project management information about the effort estimation and cost of the project. Whether the repository is public or not Single project vs. multiprojects. Whether the repository contains information of a single project with multiples versions or multiples projects and/or versions. Type of content, open source or industrial projects Format in which the information is stored and formats or technologies for accessing the information: Text. It can be just plain text, CSV (Comma Separated Values) files, Attribute-Relation File Format (ARFF) or its variants Through databases. Downloading dumps of the database. Remote access such as APIs of Web services or REST 3.2 Repositories There is a number of open research repositories in Software Engineering. Among them: FLOSSMole (Howison, Conklin, and Crowston 2006) http://flossmole.org/ FLOSSMetrics (Herraiz et al. 2009): http://flossmetrics.org/ PROMISE (PRedictOr Models In Software Engineering) [8]: http://openscience.us/repo/ Qualitas Corpus (QC) (Tempero et al. 2010): http://qualitascorpus.com/ Sourcerer Project (Linstead et al. 2009): http://sourcerer.ics.uci.edu/ Ultimate Debian Database (UDD) (Nussbaum and Zacchiroli 2010) http://udd.debian.org/ SourceForge Research Data Archive (SRDA) (Van Antwerp and Madey 2008) http://zerlot.cse.nd.edu/ SECOLD (Source code ECOsystem Linked Data): http://www.secold.org/ Software-artifact Infrastructure Repository (SIR) [http://sir.unl.edu] OpenHub: https://www.openhub.net/ Not openly available: The International Software Benchmarking Standards Group (ISBSG) http://www.isbsg.org/ TukuTuku http://www.metriq.biz/tukutuku/ Some papers and publications/theses that have been used in the literature: Helix Data Set (Vasa 2010): http://www.ict.swin.edu.au/research/projects/helix/ Bug Prediction Dataset (BPD) (D’Ambros, Lanza, and Robbes 2010,D’Ambros, Lanza, and Robbes (2011)): http://bug.inf.usi.ch/ Eclipse Bug Data (EBD) (Zimmermann, Premraj, and Zeller 2007,Nagappan et al. (2012)): http://www.st.cs.uni-saarland.de/softevo/bug-data/eclipse/ 3.3 Some Tools/Dashboards to extract data Within the open source community, two toolkits allow us to extract data that can be used to explore projects: Metrics Grimoire http://metricsgrimoire.github.io/ SonarQube http://www.sonarqube.org/ 3.4 Effort Estimation Data in Software Engineering It is worth highlighting the case of software effort estimation datasets with their peculiarities. First, most effort estimation datasets used in the literature are scattered through research papers with the exception of a few kept in the PROMISE repository. Mair et al (2005) also have analysed available datasets in the field of cost estimation identifying 65 different datasets in 50 papers. Second, their size is very small with the exception of ISBSG repository discussed previously which a small sample is available through PROMISE and the China dataset with 499 instances. Third, some can be quite old in a context and time that is not applicable to current development environments. The authors noted that the oldest datasets (COCOMO, Desharnais, Kemerer and Albrecht and Gaffney) tend to be the most studied ones and a subset of the most relevant ones. Also, from the artificial intelligence or data mining point of view effort estimation has been mainly tackled with different types of regression techniques and more recently with techniques which are also typically considered under the umbrella of data mining techniques. However, as the number of examples per dataset is increasing, other machine learning techniques are also being studied (e.g.: Dejaeger et al (2012) report on a comparison of several machine learning techniques to effort estimation with only 5 out the 9 used datasets publicly available). From the data mining point of view, the small number of instances hinders the application of machine learning techniques. However, software effort and cost estimation still remain one of the main challenges in software engineering and have attracted a great deal of interest by many researchers (2007). For example, there are continuous analyses of whether software development follows economies or diseconomies of scale (see (J.J Dolado 2001,Banker, Chang, and Kemerer (1994),B. A. Kitchenham (2002))). Next Table 3.1 (following Mair et al (2005) ) shows the most open cost/effort datasets available in the literature with their main reference. Table 3.1: Effort Estimation Dataset from articles Reference Instances Attributes Abran and Robillard (1996) 21 31 Albrecht-Gaffney (1983) 24 7 Bailey and Basili (1981) 18 9 Belady and Lehman (1979) 33 Boehm (aka COCOMO Dataset) (1981) 63 43 Desharnais (1988) 61 10 Dolado (1997) 24 7 Hastings and Sajeev (2001) 8 14 Heiat and Heiat (A. Heiat and Heiat 1997) 35 4 Jeffery and Stathis (1996) 17 7 Jorgensen (2004) 47 4 Jorgensen et al. (2003) 20 4 Kemerer (1987) 15 5 Kitchenham (Mermaid 2) (2002) 30 5 Kitchenham et al. (CSC) (2002) 145 9 Kitchenham and Taylor (ICL) (1985) 10 6 Kitchenham and Taylor (BT System X) (1985) 10 3 Kitchenham and Taylor (BT Software Houses) (1985) 12 6 Keung et al. (China dataset) (n.d.)1 499 18 Li et al.(USP05) (2007)2 202 16 Mišić and Tevsić (1998) 6 16 Maxwell (Dev Effort) (2002) 63 32 Maxwell (Maintenance Eff) (2002) 67 28 Miyazaki et al. (1994) 47 9 Moser et al. (1999) 37 4 Shepperd and Cartwright (Shepperd and Cartwright 2001) 39 3 Shepperd and Schofield (Telecom 1) (1997) 18 5 Schofield (real-time 1) (1998,Shepperd and Schofield (1997)) 21 4 Schofield (Mermaid) (1998) 30 18 Schofield (Finnish) (1998) 39 30 Schofield (Hughes) (1998) 33 14 Woodfield et al. (1981) 63 8 References "],
["exploratory-data-analysis.html", "Chapter 4 Exploratory Data Analysis 4.1 Descriptive statistics 4.2 Basic Plots 4.3 Normality 4.4 Running Example 4.5 Correlation 4.6 Confidence Intervals. Bootstrap 4.7 Nonparametric Bootstrap", " Chapter 4 Exploratory Data Analysis 4.1 Descriptive statistics The first task with any dataset is to characterise it in terms of summary statistics and graphics Displaying information graphically will help us to identify the main characteristics of the data. To describe a distribution we often want to know where it is centered and and what the spread is (mean, median, quantiles) 4.2 Basic Plots + **Histogram**: The histogram defines a sequence of breaks and then counts the number of observations in the bins formed by the breaks. + **Boxplot**: The boxplot is used to summarize data succinctly, quickly displaying if the data is symmetric or has suspected outliers. ![](./figures/boxplotexp.png) + **Q-Q plot**: This plot is used to determine if the data is close to being normally distributed. The quantiles of the standard normal distribution is represented by a straight line. The normality of the data can be evaluated by observing the extent in which the points appear on the line. When the data is normally distributed around the mean, then the mean and the median should be equal. + **Scatterplot**: A scatter plot provides a graphical view of the relationship between two sets of numbers: one numerical variable against another. + **Kernel Density Plot**: This plot visualizes the underlying distribution of a variable. Kernel density estimatiion is a non-parametric method of estimating the probability density function of continuous random variable. It helps to identify the distribution of the variable. + **Violin Plot**: A violin plot is a combination of a boxplot and a kernel density plot. 4.3 Normality A normal distribution is an arrangement of a data set in which most values cluster in the middle of the range A graphical representation of a normal distribution is sometimes called a bell curve because of its shape. Many procedures in statistics are based on this property. Parametric procedures require the normality property. In a normal distribution about 95% of the probability lies within 2 Standard Deviations of the mean. Two examples: one population with mean 60 and the standard deviation of 1, and the other with mean 60 and \\(sd=4\\) (means shifted to 0) main.title &lt;- &quot;Area within 2 SD of the mean&quot; par(mfrow=c(1,2)) plot(function(x) dnorm(x, mean = 0, sd = 1), xlim = c(-3, 3), main = &quot;SD 1&quot;, xlab = &quot;x&quot;, ylab = &quot;&quot;, cex = 2) segments(-2, 0, -2, 0.4) segments(2, 0, 2, 0.4) plot(function(x) dnorm(x, mean = 0, sd = 4), xlim = c(-12, 12), main = &quot;SD 4&quot;, xlab = &quot;x&quot;, ylab = &quot;&quot;, cex = 2) segments(-8, 0, -8, 0.1) segments(8, 0, 8, 0.1) if we sample from this population we get “another population”.fig.cap=“Simple plot” sample.means &lt;- rep(NA, 1000) for (i in 1:1000) { sample.40 &lt;- rnorm(40, mean = 60, sd = 4) #rnorm generates random numbers from normal distribution sample.means[i] &lt;- mean(sample.40) } means40 &lt;- mean(sample.means) sd40 &lt;- sd(sample.means) means40 ## [1] 60.00738 sd40 ## [1] 0.6483591 These sample means are another “population”. The sampling distribution of the sample mean is normally distributed meaning that the “mean of a representative sample provides an estimate of the unknown population mean”. This is shown in Figure 4.1 hist(sample.means) Figure 4.1: Sample means histogram 4.4 Running Example Set the path to your files Read the Telecom1 dataset and print out the summary statistics with the command summary options(digits=3) telecom1 &lt;- read.table(&quot;./datasets/effortEstimation/Telecom1.csv&quot;, sep=&quot;,&quot;,header=TRUE, stringsAsFactors=FALSE, dec = &quot;.&quot;) #read data summary(telecom1) ## size effort EstTotal ## Min. : 3.0 Min. : 24 Min. : 30 ## 1st Qu.: 37.2 1st Qu.: 119 1st Qu.:142 ## Median : 68.5 Median : 222 Median :289 ## Mean :100.3 Mean : 284 Mean :320 ## 3rd Qu.:164.0 3rd Qu.: 352 3rd Qu.:472 ## Max. :284.0 Max. :1116 Max. :777 We see that this dataset has three variables (or parameters) and few data points (18) size: the independent variable effort: the dependent variable EstTotal: the estimates coming from an estimation method Basic Plots par(mfrow=c(1,2)) #n figures per row size_telecom1 &lt;- telecom1$size effort_telecom1 &lt;- telecom1$effort hist(size_telecom1, col=&quot;blue&quot;, xlab=&#39;size&#39;, ylab = &#39;Probability&#39;, main = &#39;Histogram of project Size&#39;) lines(density(size_telecom1, na.rm = T, from = 0, to = max(size_telecom1))) plot(density(size_telecom1)) hist(effort_telecom1, col=&quot;blue&quot;) plot(density(effort_telecom1)) boxplot(size_telecom1) boxplot(effort_telecom1) # violin plots for those two variables library(vioplot) vioplot(size_telecom1, names = &#39;&#39;) title(&quot;Violin Plot of Project Size&quot;) vioplot(effort_telecom1, names = &#39;&#39;) title(&quot;Violin Plot of Project Effort&quot;) par(mfrow=c(1,1)) qqnorm(size_telecom1, main=&quot;Q-Q Plot of &#39;size&#39;&quot;) qqline(size_telecom1, col=2, lwd=2, lty=2) #draws a line through the first and third quartiles qqnorm(effort_telecom1, main=&quot;Q-Q Plot of &#39;effort&#39;&quot;) qqline(effort_telecom1) - We observe the non-normality of the data. We may look the posible relationship between size and effort with a scatterplot plot(size_telecom1, effort_telecom1) 4.4.1 Example with the China dataset (Promise Repository) library(foreign) china &lt;- read.arff(&quot;./datasets/effortEstimation/china.arff&quot;) china_size &lt;- china$AFP summary(china_size) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 9 100 215 487 438 17500 china_effort &lt;- china$Effort summary(china_effort) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 26 704 1830 3920 3830 54600 par(mfrow=c(1,2)) hist(china_size, col=&quot;blue&quot;, xlab=&quot;Adjusted Function Points&quot;, main=&quot;Distribution of AFP&quot;) hist(china_effort, col=&quot;blue&quot;,xlab=&quot;Effort&quot;, main=&quot;Distribution of Effort&quot;) boxplot(china_size) boxplot(china_effort) qqnorm(china_size) qqline(china_size) qqnorm(china_effort) qqline(china_effort) - We observe the non-normality of the data. 4.4.1.1 Normality. Galton data It is the data based on the famous 1885 Francis Galton’s study about the relationship between the heights of adult children and the heights of their parents. 4.4.1.2 Normalization Take \\(log\\)s in both independent variables. For example, with the China dataset. If the \\(log\\) transformation is used the estimation equation is: \\[y= e^{b_0 + b_1 log(x)} \\] 4.5 Correlation Correlation is a statistical relationship between two sets of data. With the whole dataset we may check for the linear Correlation of the variables we are interested in. As an example with the China dataset par(mfrow=c(1,1)) plot(china_size,china_effort) cor(china_size,china_effort) ## [1] 0.685 cor.test(china_size,china_effort) ## ## Pearson&#39;s product-moment correlation ## ## data: china_size and china_effort ## t = 20, df = 500, p-value &lt;2e-16 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## 0.635 0.729 ## sample estimates: ## cor ## 0.685 cor(china_size,china_effort, method=&quot;spearman&quot;) ## [1] 0.649 cor(china_size,china_effort, method=&quot;kendall&quot;) ## [1] 0.468 4.6 Confidence Intervals. Bootstrap Until now we have generated point estimates A confidence interval (CI) is an interval estimate of a population parameter. The parameter can be the mean, the median or other. The frequentist CI is an observed interval that is different from sample to sample. It frequently includes the value of the unobservable parameter of interes if the experiment is repeated. The confidence level is the value that measures the frequency that the constructed intervals contain the true value of the parameter. The construction of a confidence interval with an exact value of confidence level for a distribution requires some statistical properties. Usually, normality is one of the properties required for computing confidence intervals. Not all confidence intervals contain the true value of the parameter. Simulation of confidence intervals An example from Ugarte et al. (Ugarte, Militino, and Arnholt 2015) set.seed(10) norsim(sims = 100, n = 36, mu = 100, sigma = 18, conf.level = 0.95) The range defined by the confidence interval will vary with each sample, because the sample size will vary each time and the standard deviation will vary too. 95% confidence interval: it is the probability that the hypothetical confidence intervals (that would be computed from the hypothetical repeated samples) will contain the population mean. the particular interval that we compute on one sample does not mean that the population mean lies within that interval with a probability of 95%. Recommended reading: (Hoekstra et al. 2014) Robust misinterpretation of confidence intervals 4.7 Nonparametric Bootstrap For computing CIs the important thing is to know the assumptions that are made to “know” the distribution of the statistic. There is a way to compute confidence intervals without meeting the requirements of parametric methods. Resampling or bootstraping is a method to calculate estimates of a parameter taking samples from the original data and using those resamples to calculate statistics. Using the resamples usually gives more accurate results than using the original single sample to calculate an estimate of a parameter. - An example of bootstrap CI can be found in Chapter 15, “Evaluation in Software Engineering” References "],
["classical-hypothesis-testing.html", "Chapter 5 Classical Hypothesis Testing 5.1 p-values", " Chapter 5 Classical Hypothesis Testing By “classical” we mean the standard “frequentist” approach to hypothesis testing. The “frequentist” approach to probability sees it as the frequency of events in the long run. We repeat experiments over and over and we count the times that our object of interest appears in the sequence. The classical approach is usually called null hypothesis significance testing (NHST) because the process starts by setting a null hypothesis \\(H_0\\) which is the opposite about what we think is true. The rationale of the process is that the statistical hypothesis should be falsifiable, that is, we can find evidence that the hypothesis is not true. We try to find evidence against the null hypothesis in order to support our alternative hypothesis \\(H_A\\) Usually, the null hypothesis is described as the situation of “no effect” and the alternative hypothesis describes the effect that we are looking for. After collecting data, taking an actual sample, we measure the distance of our parameter of interest from the hypothesized population parameter, and use the facts of the sampling distribution to determine the probability of obtaining such a sample assuming the hypothesis is true. This is amounts to a test of the hypothesis. If the probability of our sample, given the null hypothesis is high, this provides evidence that the null hypothesis is true. Conversely, if the probability of the sample is low (given the hypothesis), this is evidence against the null hypothesis. The hypothesis being tested in this way is named the null hypothesis. The goal of the test is to determine if the null hypothesis can be rejected. A statistical test can either reject or fail to reject a null hypothesis, but never prove it true. We can make two types of errors: false positive (Type I) and false negative (Type II) Type I and Type II errors Two-tailed NHST One-tailed NHST elementary example data = c(52.7, 53.9, 41.7, 71.5, 47.6, 55.1, 62.2, 56.5, 33.4, 61.8, 54.3, 50.0, 45.3, 63.4, 53.9, 65.5, 66.6, 70.0, 52.4, 38.6, 46.1, 44.4, 60.7, 56.4); t.test(data, mu=50, alternative = &#39;greater&#39;) ## ## One Sample t-test ## ## data: data ## t = 2, df = 20, p-value = 0.02 ## alternative hypothesis: true mean is greater than 50 ## 95 percent confidence interval: ## 50.9 Inf ## sample estimates: ## mean of x ## 54.3 Keeping this simple, we could start hypothesis testing about one sample median with the wilcoxon test for non-normal distributions. “ae” is the absolute error in the China Test data median(ae) ## [1] 867 mean(ae) ## [1] 1867 wilcox.test(ae, mu=800, alternative = &#39;greater&#39;) #change the values of mu and see the results ## ## Wilcoxon signed rank test with continuity correction ## ## data: ae ## V = 9000, p-value = 8e-04 ## alternative hypothesis: true location is greater than 800 Quick introduction at https://psychstatsworkshop.wordpress.com/2014/08/06/lesson-9-hypothesis-testing/ 5.1 p-values p-value: the p-value of a statistical test is the probability, computed assuming that \\(H_0\\) is true, that the test statistic would take a value as extreme or more extreme than that actually observed. http://www.nature.com/news/psychology-journal-bans-p-values-1.17001 https://www.sciencenews.org/blog/context/p-value-ban-small-step-journal-giant-leap-science "],
["preprocessing.html", "Chapter 6 Preprocessing 6.1 Data 6.2 Missing values 6.3 Imputation methods 6.4 Noise 6.5 Outliers", " Chapter 6 Preprocessing Following the data mining process, we describe what is meant by preprocessing, classical supervised models, unsupervised models and evaluation in the context of software engineering with examples This task is probably the hardest and where most of effort is spend in the data mining process. It is quite typical to transform the data, for example, finding inconsistencies, normalising, imputing missing values, tranforming input data, merging variables, etc. Typically, preprocessing consist of the following tasks (subprocesses): Data cleaning (consistency, noise detection, outliers) Data integration Data transformation (normalisation, discretisation) and derivation of new attributes from existing ones (e.g., population density from population and area) Missing data imputation Data reduction (feature selection and instace selection) 6.1 Data Consistent data are semantically correct based on real-world knowledge of the problem, i.e., no constrains are violated and data that can be used for inducing models and analysis. For example, the LoC or effort is constrained to non-negative values. We can also consider that to multiple attributes are consistent among them, and even datasets (e.g., same metrics but collected by different tools) 6.2 Missing values Three types of problems are usually associated with MVs in DM [5]: loss of efficiency complications in handling and analyzing the data bias resulting from differences between missing and complete data. Imputation consists in replacing missing values for estimates of those missing values. Many algorithms do cannot handle missing values and imputation methods are needed. In R, a missing value is represented with NA and the analyst must decide what to do with missing data. The simplest approach is to leave out instances (ignore missing -IM-) with with missing data. This functionality is supported by many base functions through the na.rm option. 6.3 Imputation methods We can use simple approaches such as the replacing the missing values with the mean or mode of the attribute. More elaborated approaches include: EM (Expectation-Maximisation) Distance-based kNN (k Nearest Neighbours) Clustering 6.4 Noise Imperfections of the real-world data that influences negatively in the induced machine learning models. Approaches to deal with noisy data: + Robust learners capable of handling noisy data (e.g., C4.5 through pruning strategies) + Data polishing methods which aim to correct noisy instances prior training + Noise filters which are used to identify and eliminate noisy instances from the training data. Types of noise data: + Class Noise (aka label noise). + There can be contradictory cases (all attributes have the same value except the class) + Misclassifications. The class attribute is not labeled with the true label (golden truth) + Attribute Noise. Values of attributes that are noise, missing or unknown. 6.5 Outliers There is a large amount of literature related to outlier detection, and furthermore several definitions of outlier exist. library(DMwR) library(foreign) kc1 &lt;- read.arff(&quot;./datasets/defectPred/D1/KC1.arff&quot;) The LOF algorithm (lofactor), given a data set it produces a vector of local outlier factors for each case. kc1num &lt;- kc1[,1:21] outlier.scores &lt;- lofactor(kc1num, k=5) plot(density(na.omit(outlier.scores))) outliers &lt;- order(outlier.scores, decreasing=T)[1:5] print(outliers) ## [1] 1 6 14 31 33 Another simple method of Hiridoglou and Berthelot for positive observations. "],
["feature-selection-fs.html", "Chapter 7 Feature selection (FS)", " Chapter 7 Feature selection (FS) Feature Selection (FS) aims at identifying the most relevant attributes from a dataset. It is important in different ways: A reduced volume of data allows different data mining or searching techniques to be applied. Irrelevant and redundant attributes can generate less accurate and more complex models. Furthermore, data mining algorithms can be executed faster. It avoids the collection of data for those irrelevant and redundant attributes in the future. The problem of feature selection received a thorough treatment in pattern recognition and machine learning. Most of the feature selection algorithms tackle the task as a search problem, where each state in the search specifies a distinct subset of the possible attributes~. The search procedure is combined with a criterion to evaluate the merit of each candidate subset of attributes. There are a multiple possible combinations between each procedure search and each attribute measure~. There are two major approaches in FS from the method’s output point of view: Feature subset selection (FSS) Feature ranking in which attributes are ranked as a list of features which are ordered according to evaluation measures (a subset of features is often selected from the top of a ranking list). FFS algorithms designed with different evaluation criteria broadly fall into two categories: The filter model relies on general characteristics of the data to evaluate and select feature subsets without involving any data mining algorithm. The wrapper model requires one predetermined mining algorithm and uses its performance as the evaluation criterion. It searches for features better suited to the mining algorithm aiming to improve mining performance, but it also tends to be more computationally expensive than filter model~. Feature subset algorithms search through candidate feature subsets guide by a certain evaluation measure~ which captures the goodness of each subset. An optimal (or near optimal) subset is selected when the search stops. Some existing evaluation measures that have been shown effective in removing both irrelevant and redundant features include the consistency measure , the correlation measure and the estimated accuracy of a learning algorithm . Consistency measure attempts to find a minimum number of features that separate classes as consistently as the full set of features can. An inconsistency is defined as to instances having the same feature values but different class labels. Correlation measure evaluates the goodness of feature subsets based on the hypothesis that good feature subsets contain features highly correlated to the class, yet uncorrelated to each other. Wrapper-based attribute selection uses the target learning algorithm to estimate the worth of attribute subsets. The feature subset selection algorithm conducts a search for a good subset using the induction algorithm itself as part of the evaluation function. Langley~ notes that feature selection algorithms that search through the space of feature subsets must address four main issues: the starting point of the search, the organization of the search, the evaluation of features subsets and the criterion used to terminate the search. Different algorithms address theses issues differently. It is impractical to look at all possible feature subsets, even if the size is small. Feature selection algorithms usually proceed greedily. They can be classified into those that add features to an initially empty set () and those that remove features from an initially complete set (). Hybrids both add and remove features as the algorithm progresses. Forward selection is much faster than backward elimination and therefore scales better to large data sets. A wide range of search strategies can be used: best-first, branch-and-bound, simulated annealing, genetic algorithms (see Kohavi and John~ for a review). "],
["instance-selection.html", "Chapter 8 Instance selection 8.1 Discretization 8.2 Correlation Coefficient and Covariance for Numeric Data 8.3 Normalization 8.4 Transformations", " Chapter 8 Instance selection Removal of samples (complementary to the removal of attributes) in order to scale down the dataset prior to learning a model so that there is (almost) no performance loss. There are two types of processes: Prototype Selection (PS) [68] when the subset is used with a distance based method (kNN) and Training Set Selection (TSS) [21, 130] in which an actual model is learned. It is also a search problem as with feature selection. 8.1 Discretization This process transforms continuous attributes into discrete ones, by associating categorical values to intervals and thus transforming quantitative data into qualitative data. 8.2 Correlation Coefficient and Covariance for Numeric Data Two random variables \\(x\\) and \\(y\\) are called independent if the probability distribution of one variable is not affected by the presence of another. \\(\\tilde{\\chi}^2=\\frac{1}{d}\\sum_{k=1}^{n} \\frac{(O_k - E_k)^2}{E_k}\\) chisq.test(kc1$LOC_BLANK,kc1$BRANCH_TOTAL) ## ## Chi-squared test for given probabilities ## ## data: kc1$LOC_BLANK ## X-squared = 20000, df = 2000, p-value &lt;2e-16 chisq.test(kc1$DESIGN_COMPLEXITY,kc1$CYCLOMATIC_COMPLEXITY) ## ## Pearson&#39;s Chi-squared test ## ## data: kc1$DESIGN_COMPLEXITY and kc1$CYCLOMATIC_COMPLEXITY ## X-squared = 30000, df = 700, p-value &lt;2e-16 8.3 Normalization 8.3.1 Min-Max Normalization \\(z_i=\\frac{x_i-\\min(x)}{\\max(x)-\\min(x)}\\) library(caret) preObj &lt;- preProcess(kc1[, -22], method=c(&quot;center&quot;, &quot;scale&quot;)) 8.3.2 Z-score normalization TBD 8.4 Transformations 8.4.1 Linear Transformations and Quadratic Trans formations TBD 8.4.2 Box-cox transformation TBD 8.4.3 Nominal to Binary tranformations TBD "],
["preprocessing-in-r.html", "Chapter 9 Preprocessing in R 9.1 The dplyr package 9.2 Other libraries and tricks", " Chapter 9 Preprocessing in R 9.1 The dplyr package The dplyr package created by Hadley Wickham. Some functions are similar to SQL syntax and it key functions in dplyr include: select: select columns from a dataframe filter: select rows from a dataframe summarize: allows us to do summary stats based upon the grouped variable group_by: group by a factor variable arrange: order the dataset joins: as in sql left join Tutorial: https://github.com/justmarkham/dplyr-tutorial Examples library(dplyr) Describe the dataframe: str(kc1) ## &#39;data.frame&#39;: 2096 obs. of 22 variables: ## $ LOC_BLANK : num 0 0 0 0 2 0 0 0 0 2 ... ## $ BRANCH_COUNT : num 1 1 1 1 1 1 1 1 1 1 ... ## $ LOC_CODE_AND_COMMENT : num 0 0 0 0 0 0 0 0 0 0 ... ## $ LOC_COMMENTS : num 0 0 0 0 0 0 0 0 0 0 ... ## $ CYCLOMATIC_COMPLEXITY: num 1 1 1 1 1 1 1 1 1 1 ... ## $ DESIGN_COMPLEXITY : num 1 1 1 1 1 1 1 1 1 1 ... ## $ ESSENTIAL_COMPLEXITY : num 1 1 1 1 1 1 1 1 1 1 ... ## $ LOC_EXECUTABLE : num 3 1 1 1 8 3 1 1 1 9 ... ## $ HALSTEAD_CONTENT : num 11.6 0 0 0 18 ... ## $ HALSTEAD_DIFFICULTY : num 2.67 0 0 0 3.5 2.67 0 0 0 3.75 ... ## $ HALSTEAD_EFFORT : num 82.3 0 0 0 220.9 ... ## $ HALSTEAD_ERROR_EST : num 0.01 0 0 0 0.02 0.01 0 0 0 0.04 ... ## $ HALSTEAD_LENGTH : num 11 1 1 1 19 11 1 1 1 29 ... ## $ HALSTEAD_LEVEL : num 0.38 0 0 0 0.29 0.38 0 0 0 0.27 ... ## $ HALSTEAD_PROG_TIME : num 4.57 0 0 0 12.27 ... ## $ HALSTEAD_VOLUME : num 30.9 0 0 0 63.1 ... ## $ NUM_OPERANDS : num 4 0 0 0 7 4 0 0 0 10 ... ## $ NUM_OPERATORS : num 7 1 1 1 12 7 1 1 1 19 ... ## $ NUM_UNIQUE_OPERANDS : num 3 0 0 0 5 3 0 0 0 8 ... ## $ NUM_UNIQUE_OPERATORS : num 4 1 1 1 5 4 1 1 1 6 ... ## $ LOC_TOTAL : num 5 3 3 3 12 5 3 3 3 13 ... ## $ Defective : Factor w/ 2 levels &quot;N&quot;,&quot;Y&quot;: 1 1 1 1 1 1 1 1 1 1 ... tbl_df creates a “local data frame” as a wrapper for better printing kc1_tbl &lt;- tbl_df(kc1) Filter: # Filter rows: use comma or &amp; to represent AND condition filter(kc1_tbl, Defective == &quot;Y&quot; &amp; LOC_BLANK != 0) ## # A tibble: 251 × 22 ## LOC_BLANK BRANCH_COUNT LOC_CODE_AND_COMMENT LOC_COMMENTS ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 6 21 0 10 ## 2 5 15 0 2 ## 3 2 5 0 0 ## 4 4 5 0 2 ## 5 2 11 0 2 ## 6 2 23 0 3 ## 7 1 11 0 2 ## 8 1 13 0 2 ## 9 2 17 0 2 ## 10 3 1 0 0 ## # ... with 241 more rows, and 18 more variables: ## # CYCLOMATIC_COMPLEXITY &lt;dbl&gt;, DESIGN_COMPLEXITY &lt;dbl&gt;, ## # ESSENTIAL_COMPLEXITY &lt;dbl&gt;, LOC_EXECUTABLE &lt;dbl&gt;, ## # HALSTEAD_CONTENT &lt;dbl&gt;, HALSTEAD_DIFFICULTY &lt;dbl&gt;, ## # HALSTEAD_EFFORT &lt;dbl&gt;, HALSTEAD_ERROR_EST &lt;dbl&gt;, ## # HALSTEAD_LENGTH &lt;dbl&gt;, HALSTEAD_LEVEL &lt;dbl&gt;, HALSTEAD_PROG_TIME &lt;dbl&gt;, ## # HALSTEAD_VOLUME &lt;dbl&gt;, NUM_OPERANDS &lt;dbl&gt;, NUM_OPERATORS &lt;dbl&gt;, ## # NUM_UNIQUE_OPERANDS &lt;dbl&gt;, NUM_UNIQUE_OPERATORS &lt;dbl&gt;, ## # LOC_TOTAL &lt;dbl&gt;, Defective &lt;fctr&gt; Another operator is %in%. Select: select(kc1_tbl, contains(&quot;LOC&quot;), Defective) ## # A tibble: 2,096 × 6 ## LOC_BLANK LOC_CODE_AND_COMMENT LOC_COMMENTS LOC_EXECUTABLE LOC_TOTAL ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 0 0 3 5 ## 2 0 0 0 1 3 ## 3 0 0 0 1 3 ## 4 0 0 0 1 3 ## 5 2 0 0 8 12 ## 6 0 0 0 3 5 ## 7 0 0 0 1 3 ## 8 0 0 0 1 3 ## 9 0 0 0 1 3 ## 10 2 0 0 9 13 ## # ... with 2,086 more rows, and 1 more variables: Defective &lt;fctr&gt; Now, kc1_tbl contains(“LOC”), Defective Filter and Select together: # nesting method filter(select(kc1_tbl, contains(&quot;LOC&quot;), Defective), Defective !=0) ## # A tibble: 2,096 × 6 ## LOC_BLANK LOC_CODE_AND_COMMENT LOC_COMMENTS LOC_EXECUTABLE LOC_TOTAL ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 0 0 3 5 ## 2 0 0 0 1 3 ## 3 0 0 0 1 3 ## 4 0 0 0 1 3 ## 5 2 0 0 8 12 ## 6 0 0 0 3 5 ## 7 0 0 0 1 3 ## 8 0 0 0 1 3 ## 9 0 0 0 1 3 ## 10 2 0 0 9 13 ## # ... with 2,086 more rows, and 1 more variables: Defective &lt;fctr&gt; It is easier usign the chaining method: # chaining method kc1_tbl %&gt;% select(contains(&quot;LOC&quot;), Defective) %&gt;% filter(Defective !=0) ## # A tibble: 2,096 × 6 ## LOC_BLANK LOC_CODE_AND_COMMENT LOC_COMMENTS LOC_EXECUTABLE LOC_TOTAL ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 0 0 3 5 ## 2 0 0 0 1 3 ## 3 0 0 0 1 3 ## 4 0 0 0 1 3 ## 5 2 0 0 8 12 ## 6 0 0 0 3 5 ## 7 0 0 0 1 3 ## 8 0 0 0 1 3 ## 9 0 0 0 1 3 ## 10 2 0 0 9 13 ## # ... with 2,086 more rows, and 1 more variables: Defective &lt;fctr&gt; Arrange ascending # kc1_tbl %&gt;% select(LOC_TOTAL, Defective) %&gt;% arrange(LOC_TOTAL) ## # A tibble: 2,096 × 2 ## LOC_TOTAL Defective ## &lt;dbl&gt; &lt;fctr&gt; ## 1 1 N ## 2 1 N ## 3 1 N ## 4 1 N ## 5 1 N ## 6 1 N ## 7 1 N ## 8 1 N ## 9 1 N ## 10 1 N ## # ... with 2,086 more rows Arrange descending: kc1_tbl %&gt;% select(LOC_TOTAL, Defective) %&gt;% arrange(desc(LOC_TOTAL)) ## # A tibble: 2,096 × 2 ## LOC_TOTAL Defective ## &lt;dbl&gt; &lt;fctr&gt; ## 1 288 Y ## 2 286 Y ## 3 283 N ## 4 220 Y ## 5 217 Y ## 6 210 N ## 7 205 Y ## 8 184 Y ## 9 179 Y ## 10 176 Y ## # ... with 2,086 more rows Mutate: kc1_tbl %&gt;% filter(Defective == &quot;Y&quot;) %&gt;% select(NUM_OPERANDS, NUM_OPERATORS, Defective) %&gt;% mutate(HalsteadLength = NUM_OPERANDS + NUM_OPERATORS) ## # A tibble: 325 × 4 ## NUM_OPERANDS NUM_OPERATORS Defective HalsteadLength ## &lt;dbl&gt; &lt;dbl&gt; &lt;fctr&gt; &lt;dbl&gt; ## 1 64 107 Y 171 ## 2 52 89 Y 141 ## 3 17 41 Y 58 ## 4 41 74 Y 115 ## 5 54 95 Y 149 ## 6 75 156 Y 231 ## 7 54 95 Y 149 ## 8 56 99 Y 155 ## 9 69 124 Y 193 ## 10 44 60 Y 104 ## # ... with 315 more rows summarise: Reduce variables to values # Create a table grouped by Defective, and then summarise each group by taking the mean of loc kc1_tbl %&gt;% group_by(Defective) %&gt;% summarise(avg_loc = mean(LOC_TOTAL, na.rm=TRUE)) ## # A tibble: 2 × 2 ## Defective avg_loc ## &lt;fctr&gt; &lt;dbl&gt; ## 1 N 15.9 ## 2 Y 44.7 # Create a table grouped by Defective, and then summarise each group by taking the mean of loc kc1_tbl %&gt;% group_by(Defective) %&gt;% summarise_each(funs(mean, min, max), BRANCH_COUNT, LOC_TOTAL) ## # A tibble: 2 × 7 ## Defective BRANCH_COUNT_mean LOC_TOTAL_mean BRANCH_COUNT_min ## &lt;fctr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 N 3.68 15.9 1 ## 2 Y 10.12 44.7 1 ## # ... with 3 more variables: LOC_TOTAL_min &lt;dbl&gt;, BRANCH_COUNT_max &lt;dbl&gt;, ## # LOC_TOTAL_max &lt;dbl&gt; It seems than the number of Defective modules is larger than the Non-Defective ones. We can count them with: # n() or tally kc1_tbl %&gt;% group_by(Defective) %&gt;% tally() ## # A tibble: 2 × 2 ## Defective n ## &lt;fctr&gt; &lt;int&gt; ## 1 N 1771 ## 2 Y 325 It seems that it’s an imbalanced dataset… # randomly sample a fixed number of rows, without replacement kc1_tbl %&gt;% sample_n(2) ## # A tibble: 2 × 22 ## LOC_BLANK BRANCH_COUNT LOC_CODE_AND_COMMENT LOC_COMMENTS ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 4 5 0 2 ## 2 0 1 0 0 ## # ... with 18 more variables: CYCLOMATIC_COMPLEXITY &lt;dbl&gt;, ## # DESIGN_COMPLEXITY &lt;dbl&gt;, ESSENTIAL_COMPLEXITY &lt;dbl&gt;, ## # LOC_EXECUTABLE &lt;dbl&gt;, HALSTEAD_CONTENT &lt;dbl&gt;, ## # HALSTEAD_DIFFICULTY &lt;dbl&gt;, HALSTEAD_EFFORT &lt;dbl&gt;, ## # HALSTEAD_ERROR_EST &lt;dbl&gt;, HALSTEAD_LENGTH &lt;dbl&gt;, HALSTEAD_LEVEL &lt;dbl&gt;, ## # HALSTEAD_PROG_TIME &lt;dbl&gt;, HALSTEAD_VOLUME &lt;dbl&gt;, NUM_OPERANDS &lt;dbl&gt;, ## # NUM_OPERATORS &lt;dbl&gt;, NUM_UNIQUE_OPERANDS &lt;dbl&gt;, ## # NUM_UNIQUE_OPERATORS &lt;dbl&gt;, LOC_TOTAL &lt;dbl&gt;, Defective &lt;fctr&gt; # randomly sample a fraction of rows, with replacement kc1_tbl %&gt;% sample_frac(0.05, replace=TRUE) ## # A tibble: 105 × 22 ## LOC_BLANK BRANCH_COUNT LOC_CODE_AND_COMMENT LOC_COMMENTS ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 1 0 0 ## 2 0 1 0 0 ## 3 0 6 0 0 ## 4 0 1 0 0 ## 5 0 1 0 0 ## 6 5 17 0 7 ## 7 23 25 0 22 ## 8 5 13 1 4 ## 9 0 1 0 0 ## 10 0 1 0 0 ## # ... with 95 more rows, and 18 more variables: ## # CYCLOMATIC_COMPLEXITY &lt;dbl&gt;, DESIGN_COMPLEXITY &lt;dbl&gt;, ## # ESSENTIAL_COMPLEXITY &lt;dbl&gt;, LOC_EXECUTABLE &lt;dbl&gt;, ## # HALSTEAD_CONTENT &lt;dbl&gt;, HALSTEAD_DIFFICULTY &lt;dbl&gt;, ## # HALSTEAD_EFFORT &lt;dbl&gt;, HALSTEAD_ERROR_EST &lt;dbl&gt;, ## # HALSTEAD_LENGTH &lt;dbl&gt;, HALSTEAD_LEVEL &lt;dbl&gt;, HALSTEAD_PROG_TIME &lt;dbl&gt;, ## # HALSTEAD_VOLUME &lt;dbl&gt;, NUM_OPERANDS &lt;dbl&gt;, NUM_OPERATORS &lt;dbl&gt;, ## # NUM_UNIQUE_OPERANDS &lt;dbl&gt;, NUM_UNIQUE_OPERATORS &lt;dbl&gt;, ## # LOC_TOTAL &lt;dbl&gt;, Defective &lt;fctr&gt; # Better formatting adapted to the screen width glimpse(kc1_tbl) ## Observations: 2,096 ## Variables: 22 ## $ LOC_BLANK &lt;dbl&gt; 0, 0, 0, 0, 2, 0, 0, 0, 0, 2, 2, 0, 2, 1... ## $ BRANCH_COUNT &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1... ## $ LOC_CODE_AND_COMMENT &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... ## $ LOC_COMMENTS &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0... ## $ CYCLOMATIC_COMPLEXITY &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1... ## $ DESIGN_COMPLEXITY &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1... ## $ ESSENTIAL_COMPLEXITY &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1... ## $ LOC_EXECUTABLE &lt;dbl&gt; 3, 1, 1, 1, 8, 3, 1, 1, 1, 9, 8, 1, 8, 1... ## $ HALSTEAD_CONTENT &lt;dbl&gt; 11.6, 0.0, 0.0, 0.0, 18.0, 11.6, 0.0, 0.... ## $ HALSTEAD_DIFFICULTY &lt;dbl&gt; 2.67, 0.00, 0.00, 0.00, 3.50, 2.67, 0.00... ## $ HALSTEAD_EFFORT &lt;dbl&gt; 82.3, 0.0, 0.0, 0.0, 220.9, 82.3, 0.0, 0... ## $ HALSTEAD_ERROR_EST &lt;dbl&gt; 0.01, 0.00, 0.00, 0.00, 0.02, 0.01, 0.00... ## $ HALSTEAD_LENGTH &lt;dbl&gt; 11, 1, 1, 1, 19, 11, 1, 1, 1, 29, 19, 1,... ## $ HALSTEAD_LEVEL &lt;dbl&gt; 0.38, 0.00, 0.00, 0.00, 0.29, 0.38, 0.00... ## $ HALSTEAD_PROG_TIME &lt;dbl&gt; 4.57, 0.00, 0.00, 0.00, 12.27, 4.57, 0.0... ## $ HALSTEAD_VOLUME &lt;dbl&gt; 30.9, 0.0, 0.0, 0.0, 63.1, 30.9, 0.0, 0.... ## $ NUM_OPERANDS &lt;dbl&gt; 4, 0, 0, 0, 7, 4, 0, 0, 0, 10, 7, 0, 7, ... ## $ NUM_OPERATORS &lt;dbl&gt; 7, 1, 1, 1, 12, 7, 1, 1, 1, 19, 12, 1, 1... ## $ NUM_UNIQUE_OPERANDS &lt;dbl&gt; 3, 0, 0, 0, 5, 3, 0, 0, 0, 8, 5, 0, 5, 0... ## $ NUM_UNIQUE_OPERATORS &lt;dbl&gt; 4, 1, 1, 1, 5, 4, 1, 1, 1, 6, 5, 1, 5, 1... ## $ LOC_TOTAL &lt;dbl&gt; 5, 3, 3, 3, 12, 5, 3, 3, 3, 13, 12, 3, 1... ## $ Defective &lt;fctr&gt; N, N, N, N, N, N, N, N, N, N, N, N, N, ... 9.2 Other libraries and tricks The lubridate package contains a number of functions facilitating the conversion of text to POSIX dates. As an example, consider the following code. We may use this, for example, with time series. For example https://cran.r-project.org/doc/contrib/de_Jonge+van_der_Loo-Introduction_to_data_cleaning_with_R.pdf library(lubridate) dates &lt;- c(&quot;15/02/2013&quot;, &quot;15 Feb 13&quot;, &quot;It happened on 15 02 &#39;13&quot;) dmy(dates) ## [1] &quot;2013-02-15&quot; &quot;2013-02-15&quot; &quot;2013-02-15&quot; "],
["supervised-models.html", "Chapter 10 Supervised Models", " Chapter 10 Supervised Models A classification problem can be defined as the induction, from a dataset \\(\\cal D\\), of a classification function \\(\\psi\\) that, given the attribute vector of an instance/example, returns a class \\({c}\\). A regression problem, on the other hand, returns an numeric value. Dataset, \\(\\cal D\\), is typically composed of \\(n\\) attributes and a class attribute \\(C\\). \\(Att_1\\) … \\(Att_n\\) \\(Class\\) \\(a_{11}\\) … \\(a_{1n}\\) \\(c_1\\) \\(a_{21}\\) … \\(a_{2n}\\) \\(c_2\\) … … … … \\(a_{m1}\\) … \\(a_{mn}\\) \\(c_m\\) Columns are usually called attributes or features. Typically, there is a class attribute, which can be numeric or discrete. When the class is numeric, it is a regression problem. With discrete values, we can talk about binary classification or multiclass (multinomial classification) when we have more than three values. There are variants such multi-label classification (we will cover these in the advanced models section). The next chapters in this part will cover the main models for supervised learning. "],
["regression.html", "Chapter 11 Regression 11.1 Linear Regression modeling 11.2 Linear Regression Diagnostics 11.3 Multiple Linear Regression 11.4 Linear regression in Software Effort estimation 11.5 References", " Chapter 11 Regression 11.1 Linear Regression modeling Linear Regression is one of the oldest and most known predictive methods. As its name says, the idea is to try to fit a linear equation between a dependent variable and an independent, or explanatory, variable. The idea is that the independent variable \\(x\\) is something the experimenter controls and the dependent variable \\(y\\) is something that the experimenter measures. The line is used to predict the value of \\(y\\) for a known value of \\(x\\). The variable \\(x\\) is the predictor variable and \\(y\\) the response variable. Multiple linear regression uses 2 or more independent variables for building a model. See https://www.wikipedia.org/wiki/Linear_regression. First proposed many years ago but still very useful… Galton Data The equation takes the form \\(\\hat{y}=b_0+b_1 * x\\) The method used to choose the values \\(b_0\\) and \\(b_1\\) is to minimize the sum of the squares of the residual errors. 11.1.1 Regression: Galton Data Not related to Software Engineering but … library(UsingR) data(galton) par(mfrow=c(1,2)) hist(galton$child,col=&quot;blue&quot;,breaks=100) hist(galton$parent,col=&quot;blue&quot;,breaks=100) plot(galton$parent,galton$child,pch=1,col=&quot;blue&quot;, cex=0.4) lm1 &lt;- lm(galton$child ~ galton$parent) lines(galton$parent,lm1$fitted,col=&quot;red&quot;,lwd=3) plot(galton$parent,lm1$residuals,col=&quot;blue&quot;,pch=1, cex=0.4) abline(c(0,0),col=&quot;red&quot;,lwd=3) qqnorm(galton$child) 11.1.2 Simple Linear Regression Given two variables \\(Y\\) (response) and \\(X\\) (predictor), the assumption is that there is an approximate (\\(\\approx\\)) linear relation between those variables. The mathematical model of the observed data is described as (for the case of simple linear regression): \\[ Y \\approx \\beta_0 + \\beta_1 X\\] the parameter \\(\\beta_0\\) is named the intercept and \\(\\beta_1\\) is the slope Each observation can be modeled as \\[y_i = \\beta_0 + \\beta_1 x_i + \\epsilon_i; \\epsilon_i \\sim N(0,\\sigma^2)\\] - \\(\\epsilon_i\\) is the error - This means that the variable \\(y\\) is normally distributed: \\[ y_i \\sim N( \\beta_0 + \\beta_1 x_i, \\sigma^2) \\] The predictions or estimations of this model are obtained by a linear equation of the form \\(\\hat{Y}=\\hat{\\beta_0} + \\hat{\\beta}_1X\\), that is, each new prediction is computed with \\[\\hat{y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1x_i \\]. The actual parameters \\(\\beta_0\\) and \\(\\beta_1\\) are unknown The parameters \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) of the linear equation can be estimated with different methods. 11.1.3 Least Squares One of the most used methods for computing \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) is the criterion of “least squares” minimization. The data is composed of \\(n\\) pairs of observations \\((x_i, y_i)\\) Given an observation \\(y_i\\) and its corresponding estimation \\(\\hat{y_i})\\) the residual \\(e_i\\) is defined as \\[e_i= y_i - \\hat{y_i}\\] the Residual Sum of Squares is defined as \\[RSS=e_1^2+\\dots + e_i^2+\\dots+e_n^2\\] the Least Squares Approach minimizes the RSS as result of that minimizitation, it can be obtained, by means of calculus, the estimation of \\(\\hat{\\beta}_0\\) and \\(\\hat{\\beta}_1\\) as \\[\\hat{\\beta}_1=\\frac{\\sum_{i=1}^{n}{(x_i-\\bar{x})(y_i-\\bar{y})}}{\\sum_{i=1}^{n}(x_i-\\bar{x})^2}\\] and \\[\\hat{\\beta}_0=\\bar{y}-\\hat{\\beta}_1\\bar{x} \\] where \\(\\bar{y}\\) and \\(\\bar{x}\\) are the sample means. the variance \\(\\sigma^2\\) is estimated by \\[\\hat\\sigma^2 = {RSS}/{(n-2)}\\] where n is the number of observations The Residual Standard Error is defined as \\[RSE = \\sqrt{{RSS}/{(n-2)}}\\] The equation \\[ Y = \\beta_0 + \\beta_1 X + \\epsilon\\] defines the linear model, i.e., the population regression line The least squares line is \\(\\hat{Y}=\\hat{\\beta_0} + \\hat{\\beta}_1X\\) Confidence intervals are computed using the standard errors of the intercept and the slope. The \\(95\\%\\) confidence interval for the slope is computed as \\[[\\hat{\\beta}_1 - 2 \\cdot SE(\\hat{\\beta}_1), \\hat{\\beta}_1+SE(\\hat{\\beta}_1)]\\] where \\[ SE(\\hat{\\beta}_1) = \\sqrt{\\frac{\\sigma^2}{\\sum_{i=1}^{n}(x_i-\\bar{x})^2}}\\] 11.1.4 Linear regression in R The following are the basic commands in R: The basic function is lm(), that returns an object with the model. Other commands: summary prints out information about the regression, coef gives the coefficients for the linear model, fitted gives the predictd value of \\(y\\) for each value of \\(x\\), residuals contains the differences between observed and fitted values. predict will generate predicted values of the response for the values of the explanatory variable. 11.2 Linear Regression Diagnostics Several plots help to evaluate the suitability of the linear regression Residuals vs fitted: The residuals should be randomly distributed around the horizontal line representing a residual error of zero; that is, there should not be a distinct trend in the distribution of points. Standard Q-Q plot: residual errors are normally distributed Square root of the standardized residuals vs the fitted values: there should be no obvious trend. This plot is similar to the residuals versus fitted values plot, but it uses the square root of the standardized residuals. Leverage: measures the importance of each point in determining the regression result. Smaller values means that removing the observation has little effect on the regression result. 11.2.1 Simulation example 11.2.1.1 Simulate a dataset set.seed(3456) # equation is y = -6.6 + 0.13 x +e # range x 100,400 a &lt;- -6.6 b &lt;- 0.13 num_obs &lt;- 60 xmin &lt;- 100 xmax &lt;- 400 x &lt;- sample(seq(from=xmin, to = xmax, by =1), size= num_obs, replace=FALSE) sderror &lt;- 9 # sigma for the error term in the model e &lt;- rnorm(num_obs, 0, sderror) y &lt;- a + b * x + e newlm &lt;- lm(y~x) summary(newlm) ## ## Call: ## lm(formula = y ~ x) ## ## Residuals: ## Min 1Q Median 3Q Max ## -15.937 -4.617 -0.923 3.797 21.442 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -13.4765 3.0320 -4.44 4e-05 *** ## x 0.1550 0.0113 13.75 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 7.99 on 58 degrees of freedom ## Multiple R-squared: 0.765, Adjusted R-squared: 0.761 ## F-statistic: 189 on 1 and 58 DF, p-value: &lt;2e-16 cfa1 &lt;- coef(newlm)[1] cfb2 &lt;- coef(newlm)[2] plot(x,y, xlab=&quot;x axis&quot;, ylab= &quot;y axis&quot;, xlim = c(xmin, xmax), ylim = c(0,60), sub = &quot;Line in black is the actual model&quot;) title(main = paste(&quot;Line in blue is the Regression Line for &quot;, num_obs, &quot; points.&quot;)) abline(a = cfa1, b = cfb2, col= &quot;blue&quot;, lwd=3) abline(a = a, b = b, col= &quot;black&quot;, lwd=1) #original line 11.2.1.1.1 Subset a set of points from the same sample # sample from the same x to compare least squares lines # change the denominator in newsample to see how the least square lines changes accordingly. newsample &lt;- as.integer(num_obs/8) # number of pairs x,y idxs_x1 &lt;- sample(1:num_obs, size = newsample, replace = FALSE) #sample indexes x1 &lt;- x[idxs_x1] e1 &lt;- e[idxs_x1] y1 &lt;- a + b * x1 + e1 xy_obs &lt;- data.frame(x1, y1) names(xy_obs) &lt;- c(&quot;x_obs&quot;, &quot;y_obs&quot;) newlm1 &lt;- lm(y1~x1) summary(newlm1) ## ## Call: ## lm(formula = y1 ~ x1) ## ## Residuals: ## 1 2 3 4 5 6 7 ## 3.722 -5.067 4.683 -4.716 3.095 -0.813 -0.904 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -14.5356 7.0962 -2.05 0.0958 . ## x1 0.1494 0.0272 5.48 0.0027 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4.35 on 5 degrees of freedom ## Multiple R-squared: 0.857, Adjusted R-squared: 0.829 ## F-statistic: 30.1 on 1 and 5 DF, p-value: 0.00275 cfa21 &lt;- coef(newlm1)[1] cfb22 &lt;- coef(newlm1)[2] plot(x1,y1, xlab=&quot;x axis&quot;, ylab= &quot;y axis&quot;, xlim = c(xmin, xmax), ylim = c(0,60)) title(main = paste(&quot;New line in red with &quot;, newsample, &quot; points in sample&quot;)) abline(a = a, b = b, col= &quot;black&quot;, lwd=1) # True line abline(a = cfa1, b = cfb2, col= &quot;blue&quot;, lwd=1) #sample abline(a = cfa21, b = cfb22, col= &quot;red&quot;, lwd=2) #new line 11.2.1.1.2 Compute a confidence interval on the original sample regression line newx &lt;- seq(xmin, xmax) ypredicted &lt;- predict(newlm, newdata=data.frame(x=newx), interval= &quot;confidence&quot;, level= 0.90, se = TRUE) plot(x,y, xlab=&quot;x axis&quot;, ylab= &quot;y axis&quot;, xlim = c(xmin, xmax), ylim = c(0,60)) # points(x1, fitted(newlm1)) abline(newlm) lines(newx,ypredicted$fit[,2],col=&quot;red&quot;,lty=2) lines(newx,ypredicted$fit[,3],col=&quot;red&quot;,lty=2) # Plot the residuals or errors ypredicted_x &lt;- predict(newlm, newdata=data.frame(x=x)) plot(x,y, xlab=&quot;x axis&quot;, ylab= &quot;y axis&quot;, xlim = c(xmin, xmax), ylim = c(0,60), sub = &quot;&quot;, pch=19, cex=0.75) title(main = paste(&quot;Residuals or errors&quot;, num_obs, &quot; points.&quot;)) abline(newlm) segments(x, y, x, ypredicted_x) 11.2.1.1.3 Take another sample from the model and explore # equation is y = -6.6 + 0.13 x +e # range x 100,400 num_obs &lt;- 35 xmin &lt;- 100 xmax &lt;- 400 x3 &lt;- sample(seq(from=xmin, to = xmax, by =1), size= num_obs, replace=FALSE) sderror &lt;- 14 # sigma for the error term in the model e3 &lt;- rnorm(num_obs, 0, sderror) y3 &lt;- a + b * x3 + e3 newlm3 &lt;- lm(y3~x3) summary(newlm3) ## ## Call: ## lm(formula = y3 ~ x3) ## ## Residuals: ## Min 1Q Median 3Q Max ## -25.59 -11.19 2.92 8.65 39.16 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -17.5335 7.6813 -2.28 0.029 * ## x3 0.1657 0.0285 5.80 1.7e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 15 on 33 degrees of freedom ## Multiple R-squared: 0.505, Adjusted R-squared: 0.49 ## F-statistic: 33.7 on 1 and 33 DF, p-value: 1.72e-06 cfa31 &lt;- coef(newlm3)[1] cfb32 &lt;- coef(newlm3)[2] plot(x3,y3, xlab=&quot;x axis&quot;, ylab= &quot;y axis&quot;, xlim = c(xmin, xmax), ylim = c(0,60)) title(main = paste(&quot;Line in red is the Regression Line for &quot;, num_obs, &quot; points.&quot;)) abline(a = cfa31, b = cfb32, col= &quot;red&quot;, lwd=3) abline(a = a, b = b, col= &quot;black&quot;, lwd=2) #original line abline(a = cfa1, b = cfb2, col= &quot;blue&quot;, lwd=1) #first sample # confidence intervals for the new sample newx &lt;- seq(xmin, xmax) ypredicted &lt;- predict(newlm3, newdata=data.frame(x3=newx), interval= &quot;confidence&quot;, level= 0.90, se = TRUE) lines(newx,ypredicted$fit[,2],col=&quot;red&quot;,lty=2, lwd=2) lines(newx,ypredicted$fit[,3],col=&quot;red&quot;,lty=2, lwd=2) 11.2.2 Diagnostics fro assessing the regression line 11.2.2.1 Residual Standard Error It gives us an idea of the typical or average error of the model. It is the estimated standard deviation of the residuals. 11.2.2.2 \\(R^2\\) statistic This is the proportion of variability in the data that is explained by the model. Best values are those close to 1. 11.3 Multiple Linear Regression 11.3.1 Partial Least Squares If several predictors are highly correlated, the least squares approach has high variability. PLS finds linear combinations of the predictors, that are called components or latent variables. 11.4 Linear regression in Software Effort estimation Fitting a linear model to log-log - the predictive power equation is \\(y= e^{b_0 + b_1 log(x)}\\), ignoring the bias corrections - First, we are fitting the model to the whole dataset. But it is not the right way to do it, because of overfitting. library(foreign) china &lt;- read.arff(&quot;./datasets/effortEstimation/china.arff&quot;) china_size &lt;- china$AFP summary(china_size) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 9 100 215 487 438 17500 china_effort &lt;- china$Effort summary(china_effort) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 26 704 1830 3920 3830 54600 par(mfrow=c(1,2)) hist(china_size, col=&quot;blue&quot;, xlab=&quot;Adjusted Function Points&quot;, main=&quot;Distribution of AFP&quot;) hist(china_effort, col=&quot;blue&quot;,xlab=&quot;Effort&quot;, main=&quot;Distribution of Effort&quot;) boxplot(china_size) boxplot(china_effort) qqnorm(china_size) qqline(china_size) qqnorm(china_effort) qqline(china_effort) Applying the log function linmodel_logchina &lt;- lm(logchina_effort ~ logchina_size) par(mfrow=c(1,1)) plot(logchina_size, logchina_effort) abline(linmodel_logchina, lwd=3, col=3) par(mfrow=c(1,2)) plot(linmodel_logchina, ask = FALSE) linmodel_logchina ## ## Call: ## lm(formula = logchina_effort ~ logchina_size) ## ## Coefficients: ## (Intercept) logchina_size ## 3.301 0.768 11.5 References The New Statistics with R, Andy Hector, 2015 An Introduction to R, W.N. Venables and D.M. Smith and the R Development Core Team Practical Data Science with R, Nina Zumel and John Mount G. James et al, An Introduction to Statistical Learning with Applications in R, Springer, 2013 "],
["discrete-classification.html", "Chapter 12 Discrete Classification 12.1 The caret package 12.2 Linear Discriminant Analysis (LDA) 12.3 Binary Logistic Regression (BLR) 12.4 Classification Trees 12.5 Rules 12.6 Distanced-based Methods 12.7 Probabilistic Methods", " Chapter 12 Discrete Classification We also have several types of models such as trees, rules, and probabilistic classifiers that can be used to classify instances in There are many p We will show the use of different classification techniques in the problem of defect prediction as running example. In this example, the different datasets are composed of classical metrics (Halstead or McCabe metrics) based on counts of operators/operands and like or object-oriented metrics (e.g. Chidamber and Kemerer) and the class attribute indicating whether the module or class was defective. 12.1 The caret package There are hundreds of packages to perform classification task in R, but many of those can be used throught the ‘caret’ package which helps with many of the data mining process task as described next. The caret (Classification And REgression Training) package provides a unified interface for modeling and prediction with around 150 different models with tools for: + data splitting + pre-processing + feature selection + model tuning using resampling + variable importance estimation, etc. Website: http://caret.r-forge.r-project.org JSS Paper: www.jstatsoft.org/v28/i05/paper Book: Applied Predictive Modeling For example, using one of the NASA datasets used extensively in defect prediction: library(caret) library(foreign) kc1 &lt;- read.arff(&quot;./datasets/defectPred/D1/KC1.arff&quot;) str(kc1) ## &#39;data.frame&#39;: 2096 obs. of 22 variables: ## $ LOC_BLANK : num 0 0 0 0 2 0 0 0 0 2 ... ## $ BRANCH_COUNT : num 1 1 1 1 1 1 1 1 1 1 ... ## $ LOC_CODE_AND_COMMENT : num 0 0 0 0 0 0 0 0 0 0 ... ## $ LOC_COMMENTS : num 0 0 0 0 0 0 0 0 0 0 ... ## $ CYCLOMATIC_COMPLEXITY: num 1 1 1 1 1 1 1 1 1 1 ... ## $ DESIGN_COMPLEXITY : num 1 1 1 1 1 1 1 1 1 1 ... ## $ ESSENTIAL_COMPLEXITY : num 1 1 1 1 1 1 1 1 1 1 ... ## $ LOC_EXECUTABLE : num 3 1 1 1 8 3 1 1 1 9 ... ## $ HALSTEAD_CONTENT : num 11.6 0 0 0 18 ... ## $ HALSTEAD_DIFFICULTY : num 2.67 0 0 0 3.5 2.67 0 0 0 3.75 ... ## $ HALSTEAD_EFFORT : num 82.3 0 0 0 220.9 ... ## $ HALSTEAD_ERROR_EST : num 0.01 0 0 0 0.02 0.01 0 0 0 0.04 ... ## $ HALSTEAD_LENGTH : num 11 1 1 1 19 11 1 1 1 29 ... ## $ HALSTEAD_LEVEL : num 0.38 0 0 0 0.29 0.38 0 0 0 0.27 ... ## $ HALSTEAD_PROG_TIME : num 4.57 0 0 0 12.27 ... ## $ HALSTEAD_VOLUME : num 30.9 0 0 0 63.1 ... ## $ NUM_OPERANDS : num 4 0 0 0 7 4 0 0 0 10 ... ## $ NUM_OPERATORS : num 7 1 1 1 12 7 1 1 1 19 ... ## $ NUM_UNIQUE_OPERANDS : num 3 0 0 0 5 3 0 0 0 8 ... ## $ NUM_UNIQUE_OPERATORS : num 4 1 1 1 5 4 1 1 1 6 ... ## $ LOC_TOTAL : num 5 3 3 3 12 5 3 3 3 13 ... ## $ Defective : Factor w/ 2 levels &quot;N&quot;,&quot;Y&quot;: 1 1 1 1 1 1 1 1 1 1 ... Then we need to divide the data into training and testing. # Split data into training and test datasets set.seed(1) inTrain &lt;- createDataPartition(y=kc1$Defective,p=.75,list=FALSE) kc1.train &lt;- kc1[inTrain,] kc1.test &lt;- kc1[-inTrain,] Another approach to dividing the data: # Split data into training and test datasets set.seed(1) ind &lt;- sample(2, nrow(kc1), replace = TRUE, prob = c(0.75, 0.25)) kc1.train &lt;- kc1[ind==1, ] kc1.test &lt;- kc1[ind==2, ] Next we will use different types of models to predict defective modules. 12.2 Linear Discriminant Analysis (LDA) One classical approach to classification is Linear Discriminant Analysis (LDA). ldaModel &lt;- train (Defective ~ ., data=kc1.train, method=&quot;lda&quot;, preProc=c(&quot;center&quot;,&quot;scale&quot;)) ldaModel ## Linear Discriminant Analysis ## ## 1573 samples ## 21 predictors ## 2 classes: &#39;N&#39;, &#39;Y&#39; ## ## Pre-processing: centered (21), scaled (21) ## Resampling: Bootstrapped (25 reps) ## Summary of sample sizes: 1573, 1573, 1573, 1573, 1573, 1573, ... ## Resampling results: ## ## Accuracy Kappa ## 0.855 0.286 ## ## We can observe that we are training our model using Defective ~ . as a formula were ’Defective is the class variable separed by ~ and the ´.´ means the rest of the variables. Also, we are using a filter for the training data to (preProc) to center and scale. Also, as stated in the documentation about the train method : &gt; http://topepo.github.io/caret/training.html ctrl &lt;- trainControl(method = &quot;repeatedcv&quot;,repeats=3) ldaModel &lt;- train (Defective ~ ., data=kc1.train, method=&quot;lda&quot;, trControl=ctrl, preProc=c(&quot;center&quot;,&quot;scale&quot;)) ldaModel ## Linear Discriminant Analysis ## ## 1573 samples ## 21 predictors ## 2 classes: &#39;N&#39;, &#39;Y&#39; ## ## Pre-processing: centered (21), scaled (21) ## Resampling: Cross-Validated (10 fold, repeated 3 times) ## Summary of sample sizes: 1416, 1416, 1415, 1416, 1415, 1416, ... ## Resampling results: ## ## Accuracy Kappa ## 0.854 0.288 ## ## Instead of accuracy we can activate other metrics using summaryFunction=twoClassSummary such as ROC, sensitivity and specificity. To do so, we also need to speficy classProbs=TRUE. ctrl &lt;- trainControl(method = &quot;repeatedcv&quot;,repeats=3, classProbs=TRUE, summaryFunction=twoClassSummary) ldaModel3xcv10 &lt;- train (Defective ~ ., data=kc1.train, method=&quot;lda&quot;, trControl=ctrl, preProc=c(&quot;center&quot;,&quot;scale&quot;)) ldaModel3xcv10 ## Linear Discriminant Analysis ## ## 1573 samples ## 21 predictors ## 2 classes: &#39;N&#39;, &#39;Y&#39; ## ## Pre-processing: centered (21), scaled (21) ## Resampling: Cross-Validated (10 fold, repeated 3 times) ## Summary of sample sizes: 1416, 1416, 1416, 1416, 1416, 1415, ... ## Resampling results: ## ## ROC Sens Spec ## 0.789 0.962 0.26 ## ## Most methods have parameters that need to be optimised and that is one of the plsFit3x10cv &lt;- train (Defective ~ ., data=kc1.train, method=&quot;pls&quot;, trControl=trainControl(classProbs=TRUE), metric=&quot;ROC&quot;, preProc=c(&quot;center&quot;,&quot;scale&quot;)) plsFit3x10cv ## Partial Least Squares ## ## 1573 samples ## 21 predictors ## 2 classes: &#39;N&#39;, &#39;Y&#39; ## ## Pre-processing: centered (21), scaled (21) ## Resampling: Bootstrapped (25 reps) ## Summary of sample sizes: 1573, 1573, 1573, 1573, 1573, 1573, ... ## Resampling results across tuning parameters: ## ## ncomp Accuracy Kappa ## 1 0.841 0.112 ## 2 0.851 0.166 ## 3 0.852 0.191 ## ## Accuracy was used to select the optimal model using the largest value. ## The final value used for the model was ncomp = 3. plot(plsFit3x10cv) The parameter tuneLength allow us to specify the number values per parameter to consider. plsFit3x10cv &lt;- train (Defective ~ ., data=kc1.train, method=&quot;pls&quot;, trControl=ctrl, metric=&quot;ROC&quot;, tuneLength=5, preProc=c(&quot;center&quot;,&quot;scale&quot;)) plsFit3x10cv ## Partial Least Squares ## ## 1573 samples ## 21 predictors ## 2 classes: &#39;N&#39;, &#39;Y&#39; ## ## Pre-processing: centered (21), scaled (21) ## Resampling: Cross-Validated (10 fold, repeated 3 times) ## Summary of sample sizes: 1415, 1416, 1417, 1415, 1416, 1416, ... ## Resampling results across tuning parameters: ## ## ncomp ROC Sens Spec ## 1 0.788 0.981 0.0929 ## 2 0.793 0.984 0.1311 ## 3 0.790 0.982 0.1517 ## 4 0.790 0.986 0.1626 ## 5 0.789 0.985 0.1596 ## ## ROC was used to select the optimal model using the largest value. ## The final value used for the model was ncomp = 2. plot(plsFit3x10cv) Finally to predict new cases, caret will use the best classfier obtained for prediction. plsProbs &lt;- predict(plsFit3x10cv, newdata = kc1.test, type = &quot;prob&quot;) plsClasses &lt;- predict(plsFit3x10cv, newdata = kc1.test, type = &quot;raw&quot;) confusionMatrix(data=plsClasses,kc1.test$Defective) ## Confusion Matrix and Statistics ## ## Reference ## Prediction N Y ## N 439 69 ## Y 3 12 ## ## Accuracy : 0.862 ## 95% CI : (0.83, 0.891) ## No Information Rate : 0.845 ## P-Value [Acc &gt; NIR] : 0.152 ## ## Kappa : 0.212 ## Mcnemar&#39;s Test P-Value : 1.85e-14 ## ## Sensitivity : 0.993 ## Specificity : 0.148 ## Pos Pred Value : 0.864 ## Neg Pred Value : 0.800 ## Prevalence : 0.845 ## Detection Rate : 0.839 ## Detection Prevalence : 0.971 ## Balanced Accuracy : 0.571 ## ## &#39;Positive&#39; Class : N ## 12.2.1 Predicting the number of defects (numerical class) From the Bug Predictiono Repository http://bug.inf.usi.ch/download.php Some datasets contain CK and other 11 object oriented metrics for the last version of the system plus categorized (with severity and priority) post-release defects. Using such dataset: jdt &lt;- read.csv(&quot;./datasets/defectPred/BPD/single-version-ck-oo-EclipseJDTCore.csv&quot;, sep=&quot;;&quot;) # We just use the number of bugs, so we removed others jdt$classname &lt;- NULL jdt$nonTrivialBugs &lt;- NULL jdt$majorBugs &lt;- NULL jdt$minorBugs &lt;- NULL jdt$criticalBugs &lt;- NULL jdt$highPriorityBugs &lt;- NULL jdt$X &lt;- NULL # Caret library(caret) # Split data into training and test datasets set.seed(1) inTrain &lt;- createDataPartition(y=jdt$bugs,p=.8,list=FALSE) jdt.train &lt;- jdt[inTrain,] jdt.test &lt;- jdt[-inTrain,] ctrl &lt;- trainControl(method = &quot;repeatedcv&quot;,repeats=3) glmModel &lt;- train (bugs ~ ., data=jdt.train, method=&quot;glm&quot;, trControl=ctrl, preProc=c(&quot;center&quot;,&quot;scale&quot;)) glmModel ## Generalized Linear Model ## ## 798 samples ## 17 predictors ## ## Pre-processing: centered (17), scaled (17) ## Resampling: Cross-Validated (10 fold, repeated 3 times) ## Summary of sample sizes: 718, 718, 718, 718, 719, 718, ... ## Resampling results: ## ## RMSE Rsquared ## 0.841 0.386 ## ## Others such as Elasticnet: glmnetModel &lt;- train (bugs ~ ., data=jdt.train, method=&quot;glmnet&quot;, trControl=ctrl, preProc=c(&quot;center&quot;,&quot;scale&quot;)) ## Loading required package: glmnet ## Loading required package: Matrix ## Loading required package: foreach ## Loaded glmnet 2.0-5 glmnetModel ## glmnet ## ## 798 samples ## 17 predictors ## ## Pre-processing: centered (17), scaled (17) ## Resampling: Cross-Validated (10 fold, repeated 3 times) ## Summary of sample sizes: 718, 718, 718, 718, 718, 718, ... ## Resampling results across tuning parameters: ## ## alpha lambda RMSE Rsquared ## 0.10 0.0012 0.813 0.341 ## 0.10 0.0120 0.818 0.334 ## 0.10 0.1202 0.808 0.340 ## 0.55 0.0012 0.812 0.341 ## 0.55 0.0120 0.823 0.327 ## 0.55 0.1202 0.812 0.347 ## 1.00 0.0012 0.812 0.341 ## 1.00 0.0120 0.819 0.331 ## 1.00 0.1202 0.817 0.345 ## ## RMSE was used to select the optimal model using the smallest value. ## The final values used for the model were alpha = 0.1 and lambda = 0.12. 12.3 Binary Logistic Regression (BLR) Binary Logistic Regression (BLR) can models fault-proneness as follows \\[fp(X) = \\frac{e^{logit()}}{1 + e^{logit(X)}}\\] where the simplest form for logit is: \\(logit(X) = c_{0} + c_{1}X\\) jdt &lt;- read.csv(&quot;./datasets/defectPred/BPD/single-version-ck-oo-EclipseJDTCore.csv&quot;, sep=&quot;;&quot;) # Caret library(caret) # Convert the response variable into a boolean variable (0/1) jdt$bugs[jdt$bugs&gt;=1]&lt;-1 cbo &lt;- jdt$cbo bugs &lt;- jdt$bugs # Split data into training and test datasets jdt2 = data.frame(cbo, bugs) inTrain &lt;- createDataPartition(y=jdt2$bugs,p=.8,list=FALSE) jdtTrain &lt;- jdt2[inTrain,] jdtTest &lt;- jdt2[-inTrain,] BLR models fault-proneness are as follows \\[fp(X) = \\frac{e^{logit()}}{1 + e^{logit(X)}}\\] where the simplest form for logit is: \\(logit(X) = c_{0} + c_{1}X\\) # logit regression # glmLogit &lt;- train (bugs ~ ., data=jdt.train, method=&quot;glm&quot;, family=binomial(link = logit)) glmLogit &lt;- glm (bugs ~ ., data=jdtTrain, family=binomial(link = logit)) summary(glmLogit) ## ## Call: ## glm(formula = bugs ~ ., family = binomial(link = logit), data = jdtTrain) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -3.573 -0.613 -0.538 -0.497 2.099 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -2.08638 0.13462 -15.50 &lt; 2e-16 *** ## cbo 0.05646 0.00705 8.01 1.1e-15 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 831.84 on 797 degrees of freedom ## Residual deviance: 725.93 on 796 degrees of freedom ## AIC: 729.9 ## ## Number of Fisher Scoring iterations: 5 Predict a single point: newData = data.frame(cbo = 3) predict(glmLogit, newData, type = &quot;response&quot;) ## 1 ## 0.128 Draw the results, modified from: http://www.shizukalab.com/toolkits/plotting-logistic-regression-in-r results &lt;- predict(glmLogit, jdtTest, type = &quot;response&quot;) range(jdtTrain$cbo) ## [1] 0 156 range(results) ## [1] 0.110 0.984 plot(jdt2$cbo,jdt2$bugs) curve(predict(glmLogit, data.frame(cbo=x), type = &quot;response&quot;),add=TRUE) # points(jdtTrain$cbo,fitted(glmLogit)) Another type of graph: library(popbio) ## ## Attaching package: &#39;popbio&#39; ## The following object is masked from &#39;package:caret&#39;: ## ## sensitivity logi.hist.plot(jdt2$cbo,jdt2$bugs,boxp=FALSE,type=&quot;hist&quot;,col=&quot;gray&quot;) 12.4 Classification Trees There are several packages for inducing classification trees, for example with the party package (recursive partitioning): # Build a decision tree library(party) kc2 &lt;- read.arff(&quot;./datasets/defectPred/D1/MC1.arff&quot;) str(kc2) ## &#39;data.frame&#39;: 9277 obs. of 39 variables: ## $ LOC_BLANK : num 0 0 0 0 0 0 0 0 0 0 ... ## $ BRANCH_COUNT : num 1 1 1 1 1 1 1 1 1 1 ... ## $ CALL_PAIRS : num 0 0 0 0 0 0 0 0 0 0 ... ## $ LOC_CODE_AND_COMMENT : num 0 0 0 0 0 0 0 0 0 0 ... ## $ LOC_COMMENTS : num 0 0 0 0 0 0 0 0 0 0 ... ## $ CONDITION_COUNT : num 0 0 0 0 0 0 0 0 0 0 ... ## $ CYCLOMATIC_COMPLEXITY : num 1 1 1 1 1 1 1 1 1 1 ... ## $ CYCLOMATIC_DENSITY : num 1 1 1 1 1 1 1 1 1 1 ... ## $ DECISION_COUNT : num 0 0 0 0 0 0 0 0 0 0 ... ## $ DESIGN_COMPLEXITY : num 1 1 1 1 1 1 1 1 1 1 ... ## $ DESIGN_DENSITY : num 1 1 1 1 1 1 1 1 1 1 ... ## $ EDGE_COUNT : num 1 1 1 1 1 1 1 1 1 1 ... ## $ ESSENTIAL_COMPLEXITY : num 1 1 1 1 1 1 1 1 1 1 ... ## $ ESSENTIAL_DENSITY : num 0 0 0 0 0 0 0 0 0 0 ... ## $ LOC_EXECUTABLE : num 0 0 0 0 0 0 0 0 0 0 ... ## $ PARAMETER_COUNT : num 0 0 0 0 0 0 0 0 0 0 ... ## $ GLOBAL_DATA_COMPLEXITY : num 0 0 0 0 0 0 0 0 0 0 ... ## $ GLOBAL_DATA_DENSITY : num 0 0 0 0 0 0 0 0 0 0 ... ## $ HALSTEAD_CONTENT : num 0 0 0 0 0 0 0 0 0 0 ... ## $ HALSTEAD_DIFFICULTY : num 0 0 0 0 0 0 0 0 0 0 ... ## $ HALSTEAD_EFFORT : num 0 0 0 0 0 0 0 0 0 0 ... ## $ HALSTEAD_ERROR_EST : num 0 0 0 0 0 0 0 0 0 0 ... ## $ HALSTEAD_LENGTH : num 1 1 0 0 1 1 0 0 1 1 ... ## $ HALSTEAD_LEVEL : num 0 0 0 0 0 0 0 0 0 0 ... ## $ HALSTEAD_PROG_TIME : num 0 0 0 0 0 0 0 0 0 0 ... ## $ HALSTEAD_VOLUME : num 0 0 0 0 0 0 0 0 0 0 ... ## $ MAINTENANCE_SEVERITY : num 1 1 1 1 1 1 1 1 1 1 ... ## $ MODIFIED_CONDITION_COUNT : num 0 0 0 0 0 0 0 0 0 0 ... ## $ MULTIPLE_CONDITION_COUNT : num 0 0 0 0 0 0 0 0 0 0 ... ## $ NODE_COUNT : num 2 2 2 2 2 2 2 2 2 2 ... ## $ NORMALIZED_CYLOMATIC_COMPLEXITY: num 1 1 1 1 1 1 1 1 1 1 ... ## $ NUM_OPERANDS : num 0 0 0 0 0 0 0 0 0 0 ... ## $ NUM_OPERATORS : num 1 1 0 0 1 1 0 0 1 1 ... ## $ NUM_UNIQUE_OPERANDS : num 0 0 0 0 0 0 0 0 0 0 ... ## $ NUM_UNIQUE_OPERATORS : num 1 1 0 0 1 1 0 0 1 1 ... ## $ NUMBER_OF_LINES : num 1 1 1 1 1 1 1 1 1 1 ... ## $ PERCENT_COMMENTS : num 0 0 0 0 0 0 0 0 0 0 ... ## $ LOC_TOTAL : num 0 0 0 0 0 0 0 0 0 0 ... ## $ Defective : Factor w/ 2 levels &quot;N&quot;,&quot;Y&quot;: 1 1 1 1 1 1 1 1 1 1 ... set.seed(1) inTrain &lt;- createDataPartition(y=kc2$Defective,p=.60,list=FALSE) kc2.train &lt;- kc2[inTrain,] kc2.test &lt;- kc2[-inTrain,] kc2.formula &lt;- kc2$Defective ~ . kc2.ctree &lt;- ctree(kc2.formula, data = kc2.train) # predict on test data pred &lt;- predict(kc2.ctree, newdata = kc2.test) # check prediction result table(pred, kc2.test$Defective) ## ## pred N Y ## N 3683 27 ## Y 0 0 plot(kc2.ctree) Using the C50, there are two ways, specifying train and testing library(C50) c50t &lt;- C5.0(kc1.train[,-ncol(kc1.train)], kc1.train[,ncol(kc1.train)]) summary(c50t) ## ## Call: ## C5.0.default(x = kc1.train[, -ncol(kc1.train)], y = ## kc1.train[, ncol(kc1.train)]) ## ## ## C5.0 [Release 2.07 GPL Edition] Mon Apr 3 20:12:46 2017 ## ------------------------------- ## ## Class specified by attribute `outcome&#39; ## ## Read 1573 cases (22 attributes) from undefined.data ## ## Decision tree: ## ## LOC_EXECUTABLE &lt;= 4: N (745/22) ## LOC_EXECUTABLE &gt; 4: ## :...HALSTEAD_ERROR_EST &lt;= 0.36: N (734/169) ## HALSTEAD_ERROR_EST &gt; 0.36: ## :...DESIGN_COMPLEXITY &gt; 19: N (6) ## DESIGN_COMPLEXITY &lt;= 19: ## :...LOC_CODE_AND_COMMENT &lt;= 1: Y (71/22) ## LOC_CODE_AND_COMMENT &gt; 1: N (17/4) ## ## ## Evaluation on training data (1573 cases): ## ## Decision Tree ## ---------------- ## Size Errors ## ## 5 217(13.8%) &lt;&lt; ## ## ## (a) (b) &lt;-classified as ## ---- ---- ## 1307 22 (a): class N ## 195 49 (b): class Y ## ## ## Attribute usage: ## ## 100.00% LOC_EXECUTABLE ## 52.64% HALSTEAD_ERROR_EST ## 5.98% DESIGN_COMPLEXITY ## 5.59% LOC_CODE_AND_COMMENT ## ## ## Time: 0.0 secs plot(c50t) c50tPred &lt;- predict(c50t, kc1.train) table(c50tPred, kc1.train$Defective) ## ## c50tPred N Y ## N 1307 195 ## Y 22 49 or using the formula approach: # Using the formula notation c50t2 &lt;- C5.0(Defective ~ ., kc1.train) c50tPred2 &lt;- predict(c50t2, kc1.train) table(c50tPred2, kc1.train$Defective) ## ## c50tPred2 N Y ## N 1307 195 ## Y 22 49 Using the ‘rpart’ package # Using the &#39;rpart&#39; package library(rpart) kc1.rpart &lt;- rpart(Defective ~ ., data=kc1.train) plot(kc1.rpart) library(rpart.plot) #asRules(kc1.rpart) #fancyRpartPlot(kc1.rpart) 12.5 Rules C5 Rules library(C50) c50r &lt;- C5.0(kc1.train[,-ncol(kc1.train)], kc1.train[,ncol(kc1.train)], rules = TRUE) summary(c50r) ## ## Call: ## C5.0.default(x = kc1.train[, -ncol(kc1.train)], y = ## kc1.train[, ncol(kc1.train)], rules = TRUE) ## ## ## C5.0 [Release 2.07 GPL Edition] Mon Apr 3 20:12:47 2017 ## ------------------------------- ## ## Class specified by attribute `outcome&#39; ## ## Read 1573 cases (22 attributes) from undefined.data ## ## Rules: ## ## Rule 1: (1479/191, lift 1.0) ## HALSTEAD_ERROR_EST &lt;= 0.36 ## -&gt; class N [0.870] ## ## Rule 2: (94/41, lift 3.6) ## HALSTEAD_ERROR_EST &gt; 0.36 ## -&gt; class Y [0.563] ## ## Default class: N ## ## ## Evaluation on training data (1573 cases): ## ## Rules ## ---------------- ## No Errors ## ## 2 232(14.7%) &lt;&lt; ## ## ## (a) (b) &lt;-classified as ## ---- ---- ## 1288 41 (a): class N ## 191 53 (b): class Y ## ## ## Attribute usage: ## ## 100.00% HALSTEAD_ERROR_EST ## ## ## Time: 0.0 secs c50rPred &lt;- predict(c50r, kc1.train) table(c50rPred, kc1.train$Defective) ## ## c50rPred N Y ## N 1288 191 ## Y 41 53 12.6 Distanced-based Methods In this case, there is no model as such. Given a new instance to classify, this approach finds the closest \\(k\\)-neighbours to the given instance. library(class) ind &lt;- sample(2, nrow(iris), replace=T, prob=c(0.7, 0.3)) kc1.train &lt;- kc1[ind==1, ] kc1.test &lt;- kc1[ind==2, ] m1 &lt;- knn(train=kc1.train[,-22], test=kc1.test[,-22], cl=kc1.train[,22], k=3) table(kc1.test[,22],m1) ## m1 ## N Y ## N 455 24 ## Y 64 15 12.7 Probabilistic Methods 12.7.1 Naive Bayes Using the klaR package with caret: library(caret) library(klaR) model &lt;- NaiveBayes(Defective ~ ., data = kc1.train) predictions &lt;- predict(model, kc1.test[,-22]) confusionMatrix(predictions$class, kc1.test$Defective) ## Confusion Matrix and Statistics ## ## Reference ## Prediction N Y ## N 442 53 ## Y 37 26 ## ## Accuracy : 0.839 ## 95% CI : (0.806, 0.868) ## No Information Rate : 0.858 ## P-Value [Acc &gt; NIR] : 0.917 ## ## Kappa : 0.275 ## Mcnemar&#39;s Test P-Value : 0.114 ## ## Sensitivity : 0.923 ## Specificity : 0.329 ## Pos Pred Value : 0.893 ## Neg Pred Value : 0.413 ## Prevalence : 0.858 ## Detection Rate : 0.792 ## Detection Prevalence : 0.887 ## Balanced Accuracy : 0.626 ## ## &#39;Positive&#39; Class : N ## Using the e1071 package: library (e1071) n1 &lt;-naiveBayes(kc1.train$Defective ~ ., data=kc1.train) # Show first 3 results using &#39;class&#39; head(predict(n1,kc1.test, type = c(&quot;class&quot;)),3) # class by default ## [1] N N N ## Levels: N Y # Show first 3 results using &#39;raw&#39; head(predict(n1,kc1.test, type = c(&quot;raw&quot;)),3) ## N Y ## [1,] 1 2.40e-09 ## [2,] 1 5.76e-09 ## [3,] 1 5.76e-09 12.7.2 Bayesian Networks To Do "],
["unsupervised-or-descriptive-modeling.html", "Chapter 13 Unsupervised or Descriptive modeling 13.1 Clustering 13.2 Association rules", " Chapter 13 Unsupervised or Descriptive modeling From the descriptive (unsupervised) point of view, patterns are found to predict future behaviour or estimate. This include association rules, clustering, or tree clustering which aim at grouping together objects (e.g., animals) into successively larger clusters, using some measure of similarity or distance. The dataset will be as the previous table without the \\(C\\) class attribute Att1 Attn a11 … a1n a21 … a2n … … … am1 … amn 13.1 Clustering library(foreign) library(fpc) kc1 &lt;- read.arff(&quot;./datasets/defectPred/D1/KC1.arff&quot;) # Split into training and test datasets set.seed(1) ind &lt;- sample(2, nrow(kc1), replace = TRUE, prob = c(0.7, 0.3)) kc1.train &lt;- kc1[ind==1, ] kc1.test &lt;- kc1[ind==2, ] # No class kc1.train$Defective &lt;- NULL ds &lt;- dbscan(kc1.train, eps = 0.42, MinPts = 5) kc1.kmeans &lt;- kmeans(kc1.train, 2) 13.1.1 k-Means library(reshape, quietly=TRUE) kc1.kmeans &lt;- kmeans(sapply(na.omit(kc1.train), rescaler, &quot;range&quot;), 10) 13.2 Association rules library(arules) x &lt;- as.numeric(kc1$LOC_TOTAL) str(x) ## num [1:2096] 5 3 3 3 12 5 3 3 3 13 ... summary(x) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 1.0 3.0 9.0 20.4 24.0 288.0 hist(x, breaks=30, main=&quot;LoC Total&quot;) xDisc &lt;- discretize(x, categories=5) # table(xDisc) for(i in 1:21) kc1[,i] &lt;- discretize(kc1[,i], &quot;frequency&quot;, categories=5) str(kc1) ## &#39;data.frame&#39;: 2096 obs. of 22 variables: ## $ LOC_BLANK : Factor w/ 4 levels &quot;0&quot;,&quot;1&quot;,&quot;[2, 4)&quot;,..: 1 1 1 1 3 1 1 1 1 3 ... ## $ BRANCH_COUNT : Factor w/ 4 levels &quot;1&quot;,&quot;3&quot;,&quot;[4, 8)&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... ## $ LOC_CODE_AND_COMMENT : Factor w/ 2 levels &quot;0&quot;,&quot;[1,12]&quot;: 1 1 1 1 1 1 1 1 1 1 ... ## $ LOC_COMMENTS : Factor w/ 3 levels &quot;0&quot;,&quot;1&quot;,&quot;[2,44]&quot;: 1 1 1 1 1 1 1 1 1 1 ... ## $ CYCLOMATIC_COMPLEXITY: Factor w/ 4 levels &quot;1&quot;,&quot;2&quot;,&quot;[3, 5)&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... ## $ DESIGN_COMPLEXITY : Factor w/ 3 levels &quot;1&quot;,&quot;[2, 4)&quot;,&quot;[4,45]&quot;: 1 1 1 1 1 1 1 1 1 1 ... ## $ ESSENTIAL_COMPLEXITY : Factor w/ 2 levels &quot;1&quot;,&quot;[3,26]&quot;: 1 1 1 1 1 1 1 1 1 1 ... ## $ LOC_EXECUTABLE : Factor w/ 5 levels &quot; 0&quot;,&quot;[ 1, 3)&quot;,..: 3 2 2 2 3 3 2 2 2 3 ... ## $ HALSTEAD_CONTENT : Factor w/ 5 levels &quot;[ 0.00, 5.80)&quot;,..: 3 1 1 1 3 3 1 1 1 4 ... ## $ HALSTEAD_DIFFICULTY : Factor w/ 5 levels &quot;[ 0.00, 1.57)&quot;,..: 3 1 1 1 3 3 1 1 1 3 ... ## $ HALSTEAD_EFFORT : Factor w/ 5 levels &quot;[ 0.0, 12.2)&quot;,..: 3 1 1 1 3 3 1 1 1 3 ... ## $ HALSTEAD_ERROR_EST : Factor w/ 5 levels &quot;0.00&quot;,&quot;0.01&quot;,..: 2 1 1 1 3 2 1 1 1 3 ... ## $ HALSTEAD_LENGTH : Factor w/ 5 levels &quot;[ 0, 5)&quot;,&quot;[ 5, 10)&quot;,..: 3 1 1 1 3 3 1 1 1 4 ... ## $ HALSTEAD_LEVEL : Factor w/ 5 levels &quot;[0.00,0.08)&quot;,..: 4 1 1 1 3 4 1 1 1 3 ... ## $ HALSTEAD_PROG_TIME : Factor w/ 5 levels &quot;[ 0.00, 0.68)&quot;,..: 3 1 1 1 3 3 1 1 1 3 ... ## $ HALSTEAD_VOLUME : Factor w/ 5 levels &quot;[ 0.0, 10.0)&quot;,..: 3 1 1 1 3 3 1 1 1 4 ... ## $ NUM_OPERANDS : Factor w/ 5 levels &quot;[ 0, 2)&quot;,&quot;[ 2, 4)&quot;,..: 3 1 1 1 3 3 1 1 1 3 ... ## $ NUM_OPERATORS : Factor w/ 5 levels &quot;[ 0, 4)&quot;,&quot;[ 4, 7)&quot;,..: 3 1 1 1 3 3 1 1 1 4 ... ## $ NUM_UNIQUE_OPERANDS : Factor w/ 5 levels &quot;[ 0, 2)&quot;,&quot;[ 2, 4)&quot;,..: 2 1 1 1 3 2 1 1 1 4 ... ## $ NUM_UNIQUE_OPERATORS : Factor w/ 5 levels &quot;[ 0, 4)&quot;,&quot;[ 4, 6)&quot;,..: 2 1 1 1 2 2 1 1 1 3 ... ## $ LOC_TOTAL : Factor w/ 5 levels &quot;[ 1, 3)&quot;,&quot;[ 3, 6)&quot;,..: 2 2 2 2 3 2 2 2 2 3 ... ## $ Defective : Factor w/ 2 levels &quot;N&quot;,&quot;Y&quot;: 1 1 1 1 1 1 1 1 1 1 ... rules &lt;- apriori(kc1, parameter = list(support=0.60, confidence=0.800, minlen=3)) ## Apriori ## ## Parameter specification: ## confidence minval smax arem aval originalSupport maxtime support minlen ## 0.8 0.1 1 none FALSE TRUE 5 0.6 3 ## maxlen target ext ## 10 rules FALSE ## ## Algorithmic control: ## filter tree heap memopt load sort verbose ## 0.1 TRUE TRUE FALSE TRUE 2 TRUE ## ## Absolute minimum support count: 1257 ## ## set item appearances ...[0 item(s)] done [0.00s]. ## set transactions ...[94 item(s), 2096 transaction(s)] done [0.00s]. ## sorting and recoding items ... [5 item(s)] done [0.00s]. ## creating transaction tree ... done [0.00s]. ## checking subsets of size 1 2 3 4 done [0.00s]. ## writing ... [18 rule(s)] done [0.00s]. ## creating S4 object ... done [0.00s]. rules ## set of 18 rules rules &lt;- apriori(kc1, parameter = list(minlen=3, supp=0.6, conf=0.8), appearance = list(rhs=c(&quot;Defective=Y&quot;, &quot;Defective=N&quot;), default=&quot;lhs&quot;), control = list(verbose=F)) #rules &lt;- apriori(kc1, # parameter = list(minlen=2, supp=0.05, conf=0.3), # appearance = list(rhs=c(&quot;Defective=Y&quot;, &quot;Defective=N&quot;), # default=&quot;lhs&quot;)) inspect(rules) ## lhs rhs support confidence lift ## [1] {LOC_COMMENTS=0, ## ESSENTIAL_COMPLEXITY=1} =&gt; {Defective=N} 0.653 0.907 1.07 ## [2] {LOC_CODE_AND_COMMENT=0, ## LOC_COMMENTS=0} =&gt; {Defective=N} 0.668 0.887 1.05 ## [3] {LOC_CODE_AND_COMMENT=0, ## ESSENTIAL_COMPLEXITY=1} =&gt; {Defective=N} 0.729 0.878 1.04 ## [4] {LOC_CODE_AND_COMMENT=0, ## LOC_COMMENTS=0, ## ESSENTIAL_COMPLEXITY=1} =&gt; {Defective=N} 0.643 0.908 1.07 rules ## set of 4 rules library(arulesViz) plot(rules) "],
["evaluation-of-models.html", "Chapter 14 Evaluation of Models 14.1 Underfitting vs. Overfitting 14.2 Building and Validating a Model 14.3 Cross Validation (CV) 14.4 Evaluation of Classifiers", " Chapter 14 Evaluation of Models Once we obtain the model with the training data, we need to evaluate it with some new data (testing data) We cannnot use the the same data for training and testing (it is like evaluating a student with the exercises previouly solved. Student’s marks will be “optimistic” and we don’t know about student capability to generalise the learned concepts). 14.1 Underfitting vs. Overfitting For example, increasing the tree size, decreases the training and testing errors. However, at some point after (tree complexity), training error keeps decreasing but testing error increases. Many algorithms have parameters to determine the model complexity (e.g., in decision trees is the prunning parameter) Overfitting in trees 14.2 Building and Validating a Model 14.2.1 Holdout approach Holdout approach consists of dividing the dataset into training (approx. 2/3 of the data) and testing (approx 1/3 of the data). + Problems: Data can be skewed, missing classes, etc. if randomly divided Stratification ensures that each class is represented with approximately equal proportions (e.g., if data contains aprox 45% of positive cases, the training and testing datasets should mantain similar proportion of positive cases). Holdout estimate can be made more reliable by repeating the process with different subsamples (repeated holdout method) The error rates on the different iterations are averaged (overall error rate) Usually, part of the data points are used for building the model and the remaining points are used for validating the model. There are several approaches to this process. Validation Set approach: it is the simplest method. It consists of randomly dividing the available set of oservations into two parts, a training set and a validation set or hold-out set. Usually 2/3 of the data points are used for training and 1/3 is used for testing purposes. 14.3 Cross Validation (CV) k-Fold Cross-Validation: it involves randomly dividing the set of observations into \\(k\\) groups, or folds, of approximately equal size. The first fold is treated as a validation set, the the methods is fit on the remaining k-1 folds. This procedure is repeated k times. If k is equal to n we are in the previous method. 1st step: split dataset (\\(\\cal D\\)) into k subsets of approximatelly equal size \\(C_1, \\dots, C_k\\) 2nd step: we construct a dataset \\(D_i = D-C_i\\) used for training and test the accuracy of the classifier \\(D_i\\) on \\(C_i\\) subset for testing Having done this for all \\(k\\) we estimate the accuracy of the method by averaging the accuracy over the \\(k\\) cross-validation trials k-fold Leave-One-Out Cross-Validation: This is a special case of CV. Instead of creating two subsets for training and testing, a single observation is used for the validation set, and the remaining observations make up the training set. This approach is repeated n times (the total number of observations) and the estimate for the test mean squared error is the average of the n test estimates. LOO 14.3.1 China dataset. Split data into Training and Testing The data is already divided into two different files library(foreign) chinaTrain &lt;- read.arff(&quot;./datasets/effortEstimation/china3AttSelectedAFPTrain.arff&quot;) nrow(chinaTrain) ## [1] 332 logchina_size &lt;- log(chinaTrain$AFP) logchina_effort &lt;- log(chinaTrain$Effort) linmodel_logchina_train &lt;- lm(logchina_effort ~ logchina_size) par(mfrow=c(1,1)) plot(logchina_size, logchina_effort) abline(linmodel_logchina_train, lwd=3, col=4) par(mfrow=c(1,2)) plot(linmodel_logchina_train, ask = FALSE) linmodel_logchina_train ## ## Call: ## lm(formula = logchina_effort ~ logchina_size) ## ## Coefficients: ## (Intercept) logchina_size ## 3.249 0.784 14.4 Evaluation of Classifiers 14.4.1 Discrete Evaluation The confusion matrix (which can be extended to multiclass problems). The following table shows the possible outcomes for binary classification problems: \\(Pred Pos\\) \\(Pred Neg\\) \\(Act Pos\\) \\(TP\\) \\(FN\\) \\(Act Neg\\) \\(FP\\) \\(TN\\) where True Positives (\\(TP\\)) and True Negatives (\\(TN\\)) are respectively the number of positive and negative instances correctly classified, False Positives (\\(FP\\)) is the number of negative instances misclassified as positive (also called Type I errors), and False Negatives (\\(FN\\)) is the number of positive instances misclassified as negative (Type II errors). From the confusion matrix, we can calculate: True positive rate, or recall \\(TP_r = recall = r = TP/TP+FN\\) is the proportion of positive cases correctly classified as belonging to the positive class. False negative rate (\\(FN_r=FN/TP+FN\\)) is the proportion of positive cases misclassified as belonging to the negative class. False positive rate (\\(FP_r=FP/FP+TN\\)) is the proportion of negative cases misclassified as belonging to the positive class. True negative rate (\\(TN_r=TN/FP+TN\\)) is the proportion of negative cases correctly classified as belonging to the negative class. There is a tradeoff between \\(FP_r\\) and \\(FN_r\\) as the objective is minimize both metrics (or conversely, maximize the true negative and positive rates). It is possible to combine both metrics into a single figure, predictive \\(accuracy\\): (\\(accuracy = \\frac{TP + TN}{TP + TN + FP + FN}\\)) to measure performance of classifiers (or the complementary value, the error rate} which is defined as \\(1-accuracy\\)) f-measure G-mean \\(\\sqrt{PD \\times Precision}\\) G-mean2 \\(\\sqrt{PD \\times Specificity}\\) j-coeff = sensitivity + specificity - 1 = PD -PF (Jiang, Cubic and Ma, 2008 ESE) No Free Lunch theorem In the absence of any knowledge about the prediction problem, no model can be said to be uniformly better than any other 14.4.2 Prediction in probabilistic classifiers A probabilistic classifier estimates the probability of each of the posible class values given the attribute values of the instance \\(P(c|{x})\\). Then, given a new instance, \\({x}\\), the class value with the highest a posteriori probability will be assigned to that new instance (the winner takes all approach): \\(\\psi({x}) = argmax_c (P(c|{x}))\\) 14.4.3 Graphical Evaluation ROC Precision Recall Another evaluation technique to consider when data is imbalanced is the Receiver Operating Characteristic (\\(ROC\\))~ curve which provides a graphical visualisation of the results. A simple way to approximate the AUC is with the following equation: \\(AUC=\\frac{1+TP_{r}-FP_{r}}{2}\\) The Area Under the ROC Curve (AUC) also provides a quality measure between positive and negative rates with a single value. Receiver Operating Characteristic Similarly to ROC, another widely used evaluation technique is the Precision-Recall Curve (PRC), which depicts a trade off between precision and recall and can also be summarised into a single value as the Area Under the Precision-Recall Curve (AUPRC)~. %AUPCR is more accurate than the ROC for testing performances when dealing with imbalanced datasets as well as optimising ROC values does not necessarily optimises AUPR values, i.e., a good classifier in AUC space may not be so good in PRC space. %The weighted average uses weights proportional to class frequencies in the data. %The weighted average is computed by weighting the measure of class (TP rate, precision, recall …) by the proportion of instances there are in that class. Computing the average can be sometimes be misleading. For instance, if class 1 has 100 instances and you achieve a recall of 30%, and class 2 has 1 instance and you achieve recall of 100% (you predicted the only instance correctly), then when taking the average (65%) you will inflate the recall score because of the one instance you predicted correctly. Taking the weighted average will give you 30.7%, which is much more realistic measure of the performance of the classifier. %NB: I gave an example with two classes, but in fact the weighted average make sense only when you have more then two classes. When you have only two classes weighting does not make sense, and the measures should be computed relative to the minority class. In other words, you are interested to know if you are able to detect the minority 14.4.4 Metrics used in Software Engineering and Defect Classification In the domain of defect prediction and when two classes are considered, it is also customary to refer to the probability of detection, (\\(pd\\)) which corresponds to the True Positive rate (\\(TP_{rate}\\) or ) as a measure of the goodness of the model, and probability of false alarm (\\(pf\\)) as performance measures~. The objective is to find which techniques that maximise \\(pd\\) and minimise \\(pf\\). As stated by Menzies et al., the balance between these two measures depends on the project characteristics (e.g. real-time systems vs. information management systems) it is formulated as the Euclidean distance from the sweet spot \\(pf=0\\) and \\(pd=1\\) to a pair of \\((pf,pd)\\). \\(balance=1-\\frac{\\sqrt{(0-pf^2)+(1-pd^2)}}{\\sqrt{2}}\\) It is normalized by the maximum possible distance across the ROC square (\\(\\sqrt{2}, 2\\)), subtracted this value from 1, and expressed it as a percentage. 14.4.5 Numeric Prediction Evaluation RSME Mean Square Error = \\(MSE\\) = \\(\\frac{(p_1-a_1)^2 + \\ldots +(p_n-a_n)^2}{n}\\) \\({MSE}=\\frac{1}{n}\\sum_{i=1}^n(\\hat{y_i} - y_i)^2\\) \\({RMSD}=\\sqrt{\\frac{\\sum_{t=1}^n (\\hat y_t - y)^2}{n}}\\) A suitable and interesting performance metric for binary classification when data are imbalanced is the Matthew’s Correlation Coefficient (\\(MCC\\))~: \\(MCC=\\frac{TP\\times TN - FP\\times FN}{\\sqrt{(TP+FP)(TP+FN)(TN+FP)(TN+FN)}}\\) \\(MCC\\) can also be calculated from the confusion matrix. Its range goes from -1 to +1; the closer to one the better as it indicates perfect prediction whereas a value of 0 means that classification is not better than random prediction and negative values mean that predictions are worst than random. Mean-absolute error \\(MAE\\) \\(\\frac{|p_1-a_1| + \\ldots +|p_n-a_n|}{n}\\) Relative absolute error: \\(RAE = \\frac{ \\sum^N_{i=1} | \\hat{\\theta}_i - \\theta_i | } { \\sum^N_{i=1} | \\overline{\\theta} - \\theta_i | }\\) Root relative-squared error: \\(RAE = \\sqrt{ \\frac{ \\sum^N_{i=1} | \\hat{\\theta}_i - \\theta_i | } { \\sum^N_{i=1} | \\overline{\\theta} - \\theta_i | } }\\) where \\(\\hat{\\theta}\\) is a mean value of \\(\\theta\\). Relative-squared error \\(\\frac{(p_1-a_1)^2 + \\ldots +(p_n-a_n)^2}{(a_1-\\hat{a})^2 + \\ldots + (a_n-\\hat{a})^2}\\) (\\(\\hat{a}\\) is the mean value over the training data) Relative Absolut Error Correlation Coefficient Correlation coefficient between two random variables \\(X\\) and \\(Y\\) is defined as \\[\\rho(X,Y) = \\frac{{\\bf Cov}(X,Y)}{\\sqrt{{\\bf Var}(X){\\bf Var}(Y)}}.\\] The {} \\(r\\) between two samples \\(x_i\\) and \\(y_j\\) is defined as \\(r = S_{xy}/\\sqrt{S_{xx}S_{yy}}.\\) Example: Is there any linear relationship between the effort estimates (\\(p_i\\)) and actual effort (\\(a_i\\))? \\(a\\|39,43,21,64,57,47,28,75,34,52\\) \\(p\\|65,78,52,82,92,89,73,98,56,75\\) p&lt;-c(39,43,21,64,57,47,28,75,34,52) a&lt;-c(65,78,52,82,92,89,73,98,56,75) # cor(p,a) ## [1] 0.84 \\(R^2\\) "],
["evaluationSE.html", "Chapter 15 Measures of Evaluation in Software Engineering 15.1 Evaluation of the model in the Testing data 15.2 Building a Linear Model on the Telecom1 dataset 15.3 Building a Linear Model on the Telecom1 dataset with all observations 15.4 Standardised Accuracy. MARP0. ChinaTest 15.5 Standardised Accuracy. MARP0. Telecom1 15.6 Exact MARP0", " Chapter 15 Measures of Evaluation in Software Engineering There are several measures typically used in software engieering. In particular for effort estimation, the following metrics are extensively used in addition or instead of statistical measures. Mean of the Absolute Error (MAR): compute the absolute errors and take the mean Geometric Mean of the Absolute Error (gMAR): more appropriate when the distribution is skewed Mean Magnitude of the Relative Error (MMRE): this measure has been critisized many times as a biased measure (\\(\\frac{\\sum_{i=1}^{n}{|{\\hat{y}_i-y_i}|}/y_i}{n}\\)) Median Magnitude of the Relative Error (MdMRE): using the median insted of the mean Level of Prediction (\\(Pred(l)\\)) defined as the percentage of estimates that are within the percentage level \\(l\\) of the actual values. The level of prediction is typically set at 25% below and above the actual value and an estimation method is considered good if it gives a result of more than 75%. Standardised Accuracy (SA) (proposed by Shepperd&amp;MacDonnell): this measure overcomes all the problems of the MMRE. It is defined as the MAR relative to random guessing (\\(SA=1-{\\frac{MAR}{\\overline{MAR}_{P_0}}\\times100}\\)) Random guessing: \\(\\overline{MAR}_{P_0}\\) is defined as: predict a \\(\\hat{y}_t\\) for the target case t by randomly sampling (with equal probability) over all the remaining n-1 cases and take \\(\\hat{y}_t=y_r\\) where \\(r\\) is drawn randomly from \\(1\\) to \\(n\\) and \\(r\\neq t\\). Exact \\(\\overline{MAR}_{P_0}\\): it is an improvement over \\(\\overline{MAR}_{P_0}\\). For small datasets the “random guessing” can be computed exactly by iterating over all data points. 15.1 Evaluation of the model in the Testing data library(foreign) gm_mean = function(x, na.rm=TRUE){ exp(sum(log(x[x &gt; 0]), na.rm=na.rm) / length(x))} chinaTrain &lt;- read.arff(&quot;./datasets/effortEstimation/china3AttSelectedAFPTrain.arff&quot;) logchina_size &lt;- log(chinaTrain$AFP) logchina_effort &lt;- log(chinaTrain$Effort) linmodel_logchina_train &lt;- lm(logchina_effort ~ logchina_size) chinaTest &lt;- read.arff(&quot;./datasets/effortEstimation/china3AttSelectedAFPTest.arff&quot;) b0 &lt;- linmodel_logchina_train$coefficients[1] b1 &lt;- linmodel_logchina_train$coefficients[2] china_size_test &lt;- chinaTest$AFP actualEffort &lt;- chinaTest$Effort predEffort &lt;- exp(b0+b1*log(china_size_test)) err &lt;- actualEffort - predEffort #error or residual ae &lt;- abs(err) hist(ae, main=&quot;Absolute Error in the China Test data&quot;) mar &lt;- mean(ae) mre &lt;- ae/actualEffort mmre &lt;- mean(mre) mdmre &lt;- median(mre) gmar &lt;- gm_mean(ae) mar ## [1] 1867 mmre ## [1] 1.15 mdmre ## [1] 0.551 gmar ## [1] 833 level_pred &lt;- 0.25 #below and above (both) lowpred &lt;- actualEffort*(1-level_pred) uppred &lt;- actualEffort*(1+level_pred) pred &lt;- predEffort &lt;= uppred &amp; predEffort &gt;= lowpred #pred is a vector with logical values Lpred &lt;- sum(pred)/length(pred) Lpred ## [1] 0.186 15.2 Building a Linear Model on the Telecom1 dataset Although there are few datapoints we split the file into Train (2/3) and Test (1/3) telecom1 &lt;- read.table(&quot;./datasets/effortEstimation/Telecom1.csv&quot;, sep=&quot;,&quot;,header=TRUE, stringsAsFactors=FALSE, dec = &quot;.&quot;) #read data samplesize &lt;- floor(0.66*nrow(telecom1)) set.seed(012) # to make the partition reproducible train_idx &lt;- sample(seq_len(nrow(telecom1)), size = samplesize) telecom1_train &lt;- telecom1[train_idx, ] telecom1_test &lt;- telecom1[-train_idx, ] par(mfrow=c(1,1)) # transformation of variables to log-log xtrain &lt;- log(telecom1_train$size) ytrain &lt;- log(telecom1_train$effort) lmtelecom1 &lt;- lm( ytrain ~ xtrain) plot(xtrain, ytrain) abline(lmtelecom1, lwd=2, col=&quot;blue&quot;) b0_tel1 &lt;- lmtelecom1$coefficients[1] b1_tel1 &lt;- lmtelecom1$coefficients[2] # calculate residuals and predicted values res &lt;- signif(residuals(lmtelecom1), 5) xtest &lt;- telecom1_test$size ytest &lt;- telecom1_test$effort pre_tel1 &lt;- exp(b0+b1*log(xtest)) # plot distances between points and the regression line plot(xtest, ytest) curve(exp(b0_tel1+b1_tel1*log(x)), from=0, to=300, add=TRUE, col=&quot;blue&quot;, lwd=2) segments(xtest, ytest, xtest, pre_tel1, col=&quot;red&quot;) 15.3 Building a Linear Model on the Telecom1 dataset with all observations Just to visualize results par(mfrow=c(1,1)) effort_telecom1 &lt;- telecom1$effort size_telecom1 &lt;- telecom1$size lmtelecom &lt;- lm(effort_telecom1 ~ size_telecom1) plot(size_telecom1, effort_telecom1) abline(lmtelecom, lwd=3, col=&quot;blue&quot;) # calculate residuals and predicted values res &lt;- signif(residuals(lmtelecom), 5) predicted &lt;- predict(lmtelecom) # plot distances between points and the regression line segments(size_telecom1, effort_telecom1, size_telecom1, predicted, col=&quot;red&quot;) level_pred &lt;- 0.25 #below and above (both) lowpred &lt;- effort_telecom1*(1-level_pred) uppred &lt;- effort_telecom1*(1+level_pred) predict_inrange &lt;- predicted &lt;= uppred &amp; predicted &gt;= lowpred #pred is a vector with logical values Lpred &lt;- sum(predict_inrange)/length(predict_inrange) Lpred ## [1] 0.444 #Visually plot lpred segments(size_telecom1, lowpred, size_telecom1, uppred, col=&quot;red&quot;, lwd=3) err_telecom1 &lt;- abs(effort_telecom1 - predicted) mar_tel1 &lt;- mean(err_telecom1) mar_tel1 ## [1] 125 15.4 Standardised Accuracy. MARP0. ChinaTest Computing \\(MARP_0\\) in the China Test data estimEffChinaTest &lt;- predEffort # This will be overwritten, no problem numruns &lt;- 9999 randguessruns &lt;- rep(0, numruns) for (i in 1:numruns) { for (j in 1:length(estimEffChinaTest)) { estimEffChinaTest[j] &lt;- sample(actualEffort[-j],1)}#replacement with random guessingt randguessruns[i] &lt;- mean(abs(estimEffChinaTest-actualEffort)) } marp0Chinatest &lt;- mean(randguessruns) marp0Chinatest ## [1] 3956 hist(randguessruns, main=&quot;MARP0 distribution of the China dataset&quot;) saChina = (1- mar/marp0Chinatest)*100 saChina ## [1] 52.8 15.5 Standardised Accuracy. MARP0. Telecom1 Computing \\(MARP_0\\) telecom1 &lt;- read.table(&quot;./datasets/effortEstimation/Telecom1.csv&quot;, sep=&quot;,&quot;,header=TRUE, stringsAsFactors=FALSE, dec = &quot;.&quot;) #read data #par(mfrow=c(1,2)) #size &lt;- telecom1[1]$size not needed now actualEffTelecom1 &lt;- telecom1[2]$effort estimEffTelecom1 &lt;- telecom1[3]$EstTotal # this will be overwritten numruns &lt;- 9999 randguessruns &lt;- rep(0, numruns) for (i in 1:numruns) { for (j in 1:length(estimEffTelecom1)) { estimEffTelecom1[j] &lt;- sample(actualEffTelecom1[-j],1)}#replacement with random guessingt randguessruns[i] &lt;- mean(abs(estimEffTelecom1-actualEffTelecom1)) } marp0telecom1 &lt;- mean(randguessruns) marp0telecom1 ## [1] 271 hist(randguessruns, main=&quot;MARP0 distribution of the Telecom1 dataset&quot;) saTelecom1 &lt;- (1- mar_tel1/marp0telecom1)*100 saTelecom1 ## [1] 54 15.5.1 MARP0 in the Atkinson dataset For checking results you may use figure Atkinson in Shepperd&amp;MacDonnell ## [1] 281 15.6 Exact MARP0 "],
["wbl-simple-r-code-to-calculate-shepperd-and-macdonells-marp0-exactly.html", "Chapter 16 WBL simple R code to calculate Shepperd and MacDonell’s marp0 exactly 16.1 Computing the bootstraped confidence interval of the mean for the Test observations of the China dataset:", " Chapter 16 WBL simple R code to calculate Shepperd and MacDonell’s marp0 exactly #example dataset atkinson_actual_effort &lt;- c(670,912,218,595,267,344,229,190,869,109,289,616,557,416,578,438) myabs &lt;- function(x,y) abs(x-y) #diffs is square array whose i,jth element = abs(actual_i - actual_j) #in practice this is good enough but could be made more efficient by not #explicitly storing the matrix and only using the values below the diagonal. diffs &lt;- outer(atkinson_actual_effort,atkinson_actual_effort,myabs) marp0 &lt;- mean(diffs) marp0 ## [1] 264 #### same procedure without using the outer function act_effort &lt;- c(670,912,218,595,267,344,229,190,869,109,289,616,557,416,578,438) n &lt;- length(act_effort) diffs_guess &lt;- matrix(nrow=n, ncol=n) colnames(diffs_guess) &lt;- act_effort rownames(diffs_guess) &lt;- act_effort for (i in 1:n){ diffs_guess[i,] &lt;- act_effort - act_effort[i] } diffs_guess &lt;- abs(diffs_guess) means_per_point &lt;- apply(diffs_guess, 2, mean) marp0 &lt;- mean(means_per_point) marp0 ## [1] 264 16.1 Computing the bootstraped confidence interval of the mean for the Test observations of the China dataset: library(boot) ## ## Attaching package: &#39;boot&#39; ## The following object is masked from &#39;package:survival&#39;: ## ## aml ## The following object is masked from &#39;package:lattice&#39;: ## ## melanoma ## The following object is masked from &#39;package:sm&#39;: ## ## dogs hist(ae, main=&quot;Absolute Errors of the China Test data&quot;) level_confidence &lt;- 0.95 repetitionsboot &lt;- 9999 samplemean &lt;- function(x, d){return(mean(x[d]))} b_mean &lt;- boot(ae, samplemean, R=repetitionsboot) confint_mean_China &lt;- boot.ci(b_mean) ## Warning in boot.ci(b_mean): bootstrap variances needed for studentized ## intervals confint_mean_China ## BOOTSTRAP CONFIDENCE INTERVAL CALCULATIONS ## Based on 9999 bootstrap replicates ## ## CALL : ## boot.ci(boot.out = b_mean) ## ## Intervals : ## Level Normal Basic ## 95% (1412, 2315 ) (1384, 2283 ) ## ## Level Percentile BCa ## 95% (1451, 2350 ) (1486, 2419 ) ## Calculations and Intervals on Original Scale Computing the bootstraped geometric mean boot_geom_mean &lt;- function(error_vec){ log_error &lt;- log(error_vec[error_vec &gt; 0]) log_error &lt;-log_error[is.finite(log_error)] #remove the -Inf value before calculating the mean, just in case samplemean &lt;- function(x, d){return(mean(x[d]))} b &lt;- boot(log_error, samplemean, R=repetitionsboot) # with package boot # this is a boot for the logs return(b) } # BCAconfidence interval for the geometric mean BCAciboot4geommean &lt;- function(b){ conf_int &lt;- boot.ci(b, conf=level_confidence, type=&quot;bca&quot;)$bca #following 10.9 of Ugarte et al.&#39;s book conf_int[5] &lt;- exp(conf_int[5]) # the boot was computed with log. Now take the measure back to its previous units conf_int[4] &lt;- exp(conf_int[4]) return (conf_int) } # this is a boot object b_gm &lt;- boot_geom_mean(ae) #&quot;ae&quot; is the absolute error in the China Test data print(paste0(&quot;Geometric Mean of the China Test data: &quot;, round(exp(b_gm$t0), digits=3))) ## [1] &quot;Geometric Mean of the China Test data: 832.55&quot; b_ci_gm &lt;- BCAciboot4geommean(b_gm) print(paste0(&quot;Confidence Interval: &quot;, round(b_ci_gm[4], digits=3), &quot; - &quot;, round(b_ci_gm[5], digits=3))) ## [1] &quot;Confidence Interval: 673.125 - 1009.427&quot; # Make a % confidence interval bca # BCAciboot &lt;- function(b){ # conf_int &lt;- boot.ci(b, conf=level_confidence, type=&quot;bca&quot;)$bca #following 10.9 of Ugarte et al.&#39;s book # return (conf_int) # } "],
["feature-selection.html", "Chapter 17 Feature Selection 17.1 Instance Selection 17.2 Missing Data Imputation", " Chapter 17 Feature Selection This technique consists in selecting the most relevant attributes. The need of applying FS includes the following points: A reduced volume of data allows different data mining or searching techniques to be applied. Irrelevant and redundant attributes can generate less accurate and more complex models. Furthermore, data mining algorithms can be executed faster. It is possible to avoid the collection of data for those irrelevant and redundant attributes in the future. FS algorithms designed with different evaluation criteria broadly fall into two categories [21, 14, 13, 3, 1, 12, 5]: The filter model relies on general characteristics of the data to evaluate and select feature subsets without involving any data mining algorithm. The wrapper model requires one predetermined mining algorithm and uses its performance as the evaluation criterion. It searches for features better suited to the mining algorithm aiming to improve mining performance, but it also tends to be more computationally expensive than filter model [11, 12]. 17.1 Instance Selection 17.2 Missing Data Imputation "],
["feature-selection-example.html", "Chapter 18 Feature Selection Example", " Chapter 18 Feature Selection Example Feature Selection in R and Caret library(caret) library(doParallel) # parallel processing ## Loading required package: iterators ## Loading required package: parallel library(dplyr) # Used by caret library(pROC) # plot the ROC curve ## Type &#39;citation(&quot;pROC&quot;)&#39; for a citation. ## ## Attaching package: &#39;pROC&#39; ## The following object is masked from &#39;package:glmnet&#39;: ## ## auc ## The following objects are masked from &#39;package:stats&#39;: ## ## cov, smooth, var library(foreign) ### Use the segmentationData from caret # Load the data and construct indices to divided it into training and test data sets. #set.seed(10) kc1 &lt;- read.arff(&quot;./datasets/defectPred/D1/KC1.arff&quot;) inTrain &lt;- createDataPartition(y = kc1$Defective, ## the outcome data are needed p = .75, ## The percentage of data in the ## training set list = FALSE) The function createDataPartition does a stratified partitions. training &lt;- kc1[inTrain,] nrow(training) ## [1] 1573 testing &lt;- kc1[-inTrain, ] nrow(testing) ## [1] 523 The train function can be used to + evaluate, using resampling, the effect of model tuning parameters on performance + choose the “optimal” model across these parameters + estimate model performance from a training set fitControl &lt;- trainControl(## 10-fold CV method = &quot;repeatedcv&quot;, number = 10, ## repeated ten times repeats = 10) gbmFit1 &lt;- train(Defective ~ ., data = training, method = “gbm”, trControl = fitControl, ## This last option is actually one ## for gbm() that passes through verbose = FALSE) gbmFit1 plsFit &lt;- train(Defective ~ ., data = training, method = &quot;pls&quot;, ## Center and scale the predictors for the training ## set and all future samples. preProc = c(&quot;center&quot;, &quot;scale&quot;) ) To fix {r} testPred &lt;- predict(plsFit, testing) postResample(testPred, testing$Defective) sensitivity(testPred, testing$Defective) confusionMatrix(testPred, testing$Defective) When there are three or more classes, confusionMatrix will show the confusion matrix and a set of “one-versus-all” results. "],
["advanced-models.html", "Chapter 19 Advanced Models 19.1 Genetic Programming for Symbolic Regression 19.2 Genetic Programming Example 19.3 Neural Networks 19.4 Support Vector Machines 19.5 Ensembles", " Chapter 19 Advanced Models 19.1 Genetic Programming for Symbolic Regression This technique is inspired by Darwin’s evolution theory. + 1960s by I. Rechenberg in his work “Evolution strategies“ + 1975 Genetic Algorithms (GAs) invented by J Holland and published in his book”Adaption in Natural and Artificial Systems“ + 1992 J. Koza has used genetic algorithm to evolve programs to perform certain tasks. He called his method “genetic programming” Other reference for GP: Langdon WB, Poli R (2001) Foundations of Genetic Programming. Springer. Depending on the function set used and the function to be minimised, GP can generate almost any type of curve In R, we can use the “rgp” package 19.2 Genetic Programming Example 19.2.1 Load Data library(foreign) #read data telecom1 &lt;- read.table(&quot;./datasets/effortEstimation/Telecom1.csv&quot;, sep=&quot;,&quot;,header=TRUE, stringsAsFactors=FALSE, dec = &quot;.&quot;) size_telecom1 &lt;- telecom1$size effort_telecom1 &lt;- telecom1$effort chinaTrain &lt;- read.arff(&quot;./datasets/effortEstimation/china3AttSelectedAFPTrain.arff&quot;) china_train_size &lt;- chinaTrain$AFP china_train_effort &lt;- chinaTrain$Effort chinaTest &lt;- read.arff(&quot;./datasets/effortEstimation/china3AttSelectedAFPTest.arff&quot;) china_size_test &lt;- chinaTest$AFP actualEffort &lt;- chinaTest$Effort 19.2.2 Genetic Programming for Symbolic Regression: China dataset. library(&quot;rgp&quot;) ## *** RGP version 0.4-1 initialized successfully. ## Type &#39;help(package=&quot;rgp&quot;)&#39; to bring up the RGP help pages, ## or type &#39;vignette(&quot;rgp_introduction&quot;)&#39; to show RGP&#39;s package vignette. ## Type &#39;symbolicRegressionUi()&#39; to bring up the symbolic regression UI if ## the optional package &#39;rgpui&#39; is installed. options(digits = 5) stepsGenerations &lt;- 1000 initialPopulation &lt;- 500 Steps &lt;- c(1000) y &lt;- china_train_effort # x &lt;- china_train_size # data2 &lt;- data.frame(y, x) # create a data frame with effort, size # newFuncSet &lt;- mathFunctionSet # alternatives to mathFunctionSet # newFuncSet &lt;- expLogFunctionSet # sqrt&quot;, &quot;exp&quot;, and &quot;ln&quot; # newFuncSet &lt;- trigonometricFunctionSet # newFuncSet &lt;- arithmeticFunctionSet newFuncSet &lt;- functionSet(&quot;+&quot;,&quot;-&quot;,&quot;*&quot;, &quot;/&quot;,&quot;sqrt&quot;, &quot;log&quot;, &quot;exp&quot;) # ,, ) gpresult &lt;- symbolicRegression(y ~ x, data=data2, functionSet=newFuncSet, populationSize=initialPopulation, stopCondition=makeStepsStopCondition(stepsGenerations)) ## STARTING genetic programming evolution run (Age/Fitness/Complexity Pareto GP search-heuristic) ... ## evolution step 100, fitness evaluations: 4950, best fitness: 5615.407251, time elapsed: 3.11 seconds ## evolution step 200, fitness evaluations: 9950, best fitness: 5615.407251, time elapsed: 6.06 seconds ## evolution step 300, fitness evaluations: 14950, best fitness: 5615.407251, time elapsed: 8.83 seconds ## evolution step 400, fitness evaluations: 19950, best fitness: 5615.407251, time elapsed: 11.43 seconds ## evolution step 500, fitness evaluations: 24950, best fitness: 5615.407251, time elapsed: 14.27 seconds ## evolution step 600, fitness evaluations: 29950, best fitness: 5615.407251, time elapsed: 16.93 seconds ## evolution step 700, fitness evaluations: 34950, best fitness: 5615.407251, time elapsed: 19.55 seconds ## evolution step 800, fitness evaluations: 39950, best fitness: 5615.407251, time elapsed: 22.79 seconds ## evolution step 900, fitness evaluations: 44950, best fitness: 5615.407251, time elapsed: 25.37 seconds ## evolution step 1000, fitness evaluations: 49950, best fitness: 5615.407251, time elapsed: 28.11 seconds ## Genetic programming evolution run FINISHED after 1000 evolution steps, 49950 fitness evaluations and 28.11 seconds. bf &lt;- gpresult$population[[which.min(sapply(gpresult$population, gpresult$fitnessFunction))]] wf &lt;- gpresult$population[[which.max(sapply(gpresult$population, gpresult$fitnessFunction))]] bf1 &lt;- gpresult$population[[which.min((gpresult$fitnessValues))]] plot(x,y) lines(x, bf(x), type = &quot;l&quot;, col=&quot;blue&quot;, lwd=3) lines(x,wf(x), type = &quot;l&quot;, col=&quot;red&quot;, lwd=2) x_test &lt;- china_size_test estim_by_gp &lt;- bf(x_test) ae_gp &lt;- abs(actualEffort - estim_by_gp) mean(ae_gp) ## [1] 1962 19.2.3 Genetic Programming for Symbolic Regression. Telecom1 dataset. For illustration purposes only. We use all data points. # y &lt;- effort_telecom1 # all data points # x &lt;- size_telecom1 # # # data2 &lt;- data.frame(y, x) # create a data frame with effort, size # # newFuncSet &lt;- mathFunctionSet # # alternatives to mathFunctionSet # newFuncSet &lt;- expLogFunctionSet # sqrt&quot;, &quot;exp&quot;, and &quot;ln&quot; # # newFuncSet &lt;- trigonometricFunctionSet # # newFuncSet &lt;- arithmeticFunctionSet # # newFuncSet &lt;- functionSet(&quot;+&quot;,&quot;-&quot;,&quot;*&quot;, &quot;/&quot;,&quot;sqrt&quot;, &quot;log&quot;, &quot;exp&quot;) # ,, ) # # gpresult &lt;- symbolicRegression(y ~ x, # data=data2, functionSet=newFuncSet, # populationSize=initialPopulation, # stopCondition=makeStepsStopCondition(stepsGenerations)) # # bf &lt;- gpresult$population[[which.min(sapply(gpresult$population, gpresult$fitnessFunction))]] # wf &lt;- gpresult$population[[which.max(sapply(gpresult$population, gpresult$fitnessFunction))]] # # bf1 &lt;- gpresult$population[[which.min((gpresult$fitnessValues))]] # plot(x,y) # lines(x, bf(x), type = &quot;l&quot;, col=&quot;blue&quot;, lwd=3) # lines(x,wf(x), type = &quot;l&quot;, col=&quot;red&quot;, lwd=2) 19.3 Neural Networks A neural network (NN) simulates some of the learning functions of the human brain. It can recognize patterns and “learn” . Through the use of a trial and error method the system “learns” to become an “expert” in the field. A NN is composed of a set of nodes (units, neurons, processing elements) + Each node has input and output + Each node performs a simple computation by its node function Weighted connections between nodes + Connectivity gives the structure/architecture of the net + What can be computed by a NN is primarily determined by the connections and their weights There are several packages in R to work with NNs + neuralnet + nnet + RSNNS TO BE FIXED!!!: The following is an example with the neuralnet package (TO DO, denormalize!). Neural nets need scaling of variables to work properly. library(foreign) library(neuralnet) ## ## Attaching package: &#39;neuralnet&#39; ## The following object is masked from &#39;package:dplyr&#39;: ## ## compute chinaTrain &lt;- read.arff(&quot;datasets/effortEstimation/china3AttSelectedAFPTrain.arff&quot;) afpsize &lt;- chinaTrain$AFP effort_china &lt;- chinaTrain$Effort chinaTest &lt;- read.arff(&quot;datasets/effortEstimation/china3AttSelectedAFPTest.arff&quot;) AFPTest &lt;- chinaTest$AFP actualEffort &lt;- chinaTest$Effort trainingdata &lt;- cbind(afpsize,effort_china) colnames(trainingdata) &lt;- c(&quot;Input&quot;,&quot;Output&quot;) testingdata &lt;- cbind(afpsize,effort_china) colnames(trainingdata) &lt;- c(&quot;Input&quot;,&quot;Output&quot;) #Normalize data norm.fun = function(x){(x - min(x))/(max(x) - min(x))} data.norm = apply(trainingdata, 2, norm.fun) #data.norm testdata.norm &lt;- apply(trainingdata, 2, norm.fun) #testdata.norm #Train the neural network #Going to have 10 hidden layers #Threshold is a numeric value specifying the threshold for the partial #derivatives of the error function as stopping criteria. #net_eff &lt;- neuralnet(Output~Input,trainingdata, hidden=5, threshold=0.25) net_eff &lt;- neuralnet(Output~Input, data.norm, hidden=10, threshold=0.01) # Print the network # print(net_eff) #Plot the neural network plot(net_eff) #Test the neural network on some training data #testdata.norm&lt;-data.frame((testdata[,1] - min(data[, &#39;displ&#39;]))/(max(data[, &#39;displ&#39;])-min(data[, &#39;displ&#39;])),(testdata[,2] - min(data[, &#39;year&#39;]))/(max(data[, &#39;year&#39;])-min(data[, &#39;year&#39;])),(testdata[,3] - min(data[, &#39;cyl&#39;]))/(max(data[, &#39;cyl&#39;])-min(data[, &#39;cyl&#39;])),(testdata[,4] - min(data[, &#39;hwy&#39;]))/(max(data[, &#39;hwy&#39;])-min(data[, &#39;hwy&#39;]))) # Run them through the neural network # net.results &lt;- compute(net_eff, testdata.norm[,2]) #net.results &lt;- compute(net_eff, dataTest.norm) # With normalized data #Lets see what properties net.sqrt has #ls(net.results) #Lets see the results #print(net.results$net.result) #Lets display a better version of the results #cleanoutput &lt;- cbind(testdata.norm[,2],actualEffort, # as.data.frame(net.results$net.result)) #colnames(cleanoutput) &lt;- c(&quot;Input&quot;,&quot;Expected Output&quot;,&quot;Neural Net Output&quot;) #print(cleanoutput) 19.4 Support Vector Machines SVM 19.5 Ensembles Ensembles or meta-learners combine multiple models to obtain better predictions i.e., this technique consists in combining single classifiers (sometimes are also called weak classifiers). A problem with ensembles is that their models are difficult to interpret (they behave as blackboxes) in comparison to decision trees or rules which provide an explanation of their decision making process. They are typically classified as Bagging, Boosting and Stacking (Stacked generalization). 19.5.1 Bagging Bagging (also known as Bootstrap aggregating) is an ensemble technique in which a base learner is applied to multiple equal size datasets created from the original data using bootstraping. Predictions are based on voting of the individual predictions. An advantage of bagging is that it does not require any modification to the learning algorithm and takes advantage of the instability of the base classifier to create diversity among individual ensembles so that individual members of the ensemble perform well in different regions of the data. Bagging does not perform well with classifiers if their output is robust to perturbation of the data such as nearest-neighbour (NN) classifiers. 19.5.2 Boosting Boosting techniques generate multiple models that complement each other inducing models that improve regions of the data where previous induced models preformed poorly. This is achieved by increasing the weights of instances wrongly classified, so new learners focus on those instances. Finally, classification is based on a weighted voted among all members of the ensemble. In particular, AdaBoost.M1 [15] is a popular boosting algorithm for classification. The set of training examples is assigned an equal weight at the beginning and the weight of instances is either increased or decreased depending on whether the learner classified that instance incorrectly or not. The following iterations focus on those instances with higher weights. AdaBoost.M1 can be applied to any base learner. 19.5.3 Rotation Forests Rotation Forests [40] combine randomly chosen subsets of attributes (random subspaces) and bagging approaches with principal components feature generation to construct an ensemble of decision trees. Principal Component Analysis is used as a feature selection technique combining subsets of attributes which are used with a bootstrapped subset of the training data by the base classifier. 19.5.4 Boosting in R In R, there are three packages to deal with Boosting: gmb, ada and the mboost packages. An example of gbm using the caret package. # load libraries library(caret) library(pROC) ################################################# # model it ################################################# # Get names of caret supported models (just a few - head) head(names(getModelInfo())) ## [1] &quot;ada&quot; &quot;AdaBag&quot; &quot;AdaBoost.M1&quot; &quot;adaboost&quot; &quot;amdai&quot; ## [6] &quot;ANFIS&quot; # Show model info and find out what type of model it is getModelInfo()$gbm$tags ## [1] &quot;Tree-Based Model&quot; &quot;Boosting&quot; ## [3] &quot;Ensemble Model&quot; &quot;Implicit Feature Selection&quot; ## [5] &quot;Accepts Case Weights&quot; getModelInfo()$gbm$type ## [1] &quot;Regression&quot; &quot;Classification&quot; library(foreign) library(caret) library(pROC) kc1 &lt;- read.arff(&quot;./datasets/defectPred/D1/KC1.arff&quot;) # Split data into training and test datasets # TODO: Improve this with createDataParticion from Caret set.seed(1234) ind &lt;- sample(2, nrow(kc1), replace = TRUE, prob = c(0.7, 0.3)) kc1.train &lt;- kc1[ind==1, ] kc1.test &lt;- kc1[ind==2, ] # create caret trainControl object to control the number of cross-validations performed objControl &lt;- trainControl(method=&#39;cv&#39;, number=3, returnResamp=&#39;none&#39;, summaryFunction = twoClassSummary, classProbs = TRUE) # run model objModel &lt;- train(Defective ~ ., data = kc1.train, method = &#39;gbm&#39;, trControl = objControl, metric = &quot;ROC&quot; #, #preProc = c(&quot;center&quot;, &quot;scale&quot;) ) ## Loading required package: gbm ## Loading required package: splines ## Loaded gbm 2.1.1 ## Loading required package: plyr ## ------------------------------------------------------------------------- ## You have loaded plyr after dplyr - this is likely to cause problems. ## If you need functions from both plyr and dplyr, please load plyr first, then dplyr: ## library(plyr); library(dplyr) ## ------------------------------------------------------------------------- ## ## Attaching package: &#39;plyr&#39; ## The following objects are masked from &#39;package:reshape&#39;: ## ## rename, round_any ## The following object is masked from &#39;package:modeltools&#39;: ## ## empty ## The following objects are masked from &#39;package:Hmisc&#39;: ## ## is.discrete, summarize ## The following object is masked from &#39;package:lubridate&#39;: ## ## here ## The following objects are masked from &#39;package:dplyr&#39;: ## ## arrange, count, desc, failwith, id, mutate, rename, summarise, ## summarize ## The following object is masked from &#39;package:DMwR&#39;: ## ## join ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 0.8352 -nan 0.1000 0.0136 ## 2 0.8085 -nan 0.1000 0.0106 ## 3 0.7894 -nan 0.1000 0.0084 ## 4 0.7738 -nan 0.1000 0.0074 ## 5 0.7556 -nan 0.1000 0.0072 ## 6 0.7466 -nan 0.1000 0.0029 ## 7 0.7368 -nan 0.1000 0.0052 ## 8 0.7279 -nan 0.1000 0.0046 ## 9 0.7174 -nan 0.1000 0.0037 ## 10 0.7132 -nan 0.1000 0.0014 ## 20 0.6716 -nan 0.1000 0.0000 ## 40 0.6480 -nan 0.1000 -0.0004 ## 60 0.6366 -nan 0.1000 -0.0001 ## 80 0.6229 -nan 0.1000 -0.0003 ## 100 0.6118 -nan 0.1000 -0.0003 ## 120 0.6050 -nan 0.1000 -0.0005 ## 140 0.5969 -nan 0.1000 -0.0003 ## 150 0.5925 -nan 0.1000 -0.0004 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 0.8270 -nan 0.1000 0.0161 ## 2 0.8060 -nan 0.1000 0.0074 ## 3 0.7785 -nan 0.1000 0.0091 ## 4 0.7557 -nan 0.1000 0.0085 ## 5 0.7423 -nan 0.1000 0.0064 ## 6 0.7331 -nan 0.1000 0.0030 ## 7 0.7223 -nan 0.1000 0.0019 ## 8 0.7115 -nan 0.1000 0.0031 ## 9 0.7009 -nan 0.1000 0.0041 ## 10 0.6909 -nan 0.1000 0.0044 ## 20 0.6449 -nan 0.1000 -0.0002 ## 40 0.6005 -nan 0.1000 -0.0008 ## 60 0.5696 -nan 0.1000 -0.0002 ## 80 0.5462 -nan 0.1000 -0.0002 ## 100 0.5274 -nan 0.1000 -0.0008 ## 120 0.5110 -nan 0.1000 -0.0012 ## 140 0.4952 -nan 0.1000 -0.0004 ## 150 0.4880 -nan 0.1000 -0.0006 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 0.8316 -nan 0.1000 0.0133 ## 2 0.7987 -nan 0.1000 0.0126 ## 3 0.7686 -nan 0.1000 0.0116 ## 4 0.7471 -nan 0.1000 0.0086 ## 5 0.7319 -nan 0.1000 0.0051 ## 6 0.7163 -nan 0.1000 0.0049 ## 7 0.7049 -nan 0.1000 0.0027 ## 8 0.6934 -nan 0.1000 0.0038 ## 9 0.6829 -nan 0.1000 0.0035 ## 10 0.6755 -nan 0.1000 0.0027 ## 20 0.6168 -nan 0.1000 -0.0007 ## 40 0.5707 -nan 0.1000 -0.0018 ## 60 0.5396 -nan 0.1000 -0.0012 ## 80 0.5064 -nan 0.1000 -0.0011 ## 100 0.4836 -nan 0.1000 -0.0009 ## 120 0.4574 -nan 0.1000 -0.0003 ## 140 0.4343 -nan 0.1000 -0.0011 ## 150 0.4248 -nan 0.1000 -0.0004 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 0.8399 -nan 0.1000 0.0110 ## 2 0.8180 -nan 0.1000 0.0107 ## 3 0.8033 -nan 0.1000 0.0065 ## 4 0.7887 -nan 0.1000 0.0073 ## 5 0.7762 -nan 0.1000 0.0068 ## 6 0.7630 -nan 0.1000 0.0048 ## 7 0.7571 -nan 0.1000 0.0022 ## 8 0.7497 -nan 0.1000 0.0026 ## 9 0.7403 -nan 0.1000 0.0041 ## 10 0.7344 -nan 0.1000 0.0022 ## 20 0.7032 -nan 0.1000 0.0004 ## 40 0.6799 -nan 0.1000 -0.0002 ## 60 0.6617 -nan 0.1000 -0.0005 ## 80 0.6481 -nan 0.1000 -0.0007 ## 100 0.6392 -nan 0.1000 -0.0006 ## 120 0.6302 -nan 0.1000 -0.0001 ## 140 0.6217 -nan 0.1000 -0.0007 ## 150 0.6171 -nan 0.1000 -0.0004 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 0.8328 -nan 0.1000 0.0172 ## 2 0.8082 -nan 0.1000 0.0096 ## 3 0.7898 -nan 0.1000 0.0087 ## 4 0.7758 -nan 0.1000 0.0035 ## 5 0.7619 -nan 0.1000 0.0041 ## 6 0.7492 -nan 0.1000 0.0049 ## 7 0.7379 -nan 0.1000 0.0048 ## 8 0.7311 -nan 0.1000 0.0011 ## 9 0.7197 -nan 0.1000 0.0035 ## 10 0.7160 -nan 0.1000 0.0007 ## 20 0.6746 -nan 0.1000 -0.0001 ## 40 0.6312 -nan 0.1000 0.0003 ## 60 0.6019 -nan 0.1000 -0.0012 ## 80 0.5818 -nan 0.1000 -0.0006 ## 100 0.5623 -nan 0.1000 -0.0010 ## 120 0.5455 -nan 0.1000 -0.0012 ## 140 0.5317 -nan 0.1000 -0.0005 ## 150 0.5227 -nan 0.1000 -0.0003 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 0.8283 -nan 0.1000 0.0109 ## 2 0.8016 -nan 0.1000 0.0122 ## 3 0.7791 -nan 0.1000 0.0075 ## 4 0.7607 -nan 0.1000 0.0072 ## 5 0.7470 -nan 0.1000 0.0035 ## 6 0.7352 -nan 0.1000 0.0054 ## 7 0.7199 -nan 0.1000 0.0054 ## 8 0.7111 -nan 0.1000 0.0030 ## 9 0.7015 -nan 0.1000 0.0023 ## 10 0.6925 -nan 0.1000 0.0016 ## 20 0.6347 -nan 0.1000 0.0002 ## 40 0.5882 -nan 0.1000 -0.0019 ## 60 0.5546 -nan 0.1000 -0.0014 ## 80 0.5269 -nan 0.1000 -0.0007 ## 100 0.5025 -nan 0.1000 -0.0021 ## 120 0.4782 -nan 0.1000 -0.0008 ## 140 0.4555 -nan 0.1000 -0.0007 ## 150 0.4442 -nan 0.1000 -0.0016 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 0.8443 -nan 0.1000 0.0112 ## 2 0.8248 -nan 0.1000 0.0098 ## 3 0.8068 -nan 0.1000 0.0075 ## 4 0.7962 -nan 0.1000 0.0042 ## 5 0.7816 -nan 0.1000 0.0051 ## 6 0.7711 -nan 0.1000 0.0044 ## 7 0.7597 -nan 0.1000 0.0024 ## 8 0.7528 -nan 0.1000 0.0028 ## 9 0.7464 -nan 0.1000 0.0020 ## 10 0.7384 -nan 0.1000 0.0036 ## 20 0.7041 -nan 0.1000 -0.0007 ## 40 0.6821 -nan 0.1000 -0.0012 ## 60 0.6624 -nan 0.1000 -0.0006 ## 80 0.6505 -nan 0.1000 -0.0003 ## 100 0.6412 -nan 0.1000 -0.0003 ## 120 0.6333 -nan 0.1000 -0.0004 ## 140 0.6242 -nan 0.1000 -0.0004 ## 150 0.6206 -nan 0.1000 -0.0009 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 0.8370 -nan 0.1000 0.0144 ## 2 0.8118 -nan 0.1000 0.0099 ## 3 0.7922 -nan 0.1000 0.0086 ## 4 0.7748 -nan 0.1000 0.0081 ## 5 0.7630 -nan 0.1000 0.0044 ## 6 0.7521 -nan 0.1000 0.0037 ## 7 0.7405 -nan 0.1000 0.0028 ## 8 0.7317 -nan 0.1000 0.0010 ## 9 0.7265 -nan 0.1000 0.0009 ## 10 0.7196 -nan 0.1000 0.0019 ## 20 0.6801 -nan 0.1000 -0.0000 ## 40 0.6347 -nan 0.1000 -0.0006 ## 60 0.6025 -nan 0.1000 -0.0004 ## 80 0.5789 -nan 0.1000 -0.0007 ## 100 0.5579 -nan 0.1000 -0.0015 ## 120 0.5428 -nan 0.1000 -0.0005 ## 140 0.5289 -nan 0.1000 -0.0011 ## 150 0.5200 -nan 0.1000 -0.0003 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 0.8322 -nan 0.1000 0.0125 ## 2 0.8043 -nan 0.1000 0.0118 ## 3 0.7784 -nan 0.1000 0.0087 ## 4 0.7629 -nan 0.1000 0.0043 ## 5 0.7465 -nan 0.1000 0.0065 ## 6 0.7326 -nan 0.1000 0.0045 ## 7 0.7215 -nan 0.1000 0.0017 ## 8 0.7126 -nan 0.1000 0.0013 ## 9 0.7047 -nan 0.1000 -0.0010 ## 10 0.6936 -nan 0.1000 0.0031 ## 20 0.6480 -nan 0.1000 -0.0008 ## 40 0.5896 -nan 0.1000 -0.0005 ## 60 0.5552 -nan 0.1000 -0.0007 ## 80 0.5259 -nan 0.1000 -0.0005 ## 100 0.4969 -nan 0.1000 -0.0007 ## 120 0.4730 -nan 0.1000 -0.0008 ## 140 0.4522 -nan 0.1000 -0.0003 ## 150 0.4448 -nan 0.1000 -0.0009 ## ## Iter TrainDeviance ValidDeviance StepSize Improve ## 1 0.8364 -nan 0.1000 0.0147 ## 2 0.8103 -nan 0.1000 0.0136 ## 3 0.7938 -nan 0.1000 0.0081 ## 4 0.7757 -nan 0.1000 0.0085 ## 5 0.7617 -nan 0.1000 0.0071 ## 6 0.7511 -nan 0.1000 0.0046 ## 7 0.7415 -nan 0.1000 0.0032 ## 8 0.7295 -nan 0.1000 0.0037 ## 9 0.7198 -nan 0.1000 0.0031 ## 10 0.7131 -nan 0.1000 0.0028 ## 20 0.6773 -nan 0.1000 -0.0009 ## 40 0.6413 -nan 0.1000 -0.0006 ## 60 0.6180 -nan 0.1000 -0.0007 ## 80 0.6015 -nan 0.1000 -0.0008 ## 100 0.5866 -nan 0.1000 -0.0002 ## 120 0.5695 -nan 0.1000 -0.0011 ## 140 0.5569 -nan 0.1000 -0.0005 ## 150 0.5518 -nan 0.1000 -0.0006 # Find out variable importance summary(objModel) ## var rel.inf ## HALSTEAD_CONTENT HALSTEAD_CONTENT 14.6047519607 ## HALSTEAD_DIFFICULTY HALSTEAD_DIFFICULTY 10.1873906874 ## NUM_OPERANDS NUM_OPERANDS 8.9325104200 ## NUM_OPERATORS NUM_OPERATORS 7.9127746116 ## NUM_UNIQUE_OPERATORS NUM_UNIQUE_OPERATORS 6.7756582333 ## LOC_TOTAL LOC_TOTAL 6.2296682519 ## NUM_UNIQUE_OPERANDS NUM_UNIQUE_OPERANDS 6.1410664847 ## HALSTEAD_EFFORT HALSTEAD_EFFORT 5.1853100309 ## LOC_COMMENTS LOC_COMMENTS 5.1306009233 ## LOC_EXECUTABLE LOC_EXECUTABLE 4.7274594398 ## LOC_CODE_AND_COMMENT LOC_CODE_AND_COMMENT 3.8598106671 ## HALSTEAD_LENGTH HALSTEAD_LENGTH 3.7668756935 ## HALSTEAD_VOLUME HALSTEAD_VOLUME 3.4277318907 ## ESSENTIAL_COMPLEXITY ESSENTIAL_COMPLEXITY 3.4005292639 ## BRANCH_COUNT BRANCH_COUNT 3.2023641850 ## DESIGN_COMPLEXITY DESIGN_COMPLEXITY 1.9630713543 ## LOC_BLANK LOC_BLANK 1.9252046668 ## HALSTEAD_LEVEL HALSTEAD_LEVEL 1.1178129374 ## CYCLOMATIC_COMPLEXITY CYCLOMATIC_COMPLEXITY 1.0945057892 ## HALSTEAD_ERROR_EST HALSTEAD_ERROR_EST 0.4149025082 ## HALSTEAD_PROG_TIME HALSTEAD_PROG_TIME 0.0000000000 # find out model details objModel ## Stochastic Gradient Boosting ## ## 1500 samples ## 21 predictors ## 2 classes: &#39;N&#39;, &#39;Y&#39; ## ## No pre-processing ## Resampling: Cross-Validated (3 fold) ## Summary of sample sizes: 1000, 1000, 1000 ## Resampling results across tuning parameters: ## ## interaction.depth n.trees ROC Sens Spec ## 1 50 0.8054441609 0.9826224329 0.1282051282 ## 1 100 0.8090442338 0.9802527646 0.1581196581 ## 1 150 0.8050492162 0.9755134281 0.1794871795 ## 2 50 0.8068619111 0.9739336493 0.1623931624 ## 2 100 0.8085834650 0.9763033175 0.2051282051 ## 2 150 0.8108518654 0.9684044234 0.2307692308 ## 3 50 0.8042340098 0.9691943128 0.1837606838 ## 3 100 0.8054694779 0.9668246445 0.2094017094 ## 3 150 0.8047859197 0.9636650869 0.2478632479 ## ## Tuning parameter &#39;shrinkage&#39; was held constant at a value of 0.1 ## ## Tuning parameter &#39;n.minobsinnode&#39; was held constant at a value of 10 ## ROC was used to select the optimal model using the largest value. ## The final values used for the model were n.trees = 150, ## interaction.depth = 2, shrinkage = 0.1 and n.minobsinnode = 10. ################################################# # evalutate model ################################################# # get predictions on your testing data # class prediction predictions &lt;- predict(object=objModel, kc1.test[,-22], type=&#39;raw&#39;) head(predictions) ## [1] N N N N N N ## Levels: N Y postResample(pred=predictions, obs=as.factor(kc1.test[,22])) ## Accuracy Kappa ## 0.8691275168 0.2982095951 # probabilities predictions &lt;- predict(object=objModel, kc1.test[,-22], type=&#39;prob&#39;) head(predictions) ## N Y ## 1 0.9135315703 0.08646842967 ## 2 0.9756990212 0.02430097876 ## 3 0.8465454841 0.15345451590 ## 4 0.8366086073 0.16339139270 ## 5 0.8333283490 0.16667165098 ## 6 0.9344452820 0.06555471799 postResample(pred=predictions[[2]], obs=ifelse(kc1.test[,22]==&#39;yes&#39;,1,0)) ## RMSE Rsquared ## 0.2143954918 NA auc &lt;- roc(ifelse(kc1.test[,22]==&quot;Y&quot;,1,0), predictions[[2]]) print(auc$auc) ## Area under the curve: 0.8049179 "],
["further-classification-models.html", "Chapter 20 Further Classification Models 20.1 Multilabel classification 20.2 Semi-supervised Learning", " Chapter 20 Further Classification Models 20.1 Multilabel classification Some datasets, for example, reviews of applications and mobile applications repositories such as App Store or Google play contain reviews that can have several labels at the same time (e.g. bugs, feature requests, etc.) 20.2 Semi-supervised Learning Self train a model on semi-supervised data http://www.inside-r.org/packages/cran/dmwr/docs/SelfTrain library(DMwR) ## Small example with the Iris classification data set data(iris) ## Dividing the data set into train and test sets idx &lt;- sample(150,100) tr &lt;- iris[idx,] ts &lt;- iris[-idx,] ## Learn a tree with the full train set and test it stdTree &lt;- rpartXse(Species~ .,tr,se=0.5) table(predict(stdTree,ts,type=&#39;class&#39;),ts$Species) ## ## setosa versicolor virginica ## setosa 13 0 0 ## versicolor 0 17 1 ## virginica 0 1 18 ## Now let us create another training set with most of the target ## variable values unknown trSelfT &lt;- tr nas &lt;- sample(100,70) trSelfT[nas,&#39;Species&#39;] &lt;- NA ## Learn a tree using only the labelled cases and test it baseTree &lt;- rpartXse(Species~ .,trSelfT[-nas,],se=0.5) table(predict(baseTree,ts,type=&#39;class&#39;),ts$Species) ## ## setosa versicolor virginica ## setosa 13 0 0 ## versicolor 0 13 0 ## virginica 0 5 19 ## The user-defined function that will be used in the self-training process f &lt;- function(m,d) { l &lt;- predict(m,d,type=&#39;class&#39;) c &lt;- apply(predict(m,d),1,max) data.frame(cl=l,p=c) } ## Self train the same model using the semi-superside data and test the ## resulting model treeSelfT &lt;- SelfTrain(Species~ .,trSelfT,learner(&#39;rpartXse&#39;,list(se=0.5)),&#39;f&#39;) table(predict(treeSelfT,ts,type=&#39;class&#39;),ts$Species) ## ## setosa versicolor virginica ## setosa 13 0 0 ## versicolor 0 13 0 ## virginica 0 5 19 "],
["social-network-analysis-in-se.html", "Chapter 21 Social Network Analysis in SE", " Chapter 21 Social Network Analysis in SE In this example, we will data from the MSR14 challenge. Further information and datasets: http://openscience.us/repo/msr/msr14.html Similar databases can be obtained using MetricsGrimoire or other tools. In this simple example, we create a network form the users and following extracted from GitHub and stored in a MySQL database. We can read a file directely from MySQL dump library(RMySQL) # Connecting to MySQL mydb = dbConnect(MySQL(), user=&#39;msr14&#39;, password=&#39;msr14&#39;, dbname=&#39;msr14&#39;, host=&#39;localhost&#39;) # Retrieving data from MySQL sql &lt;- &quot;select user_id, follower_id from followers limit 100;&quot; rs = dbSendQuery(mydb, sql) data &lt;- fetch(rs, n=-1) Alternatively, we can create e CSV file directly from MySQL and load it $mysql -u msr14 -pmsr14 msr14 &gt; SELECT &#39;user&#39;,&#39;follower&#39; UNION ALL SELECT user_id,follower_id FROM followers LIMIT 1000 INTO OUTFILE &quot;/tmp/followers.csv&quot; FIELDS TERMINATED BY &#39;,&#39; LINES TERMINATED BY &#39;\\n&#39;; # Data already extracted and stored as CSV file (for demo purposes) dat = read.csv(&quot;./datasets/sna/followers.csv&quot;, header = FALSE, sep = &quot;,&quot;) dat &lt;- head(dat,100) We can now create the graph library(igraph) ## ## Attaching package: &#39;igraph&#39; ## The following objects are masked from &#39;package:rgp&#39;: ## ## %-&gt;%, normalize ## The following object is masked from &#39;package:arules&#39;: ## ## union ## The following object is masked from &#39;package:class&#39;: ## ## knn ## The following object is masked from &#39;package:modeltools&#39;: ## ## clusters ## The following objects are masked from &#39;package:lubridate&#39;: ## ## %--%, union ## The following objects are masked from &#39;package:dplyr&#39;: ## ## %&gt;%, as_data_frame, groups, union ## The following objects are masked from &#39;package:stats&#39;: ## ## decompose, spectrum ## The following object is masked from &#39;package:base&#39;: ## ## union # Create a graph g &lt;- graph.data.frame(dat, directed = TRUE) Some values: summary(g); ## IGRAPH DN-- 95 100 -- ## + attr: name (v/c) Plotting the graph: layout1 &lt;- layout.fruchterman.reingold(g) plot(g, layout1) ## Warning in if (axes) {: the condition has length &gt; 1 and only the first ## element will be used Other layout plot(g, layout=layout.kamada.kawai) A tk application can launched to show the plot interactively: plot(g, layout = layout.fruchterman.reingold) Some metrics: metrics &lt;- data.frame( deg = degree(g), bet = betweenness(g), clo = closeness(g), eig = evcent(g)$vector, cor = graph.coreness(g) ) # head(metrics) ## deg bet clo eig cor ## 6183 1 0 0.0001131733816 0.00000000000000001031406355 1 ## 49199 1 0 0.0001131733816 0.00000000000000001547109532 1 ## 71080 1 0 0.0001131733816 0.00000000000000002062812710 1 ## 162983 1 0 0.0001131733816 0.00000000000000002062812710 1 ## 772 3 0 0.0001156336725 0.10408536559473147153909167 2 ## 907 1 0 0.0001131733816 0.00814183210922054742542109 1 To fix and to do: Explain metrics and better graphs library(ggplot2) ggplot( metrics, aes(x=bet, y=eig, label=rownames(metrics), colour=res, size=abs(res)) )+ xlab(&quot;Betweenness Centrality&quot;)+ ylab(&quot;Eigenvector Centrality&quot;)+ geom_text() + theme(title=&quot;Key Actor Analysis&quot;) V(g)$label.cex &lt;- 2.2 * V(g)$degree / max(V(g)$degree)+ .2 V(g)$label.color &lt;- rgb(0, 0, .2, .8) V(g)$frame.color &lt;- NA egam &lt;- (log(E(g)$weight)+.4) / max(log(E(g)$weight)+.4) E(g)$color &lt;- rgb(.5, .5, 0, egam) E(g)$width &lt;- egam # plot the graph in layout1 plot(g, layout=layout1) Further information: http://sna.stanford.edu/lab.php?l=1 "],
["text-mining-software-engineering-data.html", "Chapter 22 Text Mining Software Engineering Data 22.1 Terminology 22.2 Basic tm commands 22.3 Example of classifying bugs from Bugzilla 22.4 Extracting data from Twitter", " Chapter 22 Text Mining Software Engineering Data In software engineering, there is a lot of information in plain text such as requirements, bug reports, mails, reviews from applicatons, etc. Typically that information can be extracted from Software Configuration Management Systems (SCM), Bug Tracking Systems (BTS) such as Bugzilla or application stores such as Google Play or Apple’s AppStore, etc. can be mined to extract relevant information. Here we briefly explain the text mining process and how this can be done with R. The main package for text mining is tm (Feinerer and Hornik 2015)(Feinerer, Hornik, and Meyer 2008). Another popular package is wordcloud. # - Install packages # pckgs_needed &lt;- c(&quot;tm&quot;, &quot;wordcloud&quot;) # install.packages(pckgs_needed, dependencies = TRUE) 22.1 Terminology We need to consider: * Importing data. A Corpus is a collection of text documents, implemented as VCorpus (corpora are R object held in memory). The tm provides several corpus constructors: DirSource, VectorSource, or DataframeSource (getSources()). readerControl of the corpus constructor has to be a list with the named components reader and language * Preprocessing * Inspecting and exploring data: Individual documents can be accessed via [[ * Transformations: Transformations are done via the tm_map() function. ++ tm_map(_____, stripWhitespace) ++ tm_map(_____, content_transformer(tolower)) ++ tm_map(_____, removeWords, stopwords(“english”)) ++ tm_map(reuters, stemDocument) * Creating Term-Document Matrices: TermDocumentMatrix and DocumentTermMatrix ++ * Relationships between terms. ++ findFreqTerms(_____, anumber) ++ findAssocs(Mydtm, “aterm”, anumbercorrelation) ++ A dictionary is a (multi-)set of strings. It is often used to denote relevant terms in text mining. ++ inspect(DocumentTermMatrix( newsreuters, list(dictionary = c(“term1”, “term2”, “term3”)))). It displays detailed information on a corpus or a term-document matrix. * Clustering 22.2 Basic tm commands For example, load a tm dataset 20 news articles with additional meta information from the Reuters-21578 dataset library(&#39;tm&#39;) ## Loading required package: NLP ## ## Attaching package: &#39;NLP&#39; ## The following object is masked from &#39;package:ggplot2&#39;: ## ## annotate ## ## Attaching package: &#39;tm&#39; ## The following object is masked from &#39;package:arules&#39;: ## ## inspect data(&quot;crude&quot;) crude ## &lt;&lt;VCorpus&gt;&gt; ## Metadata: corpus specific: 0, document level (indexed): 0 ## Content: documents: 20 crude[[2]]$content # returns the text content of the second news article ## [1] &quot;OPEC may be forced to meet before a\\nscheduled June session to readdress its production cutting\\nagreement if the organization wants to halt the current slide\\nin oil prices, oil industry analysts said.\\n \\&quot;The movement to higher oil prices was never to be as easy\\nas OPEC thought. They may need an emergency meeting to sort out\\nthe problems,\\&quot; said Daniel Yergin, director of Cambridge Energy\\nResearch Associates, CERA.\\n Analysts and oil industry sources said the problem OPEC\\nfaces is excess oil supply in world oil markets.\\n \\&quot;OPEC&#39;s problem is not a price problem but a production\\nissue and must be addressed in that way,\\&quot; said Paul Mlotok, oil\\nanalyst with Salomon Brothers Inc.\\n He said the market&#39;s earlier optimism about OPEC and its\\nability to keep production under control have given way to a\\npessimistic outlook that the organization must address soon if\\nit wishes to regain the initiative in oil prices.\\n But some other analysts were uncertain that even an\\nemergency meeting would address the problem of OPEC production\\nabove the 15.8 mln bpd quota set last December.\\n \\&quot;OPEC has to learn that in a buyers market you cannot have\\ndeemed quotas, fixed prices and set differentials,\\&quot; said the\\nregional manager for one of the major oil companies who spoke\\non condition that he not be named. \\&quot;The market is now trying to\\nteach them that lesson again,\\&quot; he added.\\n David T. Mizrahi, editor of Mideast reports, expects OPEC\\nto meet before June, although not immediately. However, he is\\nnot optimistic that OPEC can address its principal problems.\\n \\&quot;They will not meet now as they try to take advantage of the\\nwinter demand to sell their oil, but in late March and April\\nwhen demand slackens,\\&quot; Mizrahi said.\\n But Mizrahi said that OPEC is unlikely to do anything more\\nthan reiterate its agreement to keep output at 15.8 mln bpd.\\&quot;\\n Analysts said that the next two months will be critical for\\nOPEC&#39;s ability to hold together prices and output.\\n \\&quot;OPEC must hold to its pact for the next six to eight weeks\\nsince buyers will come back into the market then,\\&quot; said Dillard\\nSpriggs of Petroleum Analysis Ltd in New York.\\n But Bijan Moussavar-Rahmani of Harvard University&#39;s Energy\\nand Environment Policy Center said that the demand for OPEC oil\\nhas been rising through the first quarter and this may have\\nprompted excesses in its production.\\n \\&quot;Demand for their (OPEC) oil is clearly above 15.8 mln bpd\\nand is probably closer to 17 mln bpd or higher now so what we\\nare seeing characterized as cheating is OPEC meeting this\\ndemand through current production,\\&quot; he told Reuters in a\\ntelephone interview.\\n Reuter&quot; Create a data frame source: # Create a vector source. docs &lt;- c(&quot;This is a text.&quot;, &quot;This another one.&quot;) (vs &lt;- VectorSource(docs)) ## $encoding ## [1] &quot;&quot; ## ## $length ## [1] 2 ## ## $position ## [1] 0 ## ## $reader ## function (elem, language, id) ## { ## if (!is.null(elem$uri)) ## id &lt;- basename(elem$uri) ## PlainTextDocument(elem$content, id = id, language = language) ## } ## &lt;environment: namespace:tm&gt; ## ## $content ## [1] &quot;This is a text.&quot; &quot;This another one.&quot; ## ## attr(,&quot;class&quot;) ## [1] &quot;VectorSource&quot; &quot;SimpleSource&quot; &quot;Source&quot; inspect(VCorpus(vs)) ## &lt;&lt;VCorpus&gt;&gt; ## Metadata: corpus specific: 0, document level (indexed): 0 ## Content: documents: 2 ## ## [[1]] ## &lt;&lt;PlainTextDocument&gt;&gt; ## Metadata: 7 ## Content: chars: 15 ## ## [[2]] ## &lt;&lt;PlainTextDocument&gt;&gt; ## Metadata: 7 ## Content: chars: 17 # create a data frame source docs &lt;- data.frame(c(&quot;This is a text.&quot;, &quot;This another one.&quot;)) (ds &lt;- DataframeSource(docs)) ## $encoding ## [1] &quot;&quot; ## ## $length ## [1] 2 ## ## $position ## [1] 0 ## ## $reader ## function (elem, language, id) ## { ## if (!is.null(elem$uri)) ## id &lt;- basename(elem$uri) ## PlainTextDocument(elem$content, id = id, language = language) ## } ## &lt;environment: namespace:tm&gt; ## ## $content ## c..This.is.a.text.....This.another.one... ## 1 This is a text. ## 2 This another one. ## ## attr(,&quot;class&quot;) ## [1] &quot;DataframeSource&quot; &quot;SimpleSource&quot; &quot;Source&quot; inspect(VCorpus(ds)) ## &lt;&lt;VCorpus&gt;&gt; ## Metadata: corpus specific: 0, document level (indexed): 0 ## Content: documents: 2 ## ## [[1]] ## &lt;&lt;PlainTextDocument&gt;&gt; ## Metadata: 7 ## Content: chars: 15 ## ## [[2]] ## &lt;&lt;PlainTextDocument&gt;&gt; ## Metadata: 7 ## Content: chars: 17 # accessing documents, terms # For Docs and Terms, a character vector with document IDs and terms, respectively. # For nDocs and nTerms, an integer with the number of document IDs and terms, respectively. tdm &lt;- TermDocumentMatrix(crude)[1:10,1:20] # 10 terms, 20 documents tdm1 &lt;- TermDocumentMatrix(crude) Docs(tdm) ## [1] &quot;127&quot; &quot;144&quot; &quot;191&quot; &quot;194&quot; &quot;211&quot; &quot;236&quot; &quot;237&quot; &quot;242&quot; &quot;246&quot; &quot;248&quot; &quot;273&quot; ## [12] &quot;349&quot; &quot;352&quot; &quot;353&quot; &quot;368&quot; &quot;489&quot; &quot;502&quot; &quot;543&quot; &quot;704&quot; &quot;708&quot; nDocs(tdm) ## [1] 20 nTerms(tdm) ## [1] 10 Terms(tdm) ## [1] &quot;...&quot; &quot;100,000&quot; &quot;10.8&quot; &quot;1.1&quot; &quot;1.11&quot; &quot;1.15&quot; &quot;1.2&quot; ## [8] &quot;12.&quot; &quot;12.217&quot; &quot;12.32&quot; # Inspect, i.e., display detailed information on a corpus or a term-document matrix. inspect(crude[1:3]) ## &lt;&lt;VCorpus&gt;&gt; ## Metadata: corpus specific: 0, document level (indexed): 0 ## Content: documents: 3 ## ## $`reut-00001.xml` ## &lt;&lt;PlainTextDocument&gt;&gt; ## Metadata: 15 ## Content: chars: 527 ## ## $`reut-00002.xml` ## &lt;&lt;PlainTextDocument&gt;&gt; ## Metadata: 15 ## Content: chars: 2634 ## ## $`reut-00004.xml` ## &lt;&lt;PlainTextDocument&gt;&gt; ## Metadata: 15 ## Content: chars: 330 tdm &lt;- TermDocumentMatrix(crude)[1:10, 1:10] inspect(tdm) ## &lt;&lt;TermDocumentMatrix (terms: 10, documents: 10)&gt;&gt; ## Non-/sparse entries: 4/96 ## Sparsity : 96% ## Maximal term length: 7 ## Weighting : term frequency (tf) ## Sample : ## Docs ## Terms 127 144 191 194 211 236 237 242 246 248 ## ... 0 0 0 0 0 0 0 0 0 1 ## 100,000 0 0 0 0 0 0 0 0 0 0 ## 10.8 0 0 0 0 0 0 0 0 0 0 ## 1.1 0 0 0 0 0 0 0 0 0 0 ## 1.11 0 0 0 0 0 0 0 0 0 0 ## 1.15 0 0 0 0 0 0 0 0 0 0 ## 1.2 0 0 0 0 0 1 0 0 0 0 ## 12. 0 0 0 1 0 0 0 0 0 0 ## 12.217 0 0 0 0 0 0 0 0 1 0 ## 12.32 0 0 0 0 0 0 0 0 0 0 # TermDocumentMatrix Constructs or coerces to a term-document matrix or a document-term matrix. data(&quot;crude&quot;) tdmRmPct &lt;- TermDocumentMatrix(crude, control = list(removePunctuation = TRUE, stopwords = TRUE)) dtm &lt;- DocumentTermMatrix(crude, control = list(weighting = function(x) weightTfIdf(x, normalize = FALSE), stopwords = TRUE)) inspect(tdmRmPct[202:205, 1:5]) ## &lt;&lt;TermDocumentMatrix (terms: 4, documents: 5)&gt;&gt; ## Non-/sparse entries: 6/14 ## Sparsity : 70% ## Maximal term length: 9 ## Weighting : term frequency (tf) ## Sample : ## Docs ## Terms 127 144 191 194 211 ## companies 1 1 0 0 0 ## company 1 0 0 1 0 ## companys 0 0 1 0 0 ## compared 0 0 0 0 1 inspect(tdmRmPct[c(&quot;price&quot;, &quot;texas&quot;), c(&quot;127&quot;, &quot;144&quot;, &quot;191&quot;, &quot;194&quot;)]) ## &lt;&lt;TermDocumentMatrix (terms: 2, documents: 4)&gt;&gt; ## Non-/sparse entries: 6/2 ## Sparsity : 25% ## Maximal term length: 5 ## Weighting : term frequency (tf) ## Sample : ## Docs ## Terms 127 144 191 194 ## price 2 1 2 2 ## texas 1 0 0 2 inspect(dtm[1:5, 273:276]) ## &lt;&lt;DocumentTermMatrix (documents: 5, terms: 4)&gt;&gt; ## Non-/sparse entries: 0/20 ## Sparsity : 100% ## Maximal term length: 9 ## Weighting : term frequency - inverse document frequency (tf-idf) ## Sample : ## Terms ## Docs council counter countries country ## 127 0 0 0 0 ## 144 0 0 0 0 ## 191 0 0 0 0 ## 194 0 0 0 0 ## 211 0 0 0 0 #10 terms tdmRmPct10 &lt;- TermDocumentMatrix(crude, control = list(removePunctuation = TRUE, stopwords = TRUE))[1:10, 1:10] Docs(tdmRmPct10) ## [1] &quot;127&quot; &quot;144&quot; &quot;191&quot; &quot;194&quot; &quot;211&quot; &quot;236&quot; &quot;237&quot; &quot;242&quot; &quot;246&quot; &quot;248&quot; nDocs(tdmRmPct10) ## [1] 10 nTerms(tdmRmPct10) ## [1] 10 Terms(tdmRmPct10) ## [1] &quot;100000&quot; &quot;108&quot; &quot;111&quot; &quot;115&quot; &quot;12217&quot; &quot;1232&quot; ## [7] &quot;1381&quot; &quot;13member&quot; &quot;13nation&quot; &quot;150&quot; # Visualize correlations between terms of a term-document matrix. Visualization requires that package Rgraphviz is available # here we remove punctuation, numbers, stopwords tdm &lt;- TermDocumentMatrix(crude, control = list(removePunctuation = TRUE, removeNumbers = TRUE, stopwords = TRUE)) plot(tdm, corThreshold = 0.6, weighting = TRUE) # Find associations in a document-term or term-document matrix. tdm &lt;- TermDocumentMatrix(crude) findAssocs(tdm, c(&quot;oil&quot;, &quot;opec&quot;, &quot;xyz&quot;), c(0.7, 0.75, 0.1)) ## $oil ## 15.8 clearly late trying who winter analysts ## 0.87 0.80 0.80 0.80 0.80 0.80 0.79 ## said meeting above emergency market fixed that ## 0.78 0.77 0.76 0.75 0.75 0.73 0.73 ## prices agreement buyers ## 0.72 0.71 0.70 ## ## $opec ## meeting emergency 15.8 analysts buyers above said ## 0.88 0.87 0.85 0.85 0.83 0.82 0.82 ## ability they prices. agreement ## 0.80 0.80 0.79 0.76 ## ## $xyz ## numeric(0) # Find frequent terms in a document-term or term-document matrix findFreqTerms(tdm, 6, 8) ## [1] &quot;15.8&quot; &quot;above&quot; &quot;accord&quot; &quot;after&quot; ## [5] &quot;ali&quot; &quot;analysts&quot; &quot;barrel.&quot; &quot;barrels&quot; ## [9] &quot;been&quot; &quot;before&quot; &quot;below&quot; &quot;billion&quot; ## [13] &quot;budget&quot; &quot;demand&quot; &quot;economic&quot; &quot;emergency&quot; ## [17] &quot;energy&quot; &quot;exchange&quot; &quot;futures&quot; &quot;group&quot; ## [21] &quot;gulf&quot; &quot;help&quot; &quot;hold&quot; &quot;industry&quot; ## [25] &quot;international&quot; &quot;january&quot; &quot;meeting&quot; &quot;minister&quot; ## [29] &quot;month&quot; &quot;nazer&quot; &quot;nymex&quot; &quot;output&quot; ## [33] &quot;over&quot; &quot;posted&quot; &quot;present&quot; &quot;prices,&quot; ## [37] &quot;prices.&quot; &quot;recent&quot; &quot;report&quot; &quot;sell&quot; ## [41] &quot;such&quot; &quot;than&quot; &quot;their&quot; &quot;there&quot; ## [45] &quot;this&quot; &quot;traders&quot; &quot;when&quot; # termFreq Generate a term frequency vector from a text document data(&quot;crude&quot;) tmfq1 &lt;- termFreq(crude[[14]]) tmfq1 ## ## 13-nation 948,000 above after ali ## 1 1 1 1 1 ## al-khalifa al-qabas al-sabah also analysts ## 1 1 1 1 1 ## and asked barrels bpd (bpd). ## 1 1 1 1 1 ## crude daily denied emergency estimated ## 2 2 1 1 1 ## fell for has international interview ## 1 2 1 1 1 ## its kuwait kuwait&#39;s last limits. ## 2 1 1 1 1 ## local meeting meeting.&quot; members million ## 1 1 1 1 1 ## minister newspaper &quot;none oil one ## 1 1 1 4 1 ## opec over plans prices prices. ## 4 1 1 1 1 ## pumping quota quoted recent reuter ## 2 1 1 1 1 ## said saying self-imposed sharply sheikh ## 1 1 1 1 1 ## such that the there traders ## 1 3 4 1 1 ## was weakness week were world ## 3 1 1 1 1 str(tmfq1) ## int [1:65(1d)] 1 1 1 1 1 1 1 1 1 1 ... ## - attr(*, &quot;dimnames&quot;)=List of 1 ## ..$ : chr [1:65] &quot;13-nation&quot; &quot;948,000&quot; &quot;above&quot; &quot;after&quot; ... ## - attr(*, &quot;class&quot;)= chr [1:2] &quot;term_frequency&quot; &quot;table&quot; strsplit_space_tokenizer &lt;- function(x) unlist(strsplit(as.character(x), &quot;[[:space:]]+&quot;)) ctrl &lt;- list(tokenize = strsplit_space_tokenizer, removePunctuation = list(preserve_intra_word_dashes = TRUE), stopwords = c(&quot;reuter&quot;, &quot;that&quot;), stemming = TRUE, wordLengths = c(4, Inf)) tmfq2 &lt;- termFreq(crude[[14]], control = ctrl) str(tmfq2) ## int [1:47(1d)] 1 1 1 1 1 1 1 1 1 1 ... ## - attr(*, &quot;dimnames&quot;)=List of 1 ## ..$ : chr [1:47] &quot;13-nation&quot; &quot;948000&quot; &quot;abov&quot; &quot;after&quot; ... ## - attr(*, &quot;class&quot;)= chr [1:2] &quot;term_frequency&quot; &quot;table&quot; #weights # weightBin(m) Weight Binary Binary weight a term-document matrix. # WeightFunction Construct a weighting function for term-document matrices. weightCutBin &lt;- WeightFunction(function(m, cutoff) m &gt; cutoff, &quot;binary with cutoff&quot;, &quot;bincut&quot;) # weightSMART Weight a term-document matrix according to a combination of weights specified in SMART notation. TermDocumentMatrix(crude, control = list(removePunctuation = TRUE, stopwords = TRUE, weighting = function(x) weightSMART(x, spec = &quot;ntc&quot;))) ## &lt;&lt;TermDocumentMatrix (terms: 1000, documents: 20)&gt;&gt; ## Non-/sparse entries: 1678/18322 ## Sparsity : 92% ## Maximal term length: 16 ## Weighting : SMART ntc (SMART) # weightTf Weight by Term Frequency weightTf(m) # weightTfIdf Weight by Term Frequency - Inverse Document Frequency # Transformations # Remove Numbers from a Text Document crude[[1]]$content ## [1] &quot;Diamond Shamrock Corp said that\\neffective today it had cut its contract prices for crude oil by\\n1.50 dlrs a barrel.\\n The reduction brings its posted price for West Texas\\nIntermediate to 16.00 dlrs a barrel, the copany said.\\n \\&quot;The price reduction today was made in the light of falling\\noil product prices and a weak crude oil market,\\&quot; a company\\nspokeswoman said.\\n Diamond is the latest in a line of U.S. oil companies that\\nhave cut its contract, or posted, prices over the last two days\\nciting weak oil markets.\\n Reuter&quot; crude1 &lt;- removeNumbers(crude[[1]]) crude1 ## &lt;&lt;PlainTextDocument&gt;&gt; ## Metadata: 15 ## Content: chars: 520 # Remove Punctuation Marks from a Text Document crude[[14]]$content ## [1] &quot;Kuwait&#39;s oil minister said in a newspaper\\ninterview that there were no plans for an emergency OPEC\\nmeeting after the recent weakness in world oil prices.\\n Sheikh Ali al-Khalifa al-Sabah was quoted by the local\\ndaily al-Qabas as saying that \\&quot;none of the OPEC members has\\nasked for such a meeting.\\&quot;\\n He also denied that Kuwait was pumping above its OPEC quota\\nof 948,000 barrels of crude daily (bpd).\\n Crude oil prices fell sharply last week as international\\noil traders and analysts estimated the 13-nation OPEC was\\npumping up to one million bpd over its self-imposed limits.\\n Reuter&quot; cruderem &lt;- removePunctuation(crude[[14]]) cruderem ## &lt;&lt;PlainTextDocument&gt;&gt; ## Metadata: 15 ## Content: chars: 576 cruderem1 &lt;- removePunctuation(crude[[14]], preserve_intra_word_dashes = TRUE) cruderem1$content ## [1] &quot;Kuwaits oil minister said in a newspaper\\ninterview that there were no plans for an emergency OPEC\\nmeeting after the recent weakness in world oil prices\\n Sheikh Ali al-Khalifa al-Sabah was quoted by the local\\ndaily al-Qabas as saying that none of the OPEC members has\\nasked for such a meeting\\n He also denied that Kuwait was pumping above its OPEC quota\\nof 948000 barrels of crude daily bpd\\n Crude oil prices fell sharply last week as international\\noil traders and analysts estimated the 13-nation OPEC was\\npumping up to one million bpd over its self-imposed limits\\n Reuter&quot; # Remove sparse terms from a document-term or term-document matrix. sparse: A numeric for the maximal allowed sparsity in the range from bigger zero to smaller one tdm &lt;- TermDocumentMatrix(crude) tdm_lssparse &lt;- removeSparseTerms(tdm, 0.2) tdm_lssparse ## &lt;&lt;TermDocumentMatrix (terms: 5, documents: 20)&gt;&gt; ## Non-/sparse entries: 94/6 ## Sparsity : 6% ## Maximal term length: 6 ## Weighting : term frequency (tf) tdm_lssparse2 &lt;- removeSparseTerms(tdm, 0.7) #allow more terms tdm_lssparse2 ## &lt;&lt;TermDocumentMatrix (terms: 39, documents: 20)&gt;&gt; ## Non-/sparse entries: 413/367 ## Sparsity : 47% ## Maximal term length: 9 ## Weighting : term frequency (tf) Terms(tdm_lssparse2) ## [1] &quot;about&quot; &quot;also&quot; &quot;and&quot; &quot;are&quot; &quot;barrels&quot; ## [6] &quot;but&quot; &quot;crude&quot; &quot;dlrs&quot; &quot;for&quot; &quot;from&quot; ## [11] &quot;has&quot; &quot;its&quot; &quot;last&quot; &quot;market&quot; &quot;mln&quot; ## [16] &quot;new&quot; &quot;not&quot; &quot;oil&quot; &quot;one&quot; &quot;opec&quot; ## [21] &quot;pct&quot; &quot;petroleum&quot; &quot;price&quot; &quot;prices&quot; &quot;prices,&quot; ## [26] &quot;prices.&quot; &quot;reuter&quot; &quot;said&quot; &quot;said.&quot; &quot;that&quot; ## [31] &quot;the&quot; &quot;there&quot; &quot;they&quot; &quot;was&quot; &quot;were&quot; ## [36] &quot;will&quot; &quot;with&quot; &quot;world&quot; &quot;would&quot; Terms(tdm_lssparse) ## [1] &quot;and&quot; &quot;oil&quot; &quot;reuter&quot; &quot;said&quot; &quot;the&quot; tdm_lssparse3 &lt;- removeSparseTerms(tdm, 0.05) # remove almost all sparsity and empty terms tdm_lssparse3 ## &lt;&lt;TermDocumentMatrix (terms: 2, documents: 20)&gt;&gt; ## Non-/sparse entries: 40/0 ## Sparsity : 0% ## Maximal term length: 6 ## Weighting : term frequency (tf) Terms(tdm_lssparse3) ## [1] &quot;oil&quot; &quot;reuter&quot; # Remove words from a text document. crude[[1]]$content ## [1] &quot;Diamond Shamrock Corp said that\\neffective today it had cut its contract prices for crude oil by\\n1.50 dlrs a barrel.\\n The reduction brings its posted price for West Texas\\nIntermediate to 16.00 dlrs a barrel, the copany said.\\n \\&quot;The price reduction today was made in the light of falling\\noil product prices and a weak crude oil market,\\&quot; a company\\nspokeswoman said.\\n Diamond is the latest in a line of U.S. oil companies that\\nhave cut its contract, or posted, prices over the last two days\\nciting weak oil markets.\\n Reuter&quot; # remove common words in English cruderemword &lt;- removeWords(crude[[1]], stopwords(&quot;english&quot;)) cruderemword$content ## [1] &quot;Diamond Shamrock Corp said \\neffective today cut contract prices crude oil \\n1.50 dlrs barrel.\\n The reduction brings posted price West Texas\\nIntermediate 16.00 dlrs barrel, copany said.\\n \\&quot;The price reduction today made light falling\\noil product prices weak crude oil market,\\&quot; company\\nspokeswoman said.\\n Diamond latest line U.S. oil companies \\n cut contract, posted, prices last two days\\nciting weak oil markets.\\n Reuter&quot; # Stem words in a text document using Porter’s stemming algorithm. crude[[1]] ## &lt;&lt;PlainTextDocument&gt;&gt; ## Metadata: 15 ## Content: chars: 527 crudestem &lt;- stemDocument(crude[[1]]) # Return various kinds of stopwords with support for different languages. stopwords(&quot;en&quot;) ## [1] &quot;i&quot; &quot;me&quot; &quot;my&quot; &quot;myself&quot; &quot;we&quot; ## [6] &quot;our&quot; &quot;ours&quot; &quot;ourselves&quot; &quot;you&quot; &quot;your&quot; ## [11] &quot;yours&quot; &quot;yourself&quot; &quot;yourselves&quot; &quot;he&quot; &quot;him&quot; ## [16] &quot;his&quot; &quot;himself&quot; &quot;she&quot; &quot;her&quot; &quot;hers&quot; ## [21] &quot;herself&quot; &quot;it&quot; &quot;its&quot; &quot;itself&quot; &quot;they&quot; ## [26] &quot;them&quot; &quot;their&quot; &quot;theirs&quot; &quot;themselves&quot; &quot;what&quot; ## [31] &quot;which&quot; &quot;who&quot; &quot;whom&quot; &quot;this&quot; &quot;that&quot; ## [36] &quot;these&quot; &quot;those&quot; &quot;am&quot; &quot;is&quot; &quot;are&quot; ## [41] &quot;was&quot; &quot;were&quot; &quot;be&quot; &quot;been&quot; &quot;being&quot; ## [46] &quot;have&quot; &quot;has&quot; &quot;had&quot; &quot;having&quot; &quot;do&quot; ## [51] &quot;does&quot; &quot;did&quot; &quot;doing&quot; &quot;would&quot; &quot;should&quot; ## [56] &quot;could&quot; &quot;ought&quot; &quot;i&#39;m&quot; &quot;you&#39;re&quot; &quot;he&#39;s&quot; ## [61] &quot;she&#39;s&quot; &quot;it&#39;s&quot; &quot;we&#39;re&quot; &quot;they&#39;re&quot; &quot;i&#39;ve&quot; ## [66] &quot;you&#39;ve&quot; &quot;we&#39;ve&quot; &quot;they&#39;ve&quot; &quot;i&#39;d&quot; &quot;you&#39;d&quot; ## [71] &quot;he&#39;d&quot; &quot;she&#39;d&quot; &quot;we&#39;d&quot; &quot;they&#39;d&quot; &quot;i&#39;ll&quot; ## [76] &quot;you&#39;ll&quot; &quot;he&#39;ll&quot; &quot;she&#39;ll&quot; &quot;we&#39;ll&quot; &quot;they&#39;ll&quot; ## [81] &quot;isn&#39;t&quot; &quot;aren&#39;t&quot; &quot;wasn&#39;t&quot; &quot;weren&#39;t&quot; &quot;hasn&#39;t&quot; ## [86] &quot;haven&#39;t&quot; &quot;hadn&#39;t&quot; &quot;doesn&#39;t&quot; &quot;don&#39;t&quot; &quot;didn&#39;t&quot; ## [91] &quot;won&#39;t&quot; &quot;wouldn&#39;t&quot; &quot;shan&#39;t&quot; &quot;shouldn&#39;t&quot; &quot;can&#39;t&quot; ## [96] &quot;cannot&quot; &quot;couldn&#39;t&quot; &quot;mustn&#39;t&quot; &quot;let&#39;s&quot; &quot;that&#39;s&quot; ## [101] &quot;who&#39;s&quot; &quot;what&#39;s&quot; &quot;here&#39;s&quot; &quot;there&#39;s&quot; &quot;when&#39;s&quot; ## [106] &quot;where&#39;s&quot; &quot;why&#39;s&quot; &quot;how&#39;s&quot; &quot;a&quot; &quot;an&quot; ## [111] &quot;the&quot; &quot;and&quot; &quot;but&quot; &quot;if&quot; &quot;or&quot; ## [116] &quot;because&quot; &quot;as&quot; &quot;until&quot; &quot;while&quot; &quot;of&quot; ## [121] &quot;at&quot; &quot;by&quot; &quot;for&quot; &quot;with&quot; &quot;about&quot; ## [126] &quot;against&quot; &quot;between&quot; &quot;into&quot; &quot;through&quot; &quot;during&quot; ## [131] &quot;before&quot; &quot;after&quot; &quot;above&quot; &quot;below&quot; &quot;to&quot; ## [136] &quot;from&quot; &quot;up&quot; &quot;down&quot; &quot;in&quot; &quot;out&quot; ## [141] &quot;on&quot; &quot;off&quot; &quot;over&quot; &quot;under&quot; &quot;again&quot; ## [146] &quot;further&quot; &quot;then&quot; &quot;once&quot; &quot;here&quot; &quot;there&quot; ## [151] &quot;when&quot; &quot;where&quot; &quot;why&quot; &quot;how&quot; &quot;all&quot; ## [156] &quot;any&quot; &quot;both&quot; &quot;each&quot; &quot;few&quot; &quot;more&quot; ## [161] &quot;most&quot; &quot;other&quot; &quot;some&quot; &quot;such&quot; &quot;no&quot; ## [166] &quot;nor&quot; &quot;not&quot; &quot;only&quot; &quot;own&quot; &quot;same&quot; ## [171] &quot;so&quot; &quot;than&quot; &quot;too&quot; &quot;very&quot; stopwords(&quot;SMART&quot;) ## [1] &quot;a&quot; &quot;a&#39;s&quot; &quot;able&quot; &quot;about&quot; ## [5] &quot;above&quot; &quot;according&quot; &quot;accordingly&quot; &quot;across&quot; ## [9] &quot;actually&quot; &quot;after&quot; &quot;afterwards&quot; &quot;again&quot; ## [13] &quot;against&quot; &quot;ain&#39;t&quot; &quot;all&quot; &quot;allow&quot; ## [17] &quot;allows&quot; &quot;almost&quot; &quot;alone&quot; &quot;along&quot; ## [21] &quot;already&quot; &quot;also&quot; &quot;although&quot; &quot;always&quot; ## [25] &quot;am&quot; &quot;among&quot; &quot;amongst&quot; &quot;an&quot; ## [29] &quot;and&quot; &quot;another&quot; &quot;any&quot; &quot;anybody&quot; ## [33] &quot;anyhow&quot; &quot;anyone&quot; &quot;anything&quot; &quot;anyway&quot; ## [37] &quot;anyways&quot; &quot;anywhere&quot; &quot;apart&quot; &quot;appear&quot; ## [41] &quot;appreciate&quot; &quot;appropriate&quot; &quot;are&quot; &quot;aren&#39;t&quot; ## [45] &quot;around&quot; &quot;as&quot; &quot;aside&quot; &quot;ask&quot; ## [49] &quot;asking&quot; &quot;associated&quot; &quot;at&quot; &quot;available&quot; ## [53] &quot;away&quot; &quot;awfully&quot; &quot;b&quot; &quot;be&quot; ## [57] &quot;became&quot; &quot;because&quot; &quot;become&quot; &quot;becomes&quot; ## [61] &quot;becoming&quot; &quot;been&quot; &quot;before&quot; &quot;beforehand&quot; ## [65] &quot;behind&quot; &quot;being&quot; &quot;believe&quot; &quot;below&quot; ## [69] &quot;beside&quot; &quot;besides&quot; &quot;best&quot; &quot;better&quot; ## [73] &quot;between&quot; &quot;beyond&quot; &quot;both&quot; &quot;brief&quot; ## [77] &quot;but&quot; &quot;by&quot; &quot;c&quot; &quot;c&#39;mon&quot; ## [81] &quot;c&#39;s&quot; &quot;came&quot; &quot;can&quot; &quot;can&#39;t&quot; ## [85] &quot;cannot&quot; &quot;cant&quot; &quot;cause&quot; &quot;causes&quot; ## [89] &quot;certain&quot; &quot;certainly&quot; &quot;changes&quot; &quot;clearly&quot; ## [93] &quot;co&quot; &quot;com&quot; &quot;come&quot; &quot;comes&quot; ## [97] &quot;concerning&quot; &quot;consequently&quot; &quot;consider&quot; &quot;considering&quot; ## [101] &quot;contain&quot; &quot;containing&quot; &quot;contains&quot; &quot;corresponding&quot; ## [105] &quot;could&quot; &quot;couldn&#39;t&quot; &quot;course&quot; &quot;currently&quot; ## [109] &quot;d&quot; &quot;definitely&quot; &quot;described&quot; &quot;despite&quot; ## [113] &quot;did&quot; &quot;didn&#39;t&quot; &quot;different&quot; &quot;do&quot; ## [117] &quot;does&quot; &quot;doesn&#39;t&quot; &quot;doing&quot; &quot;don&#39;t&quot; ## [121] &quot;done&quot; &quot;down&quot; &quot;downwards&quot; &quot;during&quot; ## [125] &quot;e&quot; &quot;each&quot; &quot;edu&quot; &quot;eg&quot; ## [129] &quot;eight&quot; &quot;either&quot; &quot;else&quot; &quot;elsewhere&quot; ## [133] &quot;enough&quot; &quot;entirely&quot; &quot;especially&quot; &quot;et&quot; ## [137] &quot;etc&quot; &quot;even&quot; &quot;ever&quot; &quot;every&quot; ## [141] &quot;everybody&quot; &quot;everyone&quot; &quot;everything&quot; &quot;everywhere&quot; ## [145] &quot;ex&quot; &quot;exactly&quot; &quot;example&quot; &quot;except&quot; ## [149] &quot;f&quot; &quot;far&quot; &quot;few&quot; &quot;fifth&quot; ## [153] &quot;first&quot; &quot;five&quot; &quot;followed&quot; &quot;following&quot; ## [157] &quot;follows&quot; &quot;for&quot; &quot;former&quot; &quot;formerly&quot; ## [161] &quot;forth&quot; &quot;four&quot; &quot;from&quot; &quot;further&quot; ## [165] &quot;furthermore&quot; &quot;g&quot; &quot;get&quot; &quot;gets&quot; ## [169] &quot;getting&quot; &quot;given&quot; &quot;gives&quot; &quot;go&quot; ## [173] &quot;goes&quot; &quot;going&quot; &quot;gone&quot; &quot;got&quot; ## [177] &quot;gotten&quot; &quot;greetings&quot; &quot;h&quot; &quot;had&quot; ## [181] &quot;hadn&#39;t&quot; &quot;happens&quot; &quot;hardly&quot; &quot;has&quot; ## [185] &quot;hasn&#39;t&quot; &quot;have&quot; &quot;haven&#39;t&quot; &quot;having&quot; ## [189] &quot;he&quot; &quot;he&#39;s&quot; &quot;hello&quot; &quot;help&quot; ## [193] &quot;hence&quot; &quot;her&quot; &quot;here&quot; &quot;here&#39;s&quot; ## [197] &quot;hereafter&quot; &quot;hereby&quot; &quot;herein&quot; &quot;hereupon&quot; ## [201] &quot;hers&quot; &quot;herself&quot; &quot;hi&quot; &quot;him&quot; ## [205] &quot;himself&quot; &quot;his&quot; &quot;hither&quot; &quot;hopefully&quot; ## [209] &quot;how&quot; &quot;howbeit&quot; &quot;however&quot; &quot;i&quot; ## [213] &quot;i&#39;d&quot; &quot;i&#39;ll&quot; &quot;i&#39;m&quot; &quot;i&#39;ve&quot; ## [217] &quot;ie&quot; &quot;if&quot; &quot;ignored&quot; &quot;immediate&quot; ## [221] &quot;in&quot; &quot;inasmuch&quot; &quot;inc&quot; &quot;indeed&quot; ## [225] &quot;indicate&quot; &quot;indicated&quot; &quot;indicates&quot; &quot;inner&quot; ## [229] &quot;insofar&quot; &quot;instead&quot; &quot;into&quot; &quot;inward&quot; ## [233] &quot;is&quot; &quot;isn&#39;t&quot; &quot;it&quot; &quot;it&#39;d&quot; ## [237] &quot;it&#39;ll&quot; &quot;it&#39;s&quot; &quot;its&quot; &quot;itself&quot; ## [241] &quot;j&quot; &quot;just&quot; &quot;k&quot; &quot;keep&quot; ## [245] &quot;keeps&quot; &quot;kept&quot; &quot;know&quot; &quot;knows&quot; ## [249] &quot;known&quot; &quot;l&quot; &quot;last&quot; &quot;lately&quot; ## [253] &quot;later&quot; &quot;latter&quot; &quot;latterly&quot; &quot;least&quot; ## [257] &quot;less&quot; &quot;lest&quot; &quot;let&quot; &quot;let&#39;s&quot; ## [261] &quot;like&quot; &quot;liked&quot; &quot;likely&quot; &quot;little&quot; ## [265] &quot;look&quot; &quot;looking&quot; &quot;looks&quot; &quot;ltd&quot; ## [269] &quot;m&quot; &quot;mainly&quot; &quot;many&quot; &quot;may&quot; ## [273] &quot;maybe&quot; &quot;me&quot; &quot;mean&quot; &quot;meanwhile&quot; ## [277] &quot;merely&quot; &quot;might&quot; &quot;more&quot; &quot;moreover&quot; ## [281] &quot;most&quot; &quot;mostly&quot; &quot;much&quot; &quot;must&quot; ## [285] &quot;my&quot; &quot;myself&quot; &quot;n&quot; &quot;name&quot; ## [289] &quot;namely&quot; &quot;nd&quot; &quot;near&quot; &quot;nearly&quot; ## [293] &quot;necessary&quot; &quot;need&quot; &quot;needs&quot; &quot;neither&quot; ## [297] &quot;never&quot; &quot;nevertheless&quot; &quot;new&quot; &quot;next&quot; ## [301] &quot;nine&quot; &quot;no&quot; &quot;nobody&quot; &quot;non&quot; ## [305] &quot;none&quot; &quot;noone&quot; &quot;nor&quot; &quot;normally&quot; ## [309] &quot;not&quot; &quot;nothing&quot; &quot;novel&quot; &quot;now&quot; ## [313] &quot;nowhere&quot; &quot;o&quot; &quot;obviously&quot; &quot;of&quot; ## [317] &quot;off&quot; &quot;often&quot; &quot;oh&quot; &quot;ok&quot; ## [321] &quot;okay&quot; &quot;old&quot; &quot;on&quot; &quot;once&quot; ## [325] &quot;one&quot; &quot;ones&quot; &quot;only&quot; &quot;onto&quot; ## [329] &quot;or&quot; &quot;other&quot; &quot;others&quot; &quot;otherwise&quot; ## [333] &quot;ought&quot; &quot;our&quot; &quot;ours&quot; &quot;ourselves&quot; ## [337] &quot;out&quot; &quot;outside&quot; &quot;over&quot; &quot;overall&quot; ## [341] &quot;own&quot; &quot;p&quot; &quot;particular&quot; &quot;particularly&quot; ## [345] &quot;per&quot; &quot;perhaps&quot; &quot;placed&quot; &quot;please&quot; ## [349] &quot;plus&quot; &quot;possible&quot; &quot;presumably&quot; &quot;probably&quot; ## [353] &quot;provides&quot; &quot;q&quot; &quot;que&quot; &quot;quite&quot; ## [357] &quot;qv&quot; &quot;r&quot; &quot;rather&quot; &quot;rd&quot; ## [361] &quot;re&quot; &quot;really&quot; &quot;reasonably&quot; &quot;regarding&quot; ## [365] &quot;regardless&quot; &quot;regards&quot; &quot;relatively&quot; &quot;respectively&quot; ## [369] &quot;right&quot; &quot;s&quot; &quot;said&quot; &quot;same&quot; ## [373] &quot;saw&quot; &quot;say&quot; &quot;saying&quot; &quot;says&quot; ## [377] &quot;second&quot; &quot;secondly&quot; &quot;see&quot; &quot;seeing&quot; ## [381] &quot;seem&quot; &quot;seemed&quot; &quot;seeming&quot; &quot;seems&quot; ## [385] &quot;seen&quot; &quot;self&quot; &quot;selves&quot; &quot;sensible&quot; ## [389] &quot;sent&quot; &quot;serious&quot; &quot;seriously&quot; &quot;seven&quot; ## [393] &quot;several&quot; &quot;shall&quot; &quot;she&quot; &quot;should&quot; ## [397] &quot;shouldn&#39;t&quot; &quot;since&quot; &quot;six&quot; &quot;so&quot; ## [401] &quot;some&quot; &quot;somebody&quot; &quot;somehow&quot; &quot;someone&quot; ## [405] &quot;something&quot; &quot;sometime&quot; &quot;sometimes&quot; &quot;somewhat&quot; ## [409] &quot;somewhere&quot; &quot;soon&quot; &quot;sorry&quot; &quot;specified&quot; ## [413] &quot;specify&quot; &quot;specifying&quot; &quot;still&quot; &quot;sub&quot; ## [417] &quot;such&quot; &quot;sup&quot; &quot;sure&quot; &quot;t&quot; ## [421] &quot;t&#39;s&quot; &quot;take&quot; &quot;taken&quot; &quot;tell&quot; ## [425] &quot;tends&quot; &quot;th&quot; &quot;than&quot; &quot;thank&quot; ## [429] &quot;thanks&quot; &quot;thanx&quot; &quot;that&quot; &quot;that&#39;s&quot; ## [433] &quot;thats&quot; &quot;the&quot; &quot;their&quot; &quot;theirs&quot; ## [437] &quot;them&quot; &quot;themselves&quot; &quot;then&quot; &quot;thence&quot; ## [441] &quot;there&quot; &quot;there&#39;s&quot; &quot;thereafter&quot; &quot;thereby&quot; ## [445] &quot;therefore&quot; &quot;therein&quot; &quot;theres&quot; &quot;thereupon&quot; ## [449] &quot;these&quot; &quot;they&quot; &quot;they&#39;d&quot; &quot;they&#39;ll&quot; ## [453] &quot;they&#39;re&quot; &quot;they&#39;ve&quot; &quot;think&quot; &quot;third&quot; ## [457] &quot;this&quot; &quot;thorough&quot; &quot;thoroughly&quot; &quot;those&quot; ## [461] &quot;though&quot; &quot;three&quot; &quot;through&quot; &quot;throughout&quot; ## [465] &quot;thru&quot; &quot;thus&quot; &quot;to&quot; &quot;together&quot; ## [469] &quot;too&quot; &quot;took&quot; &quot;toward&quot; &quot;towards&quot; ## [473] &quot;tried&quot; &quot;tries&quot; &quot;truly&quot; &quot;try&quot; ## [477] &quot;trying&quot; &quot;twice&quot; &quot;two&quot; &quot;u&quot; ## [481] &quot;un&quot; &quot;under&quot; &quot;unfortunately&quot; &quot;unless&quot; ## [485] &quot;unlikely&quot; &quot;until&quot; &quot;unto&quot; &quot;up&quot; ## [489] &quot;upon&quot; &quot;us&quot; &quot;use&quot; &quot;used&quot; ## [493] &quot;useful&quot; &quot;uses&quot; &quot;using&quot; &quot;usually&quot; ## [497] &quot;uucp&quot; &quot;v&quot; &quot;value&quot; &quot;various&quot; ## [501] &quot;very&quot; &quot;via&quot; &quot;viz&quot; &quot;vs&quot; ## [505] &quot;w&quot; &quot;want&quot; &quot;wants&quot; &quot;was&quot; ## [509] &quot;wasn&#39;t&quot; &quot;way&quot; &quot;we&quot; &quot;we&#39;d&quot; ## [513] &quot;we&#39;ll&quot; &quot;we&#39;re&quot; &quot;we&#39;ve&quot; &quot;welcome&quot; ## [517] &quot;well&quot; &quot;went&quot; &quot;were&quot; &quot;weren&#39;t&quot; ## [521] &quot;what&quot; &quot;what&#39;s&quot; &quot;whatever&quot; &quot;when&quot; ## [525] &quot;whence&quot; &quot;whenever&quot; &quot;where&quot; &quot;where&#39;s&quot; ## [529] &quot;whereafter&quot; &quot;whereas&quot; &quot;whereby&quot; &quot;wherein&quot; ## [533] &quot;whereupon&quot; &quot;wherever&quot; &quot;whether&quot; &quot;which&quot; ## [537] &quot;while&quot; &quot;whither&quot; &quot;who&quot; &quot;who&#39;s&quot; ## [541] &quot;whoever&quot; &quot;whole&quot; &quot;whom&quot; &quot;whose&quot; ## [545] &quot;why&quot; &quot;will&quot; &quot;willing&quot; &quot;wish&quot; ## [549] &quot;with&quot; &quot;within&quot; &quot;without&quot; &quot;won&#39;t&quot; ## [553] &quot;wonder&quot; &quot;would&quot; &quot;would&quot; &quot;wouldn&#39;t&quot; ## [557] &quot;x&quot; &quot;y&quot; &quot;yes&quot; &quot;yet&quot; ## [561] &quot;you&quot; &quot;you&#39;d&quot; &quot;you&#39;ll&quot; &quot;you&#39;re&quot; ## [565] &quot;you&#39;ve&quot; &quot;your&quot; &quot;yours&quot; &quot;yourself&quot; ## [569] &quot;yourselves&quot; &quot;z&quot; &quot;zero&quot; stopwords(&quot;german&quot;) ## [1] &quot;aber&quot; &quot;alle&quot; &quot;allem&quot; &quot;allen&quot; &quot;aller&quot; ## [6] &quot;alles&quot; &quot;als&quot; &quot;also&quot; &quot;am&quot; &quot;an&quot; ## [11] &quot;ander&quot; &quot;andere&quot; &quot;anderem&quot; &quot;anderen&quot; &quot;anderer&quot; ## [16] &quot;anderes&quot; &quot;anderm&quot; &quot;andern&quot; &quot;anderr&quot; &quot;anders&quot; ## [21] &quot;auch&quot; &quot;auf&quot; &quot;aus&quot; &quot;bei&quot; &quot;bin&quot; ## [26] &quot;bis&quot; &quot;bist&quot; &quot;da&quot; &quot;damit&quot; &quot;dann&quot; ## [31] &quot;der&quot; &quot;den&quot; &quot;des&quot; &quot;dem&quot; &quot;die&quot; ## [36] &quot;das&quot; &quot;daß&quot; &quot;derselbe&quot; &quot;derselben&quot; &quot;denselben&quot; ## [41] &quot;desselben&quot; &quot;demselben&quot; &quot;dieselbe&quot; &quot;dieselben&quot; &quot;dasselbe&quot; ## [46] &quot;dazu&quot; &quot;dein&quot; &quot;deine&quot; &quot;deinem&quot; &quot;deinen&quot; ## [51] &quot;deiner&quot; &quot;deines&quot; &quot;denn&quot; &quot;derer&quot; &quot;dessen&quot; ## [56] &quot;dich&quot; &quot;dir&quot; &quot;du&quot; &quot;dies&quot; &quot;diese&quot; ## [61] &quot;diesem&quot; &quot;diesen&quot; &quot;dieser&quot; &quot;dieses&quot; &quot;doch&quot; ## [66] &quot;dort&quot; &quot;durch&quot; &quot;ein&quot; &quot;eine&quot; &quot;einem&quot; ## [71] &quot;einen&quot; &quot;einer&quot; &quot;eines&quot; &quot;einig&quot; &quot;einige&quot; ## [76] &quot;einigem&quot; &quot;einigen&quot; &quot;einiger&quot; &quot;einiges&quot; &quot;einmal&quot; ## [81] &quot;er&quot; &quot;ihn&quot; &quot;ihm&quot; &quot;es&quot; &quot;etwas&quot; ## [86] &quot;euer&quot; &quot;eure&quot; &quot;eurem&quot; &quot;euren&quot; &quot;eurer&quot; ## [91] &quot;eures&quot; &quot;für&quot; &quot;gegen&quot; &quot;gewesen&quot; &quot;hab&quot; ## [96] &quot;habe&quot; &quot;haben&quot; &quot;hat&quot; &quot;hatte&quot; &quot;hatten&quot; ## [101] &quot;hier&quot; &quot;hin&quot; &quot;hinter&quot; &quot;ich&quot; &quot;mich&quot; ## [106] &quot;mir&quot; &quot;ihr&quot; &quot;ihre&quot; &quot;ihrem&quot; &quot;ihren&quot; ## [111] &quot;ihrer&quot; &quot;ihres&quot; &quot;euch&quot; &quot;im&quot; &quot;in&quot; ## [116] &quot;indem&quot; &quot;ins&quot; &quot;ist&quot; &quot;jede&quot; &quot;jedem&quot; ## [121] &quot;jeden&quot; &quot;jeder&quot; &quot;jedes&quot; &quot;jene&quot; &quot;jenem&quot; ## [126] &quot;jenen&quot; &quot;jener&quot; &quot;jenes&quot; &quot;jetzt&quot; &quot;kann&quot; ## [131] &quot;kein&quot; &quot;keine&quot; &quot;keinem&quot; &quot;keinen&quot; &quot;keiner&quot; ## [136] &quot;keines&quot; &quot;können&quot; &quot;könnte&quot; &quot;machen&quot; &quot;man&quot; ## [141] &quot;manche&quot; &quot;manchem&quot; &quot;manchen&quot; &quot;mancher&quot; &quot;manches&quot; ## [146] &quot;mein&quot; &quot;meine&quot; &quot;meinem&quot; &quot;meinen&quot; &quot;meiner&quot; ## [151] &quot;meines&quot; &quot;mit&quot; &quot;muss&quot; &quot;musste&quot; &quot;nach&quot; ## [156] &quot;nicht&quot; &quot;nichts&quot; &quot;noch&quot; &quot;nun&quot; &quot;nur&quot; ## [161] &quot;ob&quot; &quot;oder&quot; &quot;ohne&quot; &quot;sehr&quot; &quot;sein&quot; ## [166] &quot;seine&quot; &quot;seinem&quot; &quot;seinen&quot; &quot;seiner&quot; &quot;seines&quot; ## [171] &quot;selbst&quot; &quot;sich&quot; &quot;sie&quot; &quot;ihnen&quot; &quot;sind&quot; ## [176] &quot;so&quot; &quot;solche&quot; &quot;solchem&quot; &quot;solchen&quot; &quot;solcher&quot; ## [181] &quot;solches&quot; &quot;soll&quot; &quot;sollte&quot; &quot;sondern&quot; &quot;sonst&quot; ## [186] &quot;über&quot; &quot;um&quot; &quot;und&quot; &quot;uns&quot; &quot;unse&quot; ## [191] &quot;unsem&quot; &quot;unsen&quot; &quot;unser&quot; &quot;unses&quot; &quot;unter&quot; ## [196] &quot;viel&quot; &quot;vom&quot; &quot;von&quot; &quot;vor&quot; &quot;während&quot; ## [201] &quot;war&quot; &quot;waren&quot; &quot;warst&quot; &quot;was&quot; &quot;weg&quot; ## [206] &quot;weil&quot; &quot;weiter&quot; &quot;welche&quot; &quot;welchem&quot; &quot;welchen&quot; ## [211] &quot;welcher&quot; &quot;welches&quot; &quot;wenn&quot; &quot;werde&quot; &quot;werden&quot; ## [216] &quot;wie&quot; &quot;wieder&quot; &quot;will&quot; &quot;wir&quot; &quot;wird&quot; ## [221] &quot;wirst&quot; &quot;wo&quot; &quot;wollen&quot; &quot;wollte&quot; &quot;würde&quot; ## [226] &quot;würden&quot; &quot;zu&quot; &quot;zum&quot; &quot;zur&quot; &quot;zwar&quot; ## [231] &quot;zwischen&quot; # Strip Whitespace from a Text Document crude[[1]]$content ## [1] &quot;Diamond Shamrock Corp said that\\neffective today it had cut its contract prices for crude oil by\\n1.50 dlrs a barrel.\\n The reduction brings its posted price for West Texas\\nIntermediate to 16.00 dlrs a barrel, the copany said.\\n \\&quot;The price reduction today was made in the light of falling\\noil product prices and a weak crude oil market,\\&quot; a company\\nspokeswoman said.\\n Diamond is the latest in a line of U.S. oil companies that\\nhave cut its contract, or posted, prices over the last two days\\nciting weak oil markets.\\n Reuter&quot; stripWhitespace(crude[[1]]$content) ## [1] &quot;Diamond Shamrock Corp said that effective today it had cut its contract prices for crude oil by 1.50 dlrs a barrel. The reduction brings its posted price for West Texas Intermediate to 16.00 dlrs a barrel, the copany said. \\&quot;The price reduction today was made in the light of falling oil product prices and a weak crude oil market,\\&quot; a company spokeswoman said. Diamond is the latest in a line of U.S. oil companies that have cut its contract, or posted, prices over the last two days citing weak oil markets. Reuter&quot; # getTransformations Predefined transformations (mappings) which can be used with tm_map ## Document access triggers the stemming function ## (i.e., all other documents are not stemmed yet) tm_map(crude, stemDocument, lazy = TRUE)[[1]] ## &lt;&lt;PlainTextDocument&gt;&gt; ## Metadata: 15 ## Content: chars: 484 ## Use wrapper to apply character processing function tm_map(crude, content_transformer(tolower)) ## &lt;&lt;VCorpus&gt;&gt; ## Metadata: corpus specific: 0, document level (indexed): 0 ## Content: documents: 20 ## Generate a custom transformation function which takes the heading as new content headings &lt;- function(x) PlainTextDocument(meta(x, &quot;heading&quot;), id = meta(x, &quot;id&quot;), language = meta(x, &quot;language&quot;)) inspect(tm_map(crude, headings)) ## &lt;&lt;VCorpus&gt;&gt; ## Metadata: corpus specific: 0, document level (indexed): 0 ## Content: documents: 20 ## ## $`reut-00001.xml` ## &lt;&lt;PlainTextDocument&gt;&gt; ## Metadata: 7 ## Content: chars: 40 ## ## $`reut-00002.xml` ## &lt;&lt;PlainTextDocument&gt;&gt; ## Metadata: 7 ## Content: chars: 47 ## ## $`reut-00004.xml` ## &lt;&lt;PlainTextDocument&gt;&gt; ## Metadata: 7 ## Content: chars: 41 ## ## $`reut-00005.xml` ## &lt;&lt;PlainTextDocument&gt;&gt; ## Metadata: 7 ## Content: chars: 41 ## ## $`reut-00006.xml` ## &lt;&lt;PlainTextDocument&gt;&gt; ## Metadata: 7 ## Content: chars: 41 ## ## $`reut-00007.xml` ## &lt;&lt;PlainTextDocument&gt;&gt; ## Metadata: 7 ## Content: chars: 45 ## ## $`reut-00008.xml` ## &lt;&lt;PlainTextDocument&gt;&gt; ## Metadata: 7 ## Content: chars: 49 ## ## $`reut-00009.xml` ## &lt;&lt;PlainTextDocument&gt;&gt; ## Metadata: 7 ## Content: chars: 37 ## ## $`reut-00010.xml` ## &lt;&lt;PlainTextDocument&gt;&gt; ## Metadata: 7 ## Content: chars: 39 ## ## $`reut-00011.xml` ## &lt;&lt;PlainTextDocument&gt;&gt; ## Metadata: 7 ## Content: chars: 47 ## ## $`reut-00012.xml` ## &lt;&lt;PlainTextDocument&gt;&gt; ## Metadata: 7 ## Content: chars: 46 ## ## $`reut-00013.xml` ## &lt;&lt;PlainTextDocument&gt;&gt; ## Metadata: 7 ## Content: chars: 49 ## ## $`reut-00014.xml` ## &lt;&lt;PlainTextDocument&gt;&gt; ## Metadata: 7 ## Content: chars: 49 ## ## $`reut-00015.xml` ## &lt;&lt;PlainTextDocument&gt;&gt; ## Metadata: 7 ## Content: chars: 48 ## ## $`reut-00016.xml` ## &lt;&lt;PlainTextDocument&gt;&gt; ## Metadata: 7 ## Content: chars: 40 ## ## $`reut-00018.xml` ## &lt;&lt;PlainTextDocument&gt;&gt; ## Metadata: 7 ## Content: chars: 45 ## ## $`reut-00019.xml` ## &lt;&lt;PlainTextDocument&gt;&gt; ## Metadata: 7 ## Content: chars: 45 ## ## $`reut-00021.xml` ## &lt;&lt;PlainTextDocument&gt;&gt; ## Metadata: 7 ## Content: chars: 46 ## ## $`reut-00022.xml` ## &lt;&lt;PlainTextDocument&gt;&gt; ## Metadata: 7 ## Content: chars: 44 ## ## $`reut-00023.xml` ## &lt;&lt;PlainTextDocument&gt;&gt; ## Metadata: 7 ## Content: chars: 45 22.3 Example of classifying bugs from Bugzilla Bugzilla is Issue Tracking System that allow us to maintain and track the evolution of a project. The following example shows how to work with entries from Bugzilla. It is assumed that the data has been extracted and we have the records in a flat file (this can be done using Web crawlers or directly using the SQL database). library(foreign) # path_name &lt;- file.path(&quot;C:&quot;, &quot;datasets&quot;, &quot;textMining&quot;) # path_name # dir(path_name) #d &lt;- read.arff(&quot;./datasets/textMining/compendium.arff&quot;) options(stringsAsFactors = FALSE) d &lt;- read.arff(&quot;./datasets/textMining/reviewsBugs.arff&quot; ) str(d) ## &#39;data.frame&#39;: 789 obs. of 2 variables: ## $ revContent: chr &quot;Can&#39;t see traffic colors now With latest updates I can&#39;t see the traffic green/red/yellow - I have to pull over and zoom in the&quot;| __truncated__ &quot;Google Map I like it so far, it has not steered me wrong.&quot; &quot;Could be 100X better Google should start listening to customers then they&#39;d actually build a proper product.&quot; &quot;I like that! Easily more helpful than the map app that comes with your phone.&quot; ... ## $ revBug : Factor w/ 2 levels &quot;N&quot;,&quot;Y&quot;: 2 1 1 1 1 1 2 1 2 1 ... head(d,2) ## revContent ## 1 Can&#39;t see traffic colors now With latest updates I can&#39;t see the traffic green/red/yellow - I have to pull over and zoom in the map so that one road fills the entire screen. Traffic checks are (were) the only reason I use google maps! ## 2 Google Map I like it so far, it has not steered me wrong. ## revBug ## 1 Y ## 2 N # fifth entry d$revContent[5] ## [1] &quot;Just deleted No I don&#39;t want to sign in or sign up for anything stop asking&quot; d$revBug[5] ## [1] N ## Levels: N Y Creating a Document-Term Matrix (DTM) library(tm) ds &lt;- DataframeSource(as.data.frame(d$revContent)) dsc &lt;- Corpus(ds) # weighting=TfIdf weighting is Tf-Idf # minWordLength=WL the minimum word length is WL # minDocFreq=ND each word must appear at least in ND docs # Other options of DTM # These are not really needed, if preprocessing has been carried out: # stemming=TRUE stemming is applied # stopwords=TRUE stopwords are eliminated # removeNumbers=TRUE numbers are eliminated dtm &lt;- DocumentTermMatrix(dsc, control = list(weighting = weightTfIdf, minDocFreq=3, stopwords = TRUE, removeNumbers = TRUE)) # dim(dtm) # inspect(dtm) #[1:10,1:10]) # dtm.70=removeSparseTerms(dtm,sparse=0.7) # dtm.70 # or dim(dtm.70) # note that the term-document matrix needs to be transformed (casted) # to a matrix form in the following barplot command sparseparam &lt;- 0.90 dtm_sprs &lt;- removeSparseTerms(dtm,sparse=sparseparam) maintitle &lt;-paste0(&quot;Most frequent terms (sparseness=&quot; ,sparseparam , &quot; )&quot;) barplot(as.matrix(dtm_sprs),xlab=&quot;terms&quot;,ylab=&quot;number of occurrences&quot;, main=maintitle) As data frame: #dtmdf &lt;- as.data.frame(dtm.90) inspect(dtm_sprs) dtmdf &lt;- as.data.frame(as.matrix(dtm_sprs)) # rownames(dtm)&lt;- 1:nrow(dtm) class &lt;- d$revBug dtmdf &lt;- cbind(dtmdf,class) head(dtmdf, 3) Now, we can explore things such as “which words are associated with”feature“?” # which words are associated with &quot;bug&quot;? findAssocs(dtm, &#39;bug&#39;, .3) ## $bug ## it? mini major users causing ipad ## 1.00 0.92 0.91 0.80 0.62 0.57 And find frequent terms. findFreqTerms(dtm,15) ## [1] &quot;addicting&quot; &quot;addictive&quot; &quot;amazing&quot; &quot;app&quot; &quot;awesome&quot; ## [6] &quot;best&quot; &quot;clans&quot; &quot;clash&quot; &quot;cool&quot; &quot;crashes&quot; ## [11] &quot;ever&quot; &quot;fun&quot; &quot;game&quot; &quot;game!&quot; &quot;game.&quot; ## [16] &quot;get&quot; &quot;good&quot; &quot;google&quot; &quot;great&quot; &quot;just&quot; ## [21] &quot;like&quot; &quot;love&quot; &quot;map&quot; &quot;maps&quot; &quot;much&quot; ## [26] &quot;nice&quot; &quot;play&quot; &quot;playing&quot; &quot;really&quot; &quot;review&quot; ## [31] &quot;time&quot; &quot;update&quot; Use any classifier now: library(caret) library(randomForest) ## randomForest 4.6-12 ## Type rfNews() to see new features/changes/bug fixes. ## ## Attaching package: &#39;randomForest&#39; ## The following object is masked from &#39;package:Hmisc&#39;: ## ## combine ## The following object is masked from &#39;package:dplyr&#39;: ## ## combine ## The following object is masked from &#39;package:ggplot2&#39;: ## ## margin inTraining &lt;- createDataPartition(dtmdf$class, p = .75, list = FALSE) training &lt;- dtmdf[ inTraining,] testing &lt;- dtmdf[-inTraining,] fitControl &lt;- trainControl(## 5-fold CV method = &quot;repeatedcv&quot;, number = 5, ## repeated ten times repeats = 5) gbmFit1 &lt;- train(class ~ ., data = training, method = &quot;gbm&quot;, trControl = fitControl, ## This last option is actually one ## for gbm() that passes through verbose = FALSE) gbmFit1 ## Stochastic Gradient Boosting ## ## 593 samples ## 9 predictors ## 2 classes: &#39;N&#39;, &#39;Y&#39; ## ## No pre-processing ## Resampling: Cross-Validated (5 fold, repeated 5 times) ## Summary of sample sizes: 475, 474, 475, 474, 474, 474, ... ## Resampling results across tuning parameters: ## ## interaction.depth n.trees Accuracy Kappa ## 1 50 0.7976356644 0.0000000000 ## 1 100 0.8050648056 0.1293221041 ## 1 150 0.8091041162 0.2359115315 ## 2 50 0.8074262926 0.1834100051 ## 2 100 0.8111522575 0.2719506824 ## 2 150 0.8077738214 0.2686077808 ## 3 50 0.8111408631 0.2579999810 ## 3 100 0.8128215354 0.2884184521 ## 3 150 0.8111294687 0.2884071859 ## ## Tuning parameter &#39;shrinkage&#39; was held constant at a value of 0.1 ## ## Tuning parameter &#39;n.minobsinnode&#39; was held constant at a value of 10 ## Accuracy was used to select the optimal model using the largest value. ## The final values used for the model were n.trees = 100, ## interaction.depth = 3, shrinkage = 0.1 and n.minobsinnode = 10. # trellis.par.set(caretTheme()) # plot(gbmFit1) # # trellis.par.set(caretTheme()) # plot(gbmFit1, metric = &quot;Kappa&quot;) head(predict(gbmFit1, testing, type = &quot;prob&quot;)) ## N Y ## 1 0.6863853810 0.31361461899 ## 2 0.6863853810 0.31361461899 ## 3 0.9219789185 0.07802108147 ## 4 0.7998691244 0.20013087557 ## 5 0.2923762881 0.70762371195 ## 6 0.9618734783 0.03812652166 conf_mat &lt;- confusionMatrix(testing$class, predict(gbmFit1, testing)) conf_mat ## Confusion Matrix and Statistics ## ## Reference ## Prediction N Y ## N 150 7 ## Y 26 13 ## ## Accuracy : 0.8316327 ## 95% CI : (0.7717559, 0.8811794) ## No Information Rate : 0.8979592 ## P-Value [Acc &gt; NIR] : 0.998482894 ## ## Kappa : 0.3534586 ## Mcnemar&#39;s Test P-Value : 0.001727951 ## ## Sensitivity : 0.8522727 ## Specificity : 0.6500000 ## Pos Pred Value : 0.9554140 ## Neg Pred Value : 0.3333333 ## Prevalence : 0.8979592 ## Detection Rate : 0.7653061 ## Detection Prevalence : 0.8010204 ## Balanced Accuracy : 0.7511364 ## ## &#39;Positive&#39; Class : N ## And finally, a word cloud as an example that appears everywhere these days. library(wordcloud) ## Loading required package: RColorBrewer # calculate the frequency of words and sort in descending order. wordFreqs=sort(colSums(as.matrix(dtm_sprs)),decreasing=TRUE) wordcloud(words=names(wordFreqs),freq=wordFreqs) 22.4 Extracting data from Twitter The hardest bit is to link with Twitter. Using the TwitteR package is explained following this example. References "],
["time-series.html", "Chapter 23 Time Series 23.1 Web tutorials about Time Series:", " Chapter 23 Time Series Many sources of information are time related. For example, data from Software Configuration Management (SCM) such as Git, GitHub) systems or Dashboards such as Metrics Grimoire from Bitergia or SonarQube With MetricsGrimore or SonarQube we can extract datasets or dump of databases. For example, a dashboard for the OpenStack project is located at http://activity.openstack.org/dash/browser/ and provides datasets as MySQL dumps or JSON files. With R we can read a JSON file as follows: library(jsonlite) # Get the JSON data # gm &lt;- fromJSON(&quot;http://activity.openstack.org/dash/browser/data/json/nova.git-scm-rep-evolutionary.json&quot;) gm &lt;- fromJSON(&#39;./datasets/timeSeries/nova.git-scm-rep-evolutionary.json&#39;) str(gm) ## List of 13 ## $ added_lines : num [1:287] 431874 406 577 697 7283 ... ## $ authors : int [1:287] 1 1 4 2 7 5 4 9 8 11 ... ## $ branches : int [1:287] 1 1 1 1 1 1 1 1 1 1 ... ## $ commits : int [1:287] 3 4 16 11 121 38 35 90 66 97 ... ## $ committers : int [1:287] 1 1 4 2 7 5 4 9 8 11 ... ## $ date : chr [1:287] &quot;May 2010&quot; &quot;May 2010&quot; &quot;Jun 2010&quot; &quot;Jun 2010&quot; ... ## $ files : int [1:287] 1878 9 13 7 144 111 28 1900 89 101 ... ## $ id : int [1:287] 0 1 2 3 4 5 6 7 8 9 ... ## $ newauthors : int [1:287] 1 1 2 0 4 1 0 4 2 3 ... ## $ removed_lines: num [1:287] 864 530 187 326 2619 ... ## $ repositories : int [1:287] 1 1 1 1 1 1 1 1 1 1 ... ## $ unixtime : chr [1:287] &quot;1274659200&quot; &quot;1275264000&quot; &quot;1275868800&quot; &quot;1276473600&quot; ... ## $ week : int [1:287] 201021 201022 201023 201024 201025 201026 201027 201028 201029 201030 ... Now we can use time series packages. First, after loading the libraries, we need to create a time series object. # TS libraries library(xts) ## ## Attaching package: &#39;xts&#39; ## The following objects are masked from &#39;package:dplyr&#39;: ## ## first, last library(forecast) ## ## Attaching package: &#39;forecast&#39; ## The following object is masked from &#39;package:igraph&#39;: ## ## %&gt;% # Library to deal with dates library(lubridate) # Ceate a time series object gmts &lt;- xts(gm$commits,seq(ymd(&#39;2010-05-22&#39;),ymd(&#39;2015-11-16&#39;), by = &#39;1 week&#39;)) # TS Object str(gmts) ## An &#39;xts&#39; object on 2010-05-22/2015-11-14 containing: ## Data: int [1:287, 1] 3 4 16 11 121 38 35 90 66 97 ... ## Indexed by objects of class: [Date] TZ: UTC ## xts Attributes: ## NULL head(gmts, 3) ## [,1] ## 2010-05-22 3 ## 2010-05-29 4 ## 2010-06-05 16 Visualise the time series object plot(gmts) Arima model: fit &lt;- auto.arima(gmts) fit ## Series: gmts ## ARIMA(0,1,2) ## ## Coefficients: ## ma1 ma2 ## -0.3120662 -0.3068537 ## s.e. 0.0580588 0.0642417 ## ## sigma^2 estimated as 1341.396: log likelihood=-1434.83 ## AIC=2875.67 AICc=2875.75 BIC=2886.64 forecast(fit, 5) ## Point Forecast Lo 80 Hi 80 Lo 95 Hi 95 ## 2010 7.748275458 -39.18864059 54.68519151 -64.03554306 79.53209398 ## 2017 15.157545088 -41.81337267 72.12846284 -71.97195479 102.28704496 ## 2024 15.157545088 -44.55527171 74.87036189 -76.16532750 106.48041768 ## 2031 15.157545088 -47.17667914 77.49176931 -80.17442420 110.48951438 ## 2038 15.157545088 -49.69220842 80.00729859 -84.02159424 114.33668442 plot(forecast(fit, 5)) 23.1 Web tutorials about Time Series: http://www.statoek.wiso.uni-goettingen.de/veranstaltungen/zeitreihen/sommer03/ts_r_intro.pdf http://www.statmethods.net/advstats/timeseries.html http://a-little-book-of-r-for-time-series.readthedocs.org/en/latest/ https://media.readthedocs.org/pdf/a-little-book-of-r-for-time-series/latest/a-little-book-of-r-for-time-series.pdf http://www.stat.pitt.edu/stoffer/tsa3/ "],
["references-1.html", "References", " References "]
]
